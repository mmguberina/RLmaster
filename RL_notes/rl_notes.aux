\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {0.1}Plan}{2}{section.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Old plan, look at update}{2}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Update}{2}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.1}TODOs}{3}{subsection.0.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Purpose}{3}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {0.2}Immitation learning}{3}{section.0.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {0.3}Formal setting}{3}{section.0.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.1}Markov chain}{3}{subsection.0.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.2}Markov decision process}{4}{subsection.0.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.3}Partially observed Markov decision process}{4}{subsection.0.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The goal of reinforcement learning}{5}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Note}{5}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.4}Value functions}{6}{subsection.0.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Definition: Q-function}{6}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Definition: value function}{6}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {0.4}Policy gradients}{7}{section.0.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The idea}{7}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.4.1}Reducing variance}{9}{subsection.0.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Causality}{9}{section*.10}\protected@file@percent }
\newlabel{eq:reward_to_go}{{30}{9}{Causality}{equation.0.4.30}{}}
\@writefile{toc}{\contentsline {subsubsection}{Baselines}{9}{section*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.4.2}Off-policy gradients}{11}{subsection.0.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Importance sampling}{11}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Policy gradient with automatic differentiation}{13}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Policy gradients in practice}{13}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.4.3}Advanced policy gradients}{13}{subsection.0.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {0.5}Actor-critic algorithms}{14}{section.0.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.5.1}Policy evaluation}{15}{subsection.0.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.5.2}From evaluation to actor-critic}{16}{subsection.0.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.5.3}Aside: discount factors}{17}{subsection.0.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.5.4}Actor-critic design choises}{18}{subsection.0.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.5.5}Online actor-critic in practise}{18}{subsection.0.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.5.6}Critics as state-dependent baselines}{20}{subsection.0.5.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Eligibility traces and n-step returns}{21}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Generalied advantage estimation (GAE)}{21}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {0.6}Value function methods}{22}{section.0.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Can we omit policy gradient completely?}{22}{section*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.6.1}Policy iteration}{22}{subsection.0.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Dynamic programming}{22}{section*.18}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Policy iteration with dynamic programming}{22}{section*.19}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Even simpler dynamic programming}{22}{section*.20}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Fitted value iteration and Q-iteration}{23}{section*.21}\protected@file@percent }
\newlabel{eq:fitted_q_iteration_algorithm}{{0.6.1}{23}{Fitted value iteration and Q-iteration}{Item.38}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.6.2}From Q-iteration to Q-learning}{24}{subsection.0.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why is this algorithm off-policy}{24}{section*.22}\protected@file@percent }
\newlabel{eq:online_q_iteration_algorithm}{{0.6.2}{24}{Why is this algorithm off-policy}{equation.0.6.119}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.6.3}Value function in theory}{25}{subsection.0.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{A sad corollary}{26}{section*.23}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {0.7}Deep RL with Q-functions}{26}{section.0.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Replay buffers}{27}{section*.24}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.7.1}Target networks}{27}{subsection.0.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.7.2}A general view of Q-learning}{28}{subsection.0.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {0.8}Improving Q-learning}{28}{section.0.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Are Q-values accurate?}{28}{section*.25}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.8.1}Double Q-learning}{29}{subsection.0.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Double Q-learning in practise}{29}{section*.26}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Multi-step returns}{29}{section*.27}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Q-learning with N-step returns}{30}{section*.28}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.8.2}Q-learning with continuous actions}{30}{subsection.0.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Option 1}{30}{section*.29}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Option 2}{30}{section*.30}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Option 3}{31}{section*.31}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{DDPG}{31}{section*.32}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.8.3}Implementation tips and examples}{31}{subsection.0.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Tips}{31}{section*.33}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {0.9}Even more advanced policy gradients (PPO and TRPO)}{32}{section.0.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Policy gradient as policy iteration}{32}{section*.34}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Bounding the objective value}{35}{section*.35}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.9.1}Policy gradients with constraints}{36}{subsection.0.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{How do we enforce the constraint}{36}{section*.36}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.9.2}Natural gradient}{36}{subsection.0.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Do these results carry over in practise?}{38}{section*.37}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.9.3}Practical methods and notes}{38}{subsection.0.9.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Natural policy gradient}{38}{section*.38}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Trust region policy optimization}{38}{section*.39}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Just using importance sampling objective directly}{38}{section*.40}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {0.10}Optimal control and planning}{38}{section.0.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The objective}{38}{section*.41}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Stochastic optimization}{39}{section*.42}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Cross-entropy method (CEM)}{39}{section*.43}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Discrete case: Monte Carlo tree search (MCTS)}{40}{section*.44}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.10.1}Trajectory optimization with derivatives}{41}{subsection.0.10.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Linear case: LQR}{41}{section*.45}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.10.2}LQR for stochastic and nonlinear systems}{44}{subsection.0.10.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Nonlinear case: differential dynamic programming (DDP)/ iterative LQR}{44}{section*.46}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Nonlinear model-predictive control}{46}{section*.47}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {0.11}Model-based reinforcement learning}{46}{section.0.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.11.1}Uncertainty in model-based RL}{48}{subsection.0.11.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Uncertainty-aware neural network models}{48}{section*.48}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Idea 1:}{48}{section*.49}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Idea 2:}{48}{section*.50}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Quick overview of Bayesian neural networks}{49}{section*.51}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Bootstrap ensembles}{49}{section*.52}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{How to plan with uncertainty}{50}{section*.53}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Other options:}{50}{section*.54}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.11.2}Model-based reinforcement learning with images}{50}{subsection.0.11.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{State space (latent space models)}{51}{section*.55}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {0.12}Model-based policy learning}{52}{section.0.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{What's the solution?}{53}{section*.56}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{First class of solutions}{53}{section*.57}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Second class of solutions}{54}{section*.58}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.12.1}Model-free learning with a model}{54}{subsection.0.12.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Dyna}{54}{section*.59}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{General ``Dyna-style'' model-based RL recipe}{55}{section*.60}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Local models}{56}{section*.61}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {0.13}Exploration algorithms}{56}{section.0.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Optimistic exploration}{56}{section*.62}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Probability matching/posterior sampling}{56}{section*.63}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Information gain}{56}{section*.64}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{General themes}{57}{section*.65}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.13.1}Exploration in deep reinforcement learning}{57}{subsection.0.13.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Fitting generative models}{58}{section*.66}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{What kind of bonus to use?}{58}{section*.67}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What kind of model to use?}{59}{section*.68}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.13.2}Posterior sampling in deep RL}{59}{subsection.0.13.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.13.3}Information gain in DRL}{59}{subsection.0.13.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.13.4}Exploration with model errors}{60}{subsection.0.13.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.13.5}Unsupervised exploration}{60}{subsection.0.13.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Information theoretic quantities in RL}{61}{section*.69}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {0.14}Unsupervised reinforcement learning (sketches)}{61}{section.0.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Aside: exploration with intrinsic motivation}{62}{section*.70}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.14.1}Learning diverse skills}{63}{subsection.0.14.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Diversity-promoting reward function}{63}{section*.71}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {0.15}Generalisation gap}{63}{section.0.15}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What makes modern machine learning work}{63}{section*.72}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why is offline RL hard?}{64}{section*.73}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Where does RL suffer from distributional shift?}{65}{section*.74}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.15.1}Batch RL via importance sampling}{65}{subsection.0.15.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The doubly robust estimator}{67}{section*.75}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Marginalized importance sampling}{68}{section*.76}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.15.2}Batch RL via linear fitted value functions}{68}{subsection.0.15.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {0.16}Reinforcement learning as an inference problem}{68}{section.0.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.16.1}Optimal control as a model of human behavior}{69}{subsection.0.16.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.16.2}Control as inference}{70}{subsection.0.16.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Backward messages}{70}{section*.77}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{But what the if the action prior is not uniform?}{72}{section*.78}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.16.3}Policy computation}{72}{subsection.0.16.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.16.4}Forward messages}{73}{subsection.0.16.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.16.5}Control as variational inference}{74}{subsection.0.16.5}\protected@file@percent }
\newlabel{eq:policy_inference}{{383}{74}{Control as variational inference}{equation.0.16.383}{}}
\newlabel{eq:transition_probability_inference}{{384}{74}{Control as variational inference}{equation.0.16.384}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces $ p (\bm  {s}_{1:T}, \bm  {a}_{1:T}|\mathcal  {O}_{ 1:T }) $}}{75}{figure.0.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces $ q (\bm  {s}_{1:T}, \bm  {a}_{1:T}) $}}{75}{figure.0.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Variational lower bound}{75}{section*.79}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Optimizing the variational lower bound}{76}{section*.80}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.16.6}Algorithms for RL as inference}{77}{subsection.0.16.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Q-learning with soft optimality}{77}{section*.81}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Policy gradient with soft optimality}{78}{section*.82}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Policy gradient vs Q-learning}{78}{section*.83}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Benefits of soft optimality}{79}{section*.84}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Example methods}{79}{section*.85}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {0.17}Inverse reinforcement learning}{79}{section.0.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why should we worry about learning rewards?}{80}{section*.86}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Imitation learning perspective}{80}{section*.87}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The RL perspective}{80}{section*.88}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Inverse RL}{80}{section*.89}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Feature matching IRL}{80}{section*.90}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Learning the optimality variable}{81}{section*.91}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces $ p (\bm  {s}_{1:T}, \bm  {a}_{1:T}|\mathcal  {O}_{ 1:T }) $}}{81}{figure.0.3}\protected@file@percent }

\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {0.1}Plan}{1}{section.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Old plan, look at update}{1}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Update}{1}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.1}TODOs}{1}{subsection.0.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Purpose}{2}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Berkley AI class}{3}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Immitation learning}{3}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Formal setting}{3}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Markov chain}{3}{subsection.1.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Markov decision process}{3}{subsection.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Partially observed Markov decision process}{4}{subsection.1.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The goal of reinforcement learning}{4}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Note}{4}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Value functions}{5}{subsection.1.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Definition: Q-function}{5}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Definition: value function}{5}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Policy gradients}{6}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The idea}{6}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Reducing variance}{8}{subsection.1.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Causality}{8}{section*.9}\protected@file@percent }
\newlabel{eq:reward_to_go}{{1.30}{8}{Causality}{equation.1.3.30}{}}
\@writefile{toc}{\contentsline {subsubsection}{Baselines}{8}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Off-policy gradients}{10}{subsection.1.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Importance sampling}{10}{section*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Policy gradient with automatic differentiation}{12}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Policy gradients in practice}{12}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}Advanced policy gradients}{12}{subsection.1.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Actor-critic algorithms}{13}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Policy evaluation}{14}{subsection.1.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}From evaluation to actor-critic}{16}{subsection.1.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.3}Aside: discount factors}{16}{subsection.1.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.4}Actor-critic design choises}{17}{subsection.1.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.5}Online actor-critic in practise}{17}{subsection.1.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.6}Critics as state-dependent baselines}{19}{subsection.1.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Eligibility traces and n-step returns}{20}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Generalied advantage estimation (GAE)}{20}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Value function methods}{21}{section.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Can we omit policy gradient completely?}{21}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.1}Policy iteration}{21}{subsection.1.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Dynamic programming}{21}{section*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Policy iteration with dynamic programming}{21}{section*.18}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Even simpler dynamic programming}{22}{section*.19}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Fitted value iteration and Q-iteration}{22}{section*.20}\protected@file@percent }
\newlabel{eq:fitted_q_iteration_algorithm}{{1.5.1}{23}{Fitted value iteration and Q-iteration}{Item.38}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.2}From Q-iteration to Q-learning}{23}{subsection.1.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why is this algorithm off-policy}{23}{section*.21}\protected@file@percent }
\newlabel{eq:online_q_iteration_algorithm}{{1.5.2}{23}{Why is this algorithm off-policy}{equation.1.5.119}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.3}Value function in theory}{24}{subsection.1.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{A sad corollary}{25}{section*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Deep RL with Q-functions}{25}{section.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Replay buffers}{26}{section*.23}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.1}Target networks}{26}{subsection.1.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.2}A general view of Q-learning}{27}{subsection.1.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.7}Improving Q-learning}{27}{section.1.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Are Q-values accurate?}{27}{section*.24}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.1}Double Q-learning}{28}{subsection.1.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Double Q-learning in practise}{28}{section*.25}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Multi-step returns}{28}{section*.26}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Q-learning with N-step returns}{29}{section*.27}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.2}Q-learning with continuous actions}{29}{subsection.1.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Option 1}{29}{section*.28}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Option 2}{29}{section*.29}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Option 3}{30}{section*.30}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{DDPG}{30}{section*.31}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.7.3}Implementation tips and examples}{30}{subsection.1.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Tips}{30}{section*.32}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.8}Even more advanced policy gradients (PPO and TRPO)}{31}{section.1.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.9}Optimal control and planning}{31}{section.1.9}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The objective}{31}{section*.33}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Stochastic optimization}{31}{section*.34}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Cross-entropy method (CEM)}{32}{section*.35}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Discrete case: Monte Carlo tree search (MCTS)}{32}{section*.36}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.9.1}Trajectory optimization with derivatives}{33}{subsection.1.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.10}Model-based reinforcement learning}{33}{section.1.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.10.1}Uncertainty in model-based RL}{35}{subsection.1.10.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Uncertainty-aware neural network models}{35}{section*.37}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Idea 1:}{35}{section*.38}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Idea 2:}{35}{section*.39}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Quick overview of Bayesian neural networks}{36}{section*.40}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Bootstrap ensembles}{36}{section*.41}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{How to plan with uncertainty}{37}{section*.42}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Other options:}{37}{section*.43}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.10.2}Model-based reinforcement learning with images}{37}{subsection.1.10.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{State space (latent space models)}{38}{section*.44}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.11}Model-based policy learning}{39}{section.1.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{What's the solution?}{40}{section*.45}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{First class of solutions}{40}{section*.46}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Second class of solutions}{40}{section*.47}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.11.1}Model-free learning with a model}{41}{subsection.1.11.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Dyna}{41}{section*.48}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{General ``Dyna-style'' model-based RL recipe}{42}{section*.49}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Local models}{42}{section*.50}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.12}Exploration algorithms}{43}{section.1.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Optimistic exploration}{43}{section*.51}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Probability matching/posterior sampling}{43}{section*.52}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Information gain}{43}{section*.53}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{General themes}{44}{section*.54}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.12.1}Exploration in deep reinforcement learning}{44}{subsection.1.12.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Fitting generative models}{45}{section*.55}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{What kind of bonus to use?}{45}{section*.56}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What kind of model to use?}{45}{section*.57}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.12.2}Posterior sampling in deep RL}{46}{subsection.1.12.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.12.3}Information gain in DRL}{46}{subsection.1.12.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.12.4}Exploration with model errors}{47}{subsection.1.12.4}\protected@file@percent }

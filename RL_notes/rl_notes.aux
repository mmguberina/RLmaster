\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {0.1}Plan}{1}{section.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Update}{1}{section*.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.1}TODOs}{1}{subsection.0.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Purpose}{1}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Berkley AI class}{2}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Immitation learning}{2}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Formal setting}{2}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}Markov chain}{2}{subsection.1.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Markov decision process}{2}{subsection.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.3}Partially observed Markov decision process}{3}{subsection.1.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The goal of reinforcement learning}{3}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Note}{3}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.4}Value functions}{4}{subsection.1.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Definition: Q-function}{4}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Definition: value function}{4}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Policy gradients}{5}{section.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{The idea}{5}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.1}Reducing variance}{7}{subsection.1.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Causality}{7}{section*.8}\protected@file@percent }
\newlabel{eq:reward_to_go}{{1.30}{7}{Causality}{equation.1.3.30}{}}
\@writefile{toc}{\contentsline {subsubsection}{Baselines}{7}{section*.9}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.2}Off-policy gradients}{9}{subsection.1.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Importance sampling}{9}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Policy gradient with automatic differentiation}{11}{section*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Policy gradients in practice}{11}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3.3}Advanced policy gradients}{11}{subsection.1.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Actor-critic algorithms}{12}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}Policy evaluation}{13}{subsection.1.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}From evaluation to actor-critic}{15}{subsection.1.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.3}Aside: discount factors}{15}{subsection.1.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.4}Actor-critic design choises}{16}{subsection.1.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.5}Online actor-critic in practise}{16}{subsection.1.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.6}Critics as state-dependent baselines}{18}{subsection.1.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Eligibility traces and n-step returns}{19}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Generalied advantage estimation (GAE)}{19}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Value function methods}{20}{section.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Can we omit policy gradient completely?}{20}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.1}Policy iteration}{20}{subsection.1.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Dynamic programming}{20}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Policy iteration with dynamic programming}{20}{section*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Even simpler dynamic programming}{21}{section*.18}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Fitted value iteration and Q-iteration}{21}{section*.19}\protected@file@percent }
\newlabel{eq:fitted_q_iteration_algorithm}{{1.5.1}{22}{Fitted value iteration and Q-iteration}{Item.36}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.2}From Q-iteration to Q-learning}{22}{subsection.1.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Why is this algorithm off-policy}{22}{section*.20}\protected@file@percent }
\newlabel{eq:online_q_iteration_algorithm}{{1.5.2}{22}{Why is this algorithm off-policy}{equation.1.5.119}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.3}Value function in theory}{23}{subsection.1.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{A sad corollary}{24}{section*.21}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.6}Deep RL with Q-functions}{24}{section.1.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Replay buffers}{25}{section*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.6.1}Target networks}{25}{subsection.1.6.1}\protected@file@percent }

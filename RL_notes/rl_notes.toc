\contentsline {section}{\numberline {0.1}Plan}{2}{section.0.1}%
\contentsline {paragraph}{Old plan, look at update}{2}{section*.2}%
\contentsline {paragraph}{Update}{2}{section*.3}%
\contentsline {subsection}{\numberline {0.1.1}TODOs}{3}{subsection.0.1.1}%
\contentsline {paragraph}{Purpose}{3}{section*.4}%
\contentsline {chapter}{\numberline {1}Berkley AI class}{4}{chapter.1}%
\contentsline {section}{\numberline {1.1}Immitation learning}{4}{section.1.1}%
\contentsline {section}{\numberline {1.2}Formal setting}{4}{section.1.2}%
\contentsline {subsection}{\numberline {1.2.1}Markov chain}{4}{subsection.1.2.1}%
\contentsline {subsection}{\numberline {1.2.2}Markov decision process}{4}{subsection.1.2.2}%
\contentsline {subsection}{\numberline {1.2.3}Partially observed Markov decision process}{5}{subsection.1.2.3}%
\contentsline {subsubsection}{The goal of reinforcement learning}{5}{section*.5}%
\contentsline {paragraph}{Note}{6}{section*.6}%
\contentsline {subsection}{\numberline {1.2.4}Value functions}{6}{subsection.1.2.4}%
\contentsline {paragraph}{Definition: Q-function}{7}{section*.7}%
\contentsline {paragraph}{Definition: value function}{7}{section*.8}%
\contentsline {section}{\numberline {1.3}Policy gradients}{7}{section.1.3}%
\contentsline {paragraph}{The idea}{7}{section*.9}%
\contentsline {subsection}{\numberline {1.3.1}Reducing variance}{10}{subsection.1.3.1}%
\contentsline {paragraph}{Causality}{10}{section*.10}%
\contentsline {subsubsection}{Baselines}{10}{section*.11}%
\contentsline {subsection}{\numberline {1.3.2}Off-policy gradients}{11}{subsection.1.3.2}%
\contentsline {paragraph}{Importance sampling}{12}{section*.12}%
\contentsline {subsubsection}{Policy gradient with automatic differentiation}{13}{section*.13}%
\contentsline {subsubsection}{Policy gradients in practice}{14}{section*.14}%
\contentsline {subsection}{\numberline {1.3.3}Advanced policy gradients}{14}{subsection.1.3.3}%
\contentsline {section}{\numberline {1.4}Actor-critic algorithms}{15}{section.1.4}%
\contentsline {subsection}{\numberline {1.4.1}Policy evaluation}{16}{subsection.1.4.1}%
\contentsline {subsection}{\numberline {1.4.2}From evaluation to actor-critic}{17}{subsection.1.4.2}%
\contentsline {subsection}{\numberline {1.4.3}Aside: discount factors}{18}{subsection.1.4.3}%
\contentsline {subsection}{\numberline {1.4.4}Actor-critic design choises}{18}{subsection.1.4.4}%
\contentsline {subsection}{\numberline {1.4.5}Online actor-critic in practise}{19}{subsection.1.4.5}%
\contentsline {subsection}{\numberline {1.4.6}Critics as state-dependent baselines}{21}{subsection.1.4.6}%
\contentsline {subsubsection}{Eligibility traces and n-step returns}{22}{section*.15}%
\contentsline {subsubsection}{Generalied advantage estimation (GAE)}{22}{section*.16}%
\contentsline {section}{\numberline {1.5}Value function methods}{23}{section.1.5}%
\contentsline {subsubsection}{Can we omit policy gradient completely?}{23}{section*.17}%
\contentsline {subsection}{\numberline {1.5.1}Policy iteration}{23}{subsection.1.5.1}%
\contentsline {subsubsection}{Dynamic programming}{23}{section*.18}%
\contentsline {subsubsection}{Policy iteration with dynamic programming}{23}{section*.19}%
\contentsline {subsubsection}{Even simpler dynamic programming}{23}{section*.20}%
\contentsline {subsubsection}{Fitted value iteration and Q-iteration}{24}{section*.21}%
\contentsline {subsection}{\numberline {1.5.2}From Q-iteration to Q-learning}{25}{subsection.1.5.2}%
\contentsline {subsubsection}{Why is this algorithm off-policy}{25}{section*.22}%
\contentsline {subsection}{\numberline {1.5.3}Value function in theory}{26}{subsection.1.5.3}%
\contentsline {subsubsection}{A sad corollary}{27}{section*.23}%
\contentsline {section}{\numberline {1.6}Deep RL with Q-functions}{27}{section.1.6}%
\contentsline {subsubsection}{Replay buffers}{28}{section*.24}%
\contentsline {subsection}{\numberline {1.6.1}Target networks}{28}{subsection.1.6.1}%
\contentsline {subsection}{\numberline {1.6.2}A general view of Q-learning}{29}{subsection.1.6.2}%
\contentsline {section}{\numberline {1.7}Improving Q-learning}{29}{section.1.7}%
\contentsline {subsubsection}{Are Q-values accurate?}{29}{section*.25}%
\contentsline {subsection}{\numberline {1.7.1}Double Q-learning}{30}{subsection.1.7.1}%
\contentsline {subsubsection}{Double Q-learning in practise}{30}{section*.26}%
\contentsline {subsubsection}{Multi-step returns}{30}{section*.27}%
\contentsline {subsubsection}{Q-learning with N-step returns}{31}{section*.28}%
\contentsline {subsection}{\numberline {1.7.2}Q-learning with continuous actions}{31}{subsection.1.7.2}%
\contentsline {paragraph}{Option 1}{31}{section*.29}%
\contentsline {paragraph}{Option 2}{31}{section*.30}%
\contentsline {paragraph}{Option 3}{32}{section*.31}%
\contentsline {subsubsection}{DDPG}{32}{section*.32}%
\contentsline {subsection}{\numberline {1.7.3}Implementation tips and examples}{32}{subsection.1.7.3}%
\contentsline {paragraph}{Tips}{32}{section*.33}%
\contentsline {section}{\numberline {1.8}Even more advanced policy gradients (PPO and TRPO)}{33}{section.1.8}%
\contentsline {subsubsection}{Policy gradient as policy iteration}{33}{section*.34}%
\contentsline {subsubsection}{Bounding the objective value}{36}{section*.35}%
\contentsline {subsection}{\numberline {1.8.1}Policy gradients with constraints}{37}{subsection.1.8.1}%
\contentsline {subsubsection}{How do we enforce the constraint}{37}{section*.36}%
\contentsline {subsection}{\numberline {1.8.2}Natural gradient}{37}{subsection.1.8.2}%
\contentsline {paragraph}{Do these results carry over in practise?}{39}{section*.37}%
\contentsline {subsection}{\numberline {1.8.3}Practical methods and notes}{39}{subsection.1.8.3}%
\contentsline {paragraph}{Natural policy gradient}{39}{section*.38}%
\contentsline {paragraph}{Trust region policy optimization}{39}{section*.39}%
\contentsline {paragraph}{Just using importance sampling objective directly}{39}{section*.40}%
\contentsline {section}{\numberline {1.9}Optimal control and planning}{39}{section.1.9}%
\contentsline {paragraph}{The objective}{39}{section*.41}%
\contentsline {subsubsection}{Stochastic optimization}{40}{section*.42}%
\contentsline {subsubsection}{Cross-entropy method (CEM)}{40}{section*.43}%
\contentsline {subsubsection}{Discrete case: Monte Carlo tree search (MCTS)}{41}{section*.44}%
\contentsline {subsection}{\numberline {1.9.1}Trajectory optimization with derivatives}{42}{subsection.1.9.1}%
\contentsline {subsubsection}{Linear case: LQR}{42}{section*.45}%
\contentsline {subsection}{\numberline {1.9.2}LQR for stochastic and nonlinear systems}{45}{subsection.1.9.2}%
\contentsline {subsubsection}{Nonlinear case: differential dynamic programming (DDP)/ iterative LQR}{45}{section*.46}%
\contentsline {subsubsection}{Nonlinear model-predictive control}{47}{section*.47}%
\contentsline {section}{\numberline {1.10}Model-based reinforcement learning}{47}{section.1.10}%
\contentsline {subsection}{\numberline {1.10.1}Uncertainty in model-based RL}{49}{subsection.1.10.1}%
\contentsline {subsubsection}{Uncertainty-aware neural network models}{49}{section*.48}%
\contentsline {paragraph}{Idea 1:}{49}{section*.49}%
\contentsline {paragraph}{Idea 2:}{49}{section*.50}%
\contentsline {subsubsection}{Quick overview of Bayesian neural networks}{50}{section*.51}%
\contentsline {subsubsection}{Bootstrap ensembles}{50}{section*.52}%
\contentsline {subsubsection}{How to plan with uncertainty}{51}{section*.53}%
\contentsline {paragraph}{Other options:}{51}{section*.54}%
\contentsline {subsection}{\numberline {1.10.2}Model-based reinforcement learning with images}{52}{subsection.1.10.2}%
\contentsline {subsubsection}{State space (latent space models)}{52}{section*.55}%
\contentsline {section}{\numberline {1.11}Model-based policy learning}{53}{section.1.11}%
\contentsline {subsubsection}{What's the solution?}{55}{section*.56}%
\contentsline {paragraph}{First class of solutions}{55}{section*.57}%
\contentsline {paragraph}{Second class of solutions}{55}{section*.58}%
\contentsline {subsection}{\numberline {1.11.1}Model-free learning with a model}{55}{subsection.1.11.1}%
\contentsline {subsubsection}{Dyna}{55}{section*.59}%
\contentsline {paragraph}{General ``Dyna-style'' model-based RL recipe}{56}{section*.60}%
\contentsline {subsubsection}{Local models}{57}{section*.61}%
\contentsline {section}{\numberline {1.12}Exploration algorithms}{57}{section.1.12}%
\contentsline {subsubsection}{Optimistic exploration}{57}{section*.62}%
\contentsline {subsubsection}{Probability matching/posterior sampling}{57}{section*.63}%
\contentsline {subsubsection}{Information gain}{57}{section*.64}%
\contentsline {subsubsection}{General themes}{58}{section*.65}%
\contentsline {subsection}{\numberline {1.12.1}Exploration in deep reinforcement learning}{58}{subsection.1.12.1}%
\contentsline {subsubsection}{Fitting generative models}{59}{section*.66}%
\contentsline {subsubsection}{What kind of bonus to use?}{59}{section*.67}%
\contentsline {paragraph}{What kind of model to use?}{60}{section*.68}%
\contentsline {subsection}{\numberline {1.12.2}Posterior sampling in deep RL}{60}{subsection.1.12.2}%
\contentsline {subsection}{\numberline {1.12.3}Information gain in DRL}{60}{subsection.1.12.3}%
\contentsline {subsection}{\numberline {1.12.4}Exploration with model errors}{61}{subsection.1.12.4}%
\contentsline {subsection}{\numberline {1.12.5}Unsupervised exploration}{61}{subsection.1.12.5}%
\contentsline {subsubsection}{Information theoretic quantities in RL}{62}{section*.69}%
\contentsline {section}{\numberline {1.13}Unsupervised reinforcement learning (sketches)}{62}{section.1.13}%
\contentsline {subsubsection}{Aside: exploration with intrinsic motivation}{63}{section*.70}%
\contentsline {subsection}{\numberline {1.13.1}Learning diverse skills}{64}{subsection.1.13.1}%
\contentsline {subsubsection}{Diversity-promoting reward function}{64}{section*.71}%
\contentsline {section}{\numberline {1.14}Generalisation gap}{64}{section.1.14}%
\contentsline {paragraph}{What makes modern machine learning work}{64}{section*.72}%
\contentsline {subsubsection}{Why is offline RL hard?}{65}{section*.73}%
\contentsline {subsubsection}{Where does RL suffer from distributional shift?}{66}{section*.74}%
\contentsline {subsection}{\numberline {1.14.1}Batch RL via importance sampling}{66}{subsection.1.14.1}%
\contentsline {subsubsection}{The doubly robust estimator}{68}{section*.75}%
\contentsline {subsubsection}{Marginalized importance sampling}{69}{section*.76}%
\contentsline {subsection}{\numberline {1.14.2}Batch RL via linear fitted value functions}{69}{subsection.1.14.2}%
\contentsline {section}{\numberline {1.15}Reinforcement learning as an inference problem}{69}{section.1.15}%
\contentsline {subsection}{\numberline {1.15.1}Optimal control as a model of human behavior}{70}{subsection.1.15.1}%
\contentsline {subsection}{\numberline {1.15.2}Control as inference}{71}{subsection.1.15.2}%
\contentsline {subsubsection}{Backward messages}{71}{section*.77}%
\contentsline {subsubsection}{But what the if the action prior is not uniform?}{73}{section*.78}%
\contentsline {subsection}{\numberline {1.15.3}Policy computation}{73}{subsection.1.15.3}%
\contentsline {subsection}{\numberline {1.15.4}Forward messages}{74}{subsection.1.15.4}%

\BOOKMARK [0][-]{chapter.1}{Plan}{}% 1
\BOOKMARK [1][-]{section.1.1}{TODOs}{chapter.1}% 2
\BOOKMARK [0][-]{chapter.2}{Immitation learning}{}% 3
\BOOKMARK [0][-]{chapter.3}{Formal setting}{}% 4
\BOOKMARK [1][-]{section.3.1}{Markov chain}{chapter.3}% 5
\BOOKMARK [1][-]{section.3.2}{Markov decision process}{chapter.3}% 6
\BOOKMARK [1][-]{section.3.3}{Partially observed Markov decision process}{chapter.3}% 7
\BOOKMARK [2][-]{subsection.3.3.1}{The goal of reinforcement learning}{section.3.3}% 8
\BOOKMARK [1][-]{section.3.4}{Value functions}{chapter.3}% 9
\BOOKMARK [0][-]{chapter.4}{Policy gradients}{}% 10
\BOOKMARK [1][-]{section.4.1}{Reducing variance}{chapter.4}% 11
\BOOKMARK [2][-]{subsection.4.1.1}{Baselines}{section.4.1}% 12
\BOOKMARK [1][-]{section.4.2}{Off-policy gradients}{chapter.4}% 13
\BOOKMARK [2][-]{subsection.4.2.1}{Policy gradient with automatic differentiation}{section.4.2}% 14
\BOOKMARK [2][-]{subsection.4.2.2}{Policy gradients in practice}{section.4.2}% 15
\BOOKMARK [1][-]{section.4.3}{Advanced policy gradients}{chapter.4}% 16
\BOOKMARK [0][-]{chapter.5}{Actor-critic algorithms}{}% 17
\BOOKMARK [1][-]{section.5.1}{Policy evaluation}{chapter.5}% 18
\BOOKMARK [1][-]{section.5.2}{From evaluation to actor-critic}{chapter.5}% 19
\BOOKMARK [1][-]{section.5.3}{Aside: discount factors}{chapter.5}% 20
\BOOKMARK [1][-]{section.5.4}{Actor-critic design choises}{chapter.5}% 21
\BOOKMARK [1][-]{section.5.5}{Online actor-critic in practise}{chapter.5}% 22
\BOOKMARK [1][-]{section.5.6}{Critics as state-dependent baselines}{chapter.5}% 23
\BOOKMARK [2][-]{subsection.5.6.1}{Eligibility traces and n-step returns}{section.5.6}% 24
\BOOKMARK [2][-]{subsection.5.6.2}{Generalied advantage estimation \(GAE\)}{section.5.6}% 25
\BOOKMARK [0][-]{chapter.6}{Value function methods}{}% 26
\BOOKMARK [1][-]{subsection.6.0.1}{Can we omit policy gradient completely?}{chapter.6}% 27
\BOOKMARK [1][-]{section.6.1}{Policy iteration}{chapter.6}% 28
\BOOKMARK [2][-]{subsection.6.1.1}{Dynamic programming}{section.6.1}% 29
\BOOKMARK [2][-]{subsection.6.1.2}{Policy iteration with dynamic programming}{section.6.1}% 30
\BOOKMARK [2][-]{subsection.6.1.3}{Even simpler dynamic programming}{section.6.1}% 31
\BOOKMARK [2][-]{subsection.6.1.4}{Fitted value iteration and Q-iteration}{section.6.1}% 32
\BOOKMARK [1][-]{section.6.2}{From Q-iteration to Q-learning}{chapter.6}% 33
\BOOKMARK [2][-]{subsection.6.2.1}{Why is this algorithm off-policy}{section.6.2}% 34
\BOOKMARK [1][-]{section.6.3}{Value function in theory}{chapter.6}% 35
\BOOKMARK [2][-]{subsection.6.3.1}{A sad corollary}{section.6.3}% 36
\BOOKMARK [0][-]{chapter.7}{Deep RL with Q-functions}{}% 37
\BOOKMARK [1][-]{subsection.7.0.1}{Replay buffers}{chapter.7}% 38
\BOOKMARK [1][-]{section.7.1}{Target networks}{chapter.7}% 39
\BOOKMARK [1][-]{section.7.2}{A general view of Q-learning}{chapter.7}% 40
\BOOKMARK [0][-]{chapter.8}{Improving Q-learning}{}% 41
\BOOKMARK [1][-]{subsection.8.0.1}{Are Q-values accurate?}{chapter.8}% 42
\BOOKMARK [1][-]{section.8.1}{Double Q-learning}{chapter.8}% 43
\BOOKMARK [2][-]{subsection.8.1.1}{Double Q-learning in practise}{section.8.1}% 44
\BOOKMARK [2][-]{subsection.8.1.2}{Multi-step returns}{section.8.1}% 45
\BOOKMARK [2][-]{subsection.8.1.3}{Q-learning with N-step returns}{section.8.1}% 46
\BOOKMARK [1][-]{section.8.2}{Q-learning with continuous actions}{chapter.8}% 47
\BOOKMARK [2][-]{subsection.8.2.1}{DDPG}{section.8.2}% 48
\BOOKMARK [1][-]{section.8.3}{Implementation tips and examples}{chapter.8}% 49
\BOOKMARK [0][-]{chapter.9}{Even more advanced policy gradients \(PPO and TRPO\)}{}% 50
\BOOKMARK [1][-]{subsection.9.0.1}{Policy gradient as policy iteration}{chapter.9}% 51
\BOOKMARK [2][-]{subsection.9.0.2}{Bounding the objective value}{subsection.9.0.1}% 52
\BOOKMARK [1][-]{section.9.1}{Policy gradients with constraints}{chapter.9}% 53
\BOOKMARK [2][-]{subsection.9.1.1}{How do we enforce the constraint}{section.9.1}% 54
\BOOKMARK [1][-]{section.9.2}{Natural gradient}{chapter.9}% 55
\BOOKMARK [1][-]{section.9.3}{Practical methods and notes}{chapter.9}% 56
\BOOKMARK [0][-]{chapter.10}{Optimal control and planning}{}% 57
\BOOKMARK [1][-]{subsection.10.0.1}{Stochastic optimization}{chapter.10}% 58
\BOOKMARK [2][-]{subsection.10.0.2}{Cross-entropy method \(CEM\)}{subsection.10.0.1}% 59
\BOOKMARK [2][-]{subsection.10.0.3}{Discrete case: Monte Carlo tree search \(MCTS\)}{subsection.10.0.1}% 60
\BOOKMARK [1][-]{section.10.1}{Trajectory optimization with derivatives}{chapter.10}% 61
\BOOKMARK [2][-]{subsection.10.1.1}{Linear case: LQR}{section.10.1}% 62
\BOOKMARK [1][-]{section.10.2}{LQR for stochastic and nonlinear systems}{chapter.10}% 63
\BOOKMARK [2][-]{subsection.10.2.1}{Nonlinear case: differential dynamic programming \(DDP\)/ iterative LQR}{section.10.2}% 64
\BOOKMARK [2][-]{subsection.10.2.2}{Nonlinear model-predictive control}{section.10.2}% 65
\BOOKMARK [0][-]{chapter.11}{Model-based reinforcement learning}{}% 66
\BOOKMARK [1][-]{section.11.1}{Uncertainty in model-based RL}{chapter.11}% 67
\BOOKMARK [2][-]{subsection.11.1.1}{Uncertainty-aware neural network models}{section.11.1}% 68
\BOOKMARK [2][-]{subsection.11.1.2}{Quick overview of Bayesian neural networks}{section.11.1}% 69
\BOOKMARK [2][-]{subsection.11.1.3}{Bootstrap ensembles}{section.11.1}% 70
\BOOKMARK [2][-]{subsection.11.1.4}{How to plan with uncertainty}{section.11.1}% 71
\BOOKMARK [1][-]{section.11.2}{Model-based reinforcement learning with images}{chapter.11}% 72
\BOOKMARK [2][-]{subsection.11.2.1}{State space \(latent space models\)}{section.11.2}% 73
\BOOKMARK [0][-]{chapter.12}{Model-based policy learning}{}% 74
\BOOKMARK [1][-]{subsection.12.0.1}{What's the solution?}{chapter.12}% 75
\BOOKMARK [1][-]{section.12.1}{Model-free learning with a model}{chapter.12}% 76
\BOOKMARK [2][-]{subsection.12.1.1}{Dyna}{section.12.1}% 77
\BOOKMARK [2][-]{subsection.12.1.2}{Local models}{section.12.1}% 78
\BOOKMARK [0][-]{chapter.13}{Exploration algorithms}{}% 79
\BOOKMARK [1][-]{subsection.13.0.1}{Optimistic exploration}{chapter.13}% 80
\BOOKMARK [2][-]{subsection.13.0.2}{Probability matching/posterior sampling}{subsection.13.0.1}% 81
\BOOKMARK [2][-]{subsection.13.0.3}{Information gain}{subsection.13.0.1}% 82
\BOOKMARK [2][-]{subsection.13.0.4}{General themes}{subsection.13.0.1}% 83
\BOOKMARK [1][-]{section.13.1}{Exploration in deep reinforcement learning}{chapter.13}% 84
\BOOKMARK [2][-]{subsection.13.1.1}{Fitting generative models}{section.13.1}% 85
\BOOKMARK [2][-]{subsection.13.1.2}{What kind of bonus to use?}{section.13.1}% 86
\BOOKMARK [1][-]{section.13.2}{Posterior sampling in deep RL}{chapter.13}% 87
\BOOKMARK [1][-]{section.13.3}{Information gain in DRL}{chapter.13}% 88
\BOOKMARK [1][-]{section.13.4}{Exploration with model errors}{chapter.13}% 89
\BOOKMARK [1][-]{section.13.5}{Unsupervised exploration}{chapter.13}% 90
\BOOKMARK [2][-]{subsection.13.5.1}{Information theoretic quantities in RL}{section.13.5}% 91
\BOOKMARK [0][-]{chapter.14}{Unsupervised reinforcement learning \(sketches\)}{}% 92
\BOOKMARK [1][-]{subsection.14.0.1}{Aside: exploration with intrinsic motivation}{chapter.14}% 93
\BOOKMARK [1][-]{section.14.1}{Learning diverse skills}{chapter.14}% 94
\BOOKMARK [2][-]{subsection.14.1.1}{Diversity-promoting reward function}{section.14.1}% 95
\BOOKMARK [0][-]{chapter.15}{Generalisation gap}{}% 96
\BOOKMARK [1][-]{subsection.15.0.1}{Why is offline RL hard?}{chapter.15}% 97
\BOOKMARK [2][-]{subsection.15.0.2}{Where does RL suffer from distributional shift?}{subsection.15.0.1}% 98
\BOOKMARK [1][-]{section.15.1}{Batch RL via importance sampling}{chapter.15}% 99
\BOOKMARK [2][-]{subsection.15.1.1}{The doubly robust estimator}{section.15.1}% 100
\BOOKMARK [2][-]{subsection.15.1.2}{Marginalized importance sampling}{section.15.1}% 101
\BOOKMARK [1][-]{section.15.2}{Batch RL via linear fitted value functions}{chapter.15}% 102
\BOOKMARK [0][-]{chapter.16}{Reinforcement learning as an inference problem}{}% 103
\BOOKMARK [1][-]{section.16.1}{Optimal control as a model of human behavior}{chapter.16}% 104
\BOOKMARK [1][-]{section.16.2}{Control as inference}{chapter.16}% 105
\BOOKMARK [2][-]{subsection.16.2.1}{Backward messages}{section.16.2}% 106
\BOOKMARK [2][-]{subsection.16.2.2}{But what the if the action prior is not uniform?}{section.16.2}% 107
\BOOKMARK [1][-]{section.16.3}{Policy computation}{chapter.16}% 108
\BOOKMARK [1][-]{section.16.4}{Forward messages}{chapter.16}% 109
\BOOKMARK [1][-]{section.16.5}{Control as variational inference}{chapter.16}% 110
\BOOKMARK [2][-]{subsection.16.5.1}{Variational lower bound}{section.16.5}% 111
\BOOKMARK [2][-]{subsection.16.5.2}{Optimizing the variational lower bound}{section.16.5}% 112
\BOOKMARK [1][-]{section.16.6}{Algorithms for RL as inference}{chapter.16}% 113
\BOOKMARK [2][-]{subsection.16.6.1}{Q-learning with soft optimality}{section.16.6}% 114
\BOOKMARK [2][-]{subsection.16.6.2}{Policy gradient with soft optimality}{section.16.6}% 115
\BOOKMARK [2][-]{subsection.16.6.3}{Policy gradient vs Q-learning}{section.16.6}% 116
\BOOKMARK [2][-]{subsection.16.6.4}{Benefits of soft optimality}{section.16.6}% 117
\BOOKMARK [2][-]{subsection.16.6.5}{Example methods}{section.16.6}% 118
\BOOKMARK [0][-]{chapter.17}{Inverse reinforcement learning}{}% 119
\BOOKMARK [1][-]{subsection.17.0.1}{Why should we worry about learning rewards?}{chapter.17}% 120
\BOOKMARK [2][-]{subsection.17.0.2}{Feature matching IRL}{subsection.17.0.1}% 121
\BOOKMARK [2][-]{subsection.17.0.3}{Learning the optimality variable}{subsection.17.0.1}% 122

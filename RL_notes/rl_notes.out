\BOOKMARK [1][-]{section.0.1}{Plan}{}% 1
\BOOKMARK [2][-]{subsection.0.1.1}{TODOs}{section.0.1}% 2
\BOOKMARK [0][-]{chapter.1}{Berkley AI class}{}% 3
\BOOKMARK [1][-]{section.1.1}{Immitation learning}{chapter.1}% 4
\BOOKMARK [1][-]{section.1.2}{Formal setting}{chapter.1}% 5
\BOOKMARK [2][-]{subsection.1.2.1}{Markov chain}{section.1.2}% 6
\BOOKMARK [2][-]{subsection.1.2.2}{Markov decision process}{section.1.2}% 7
\BOOKMARK [2][-]{subsection.1.2.3}{Partially observed Markov decision process}{section.1.2}% 8
\BOOKMARK [2][-]{subsection.1.2.4}{Value functions}{section.1.2}% 9
\BOOKMARK [1][-]{section.1.3}{Policy gradients}{chapter.1}% 10
\BOOKMARK [2][-]{subsection.1.3.1}{Reducing variance}{section.1.3}% 11
\BOOKMARK [2][-]{subsection.1.3.2}{Off-policy gradients}{section.1.3}% 12
\BOOKMARK [2][-]{subsection.1.3.3}{Advanced policy gradients}{section.1.3}% 13
\BOOKMARK [1][-]{section.1.4}{Actor-critic algorithms}{chapter.1}% 14
\BOOKMARK [2][-]{subsection.1.4.1}{Policy evaluation}{section.1.4}% 15
\BOOKMARK [2][-]{subsection.1.4.2}{From evaluation to actor-critic}{section.1.4}% 16
\BOOKMARK [2][-]{subsection.1.4.3}{Aside: discount factors}{section.1.4}% 17
\BOOKMARK [2][-]{subsection.1.4.4}{Actor-critic design choises}{section.1.4}% 18
\BOOKMARK [2][-]{subsection.1.4.5}{Online actor-critic in practise}{section.1.4}% 19
\BOOKMARK [2][-]{subsection.1.4.6}{Critics as state-dependent baselines}{section.1.4}% 20
\BOOKMARK [1][-]{section.1.5}{Value function methods}{chapter.1}% 21
\BOOKMARK [2][-]{subsection.1.5.1}{Policy iteration}{section.1.5}% 22
\BOOKMARK [2][-]{subsection.1.5.2}{From Q-iteration to Q-learning}{section.1.5}% 23
\BOOKMARK [2][-]{subsection.1.5.3}{Value function in theory}{section.1.5}% 24
\BOOKMARK [1][-]{section.1.6}{Deep RL with Q-functions}{chapter.1}% 25
\BOOKMARK [2][-]{subsection.1.6.1}{Target networks}{section.1.6}% 26
\BOOKMARK [2][-]{subsection.1.6.2}{A general view of Q-learning}{section.1.6}% 27
\BOOKMARK [1][-]{section.1.7}{Improving Q-learning}{chapter.1}% 28
\BOOKMARK [2][-]{subsection.1.7.1}{Double Q-learning}{section.1.7}% 29
\BOOKMARK [2][-]{subsection.1.7.2}{Q-learning with continuous actions}{section.1.7}% 30
\BOOKMARK [2][-]{subsection.1.7.3}{Implementation tips and examples}{section.1.7}% 31
\BOOKMARK [1][-]{section.1.8}{Even more advanced policy gradients \(PPO and TRPO\)}{chapter.1}% 32
\BOOKMARK [1][-]{section.1.9}{Optimal control and planning}{chapter.1}% 33
\BOOKMARK [2][-]{subsection.1.9.1}{Trajectory optimization with derivatives}{section.1.9}% 34
\BOOKMARK [1][-]{section.1.10}{Model-based reinforcement learning}{chapter.1}% 35
\BOOKMARK [2][-]{subsection.1.10.1}{Uncertainty in model-based RL}{section.1.10}% 36
\BOOKMARK [2][-]{subsection.1.10.2}{Model-based reinforcement learning with images}{section.1.10}% 37
\BOOKMARK [1][-]{section.1.11}{Model-based policy learning}{chapter.1}% 38
\BOOKMARK [2][-]{subsection.1.11.1}{Model-free learning with a model}{section.1.11}% 39
\BOOKMARK [1][-]{section.1.12}{Exploration algorithms}{chapter.1}% 40
\BOOKMARK [2][-]{subsection.1.12.1}{Exploration in deep reinforcement learning}{section.1.12}% 41
\BOOKMARK [2][-]{subsection.1.12.2}{Posterior sampling in deep RL}{section.1.12}% 42
\BOOKMARK [2][-]{subsection.1.12.3}{Information gain in DRL}{section.1.12}% 43
\BOOKMARK [2][-]{subsection.1.12.4}{Exploration with model errors}{section.1.12}% 44
\BOOKMARK [2][-]{subsection.1.12.5}{Unsupervised exploration}{section.1.12}% 45
\BOOKMARK [1][-]{section.1.13}{Unsupervised reinforcement learning \(sketches\)}{chapter.1}% 46
\BOOKMARK [2][-]{subsection.1.13.1}{Learning diverse skills}{section.1.13}% 47
\BOOKMARK [1][-]{section.1.14}{Generalisation gap}{chapter.1}% 48
\BOOKMARK [2][-]{subsection.1.14.1}{Batch RL via importance sampling}{section.1.14}% 49

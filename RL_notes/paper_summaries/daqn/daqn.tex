\documentclass{article}
\usepackage{amsmath}
\newcommand{\argmin}{\arg\!\min} 
\newcommand{\argmax}{\arg\!\max} 
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage[T1]{fontenc}
\usepackage{bm}
\usepackage{array}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}

\title{Paper summary: DAQN: deep autoencoder and q-network}
\date{date}
\author{Marko Guberina 19970821-1256}

\begin{document}
\maketitle




\section{Idea in few sentances}


\section{Explanation of the central concept}




\section{Methodology}


\section{Initial rambly notes}
Has 19 citations so kinda lame. The papers that cited it have even lower citations.
But it seems like it's exactly what we're doing now and
it's good to see what a run-of-the-mill paper in the field looks like.
The paper is literred with bad grammar at worst and
a lot of plainly bad language at best.


\subsection{Abstract}
Just running RL on real robotics entails
requiring more samples and dealing with noise.
Sticking a ``generative model'' (an autoencoder in this case (how is ae generative???)) to initialize the network
should reduce the number of trials.
Learning was 2.5 times faster on a task on a real robot.

\subsection{Introduction}
Autoencoders can be pretrained when taking random actions.
That's due to the fact that they work well for unsupervised learning.
There are some studies which use convolutional autoencoders 
to pretrain convolutional layers for general supervised learning tasks.
\paragraph{The hypothesis} is that pretrainining an autoencoder will yield better
performance as it is much easier to train an autoencoder on just observations
in an unsupervised manner than it is to collect rewards and optimal actions
and use a loss generated on them.
In short, unsupervised learning is easier than plain RL so let's 
leverage that to boost RL.


\subsection{Method}
\fbox{
\parbox{\textwidth}{
		\underline{The method consists of the following steps:}
\begin{enumerate}
		\item train a deep autoencoder using unsupervised learning
		\item delete the decoder layers and add a fully-connected layer on top of encoder layers
		\item use that network as a starting point for a policiy train with deep q-learning
\end{enumerate}
}} 
In math notation we have:
\begin{enumerate}
		\item let $ \mathcal{X} = \mathbb{R}^{ n }, \mathcal{Y} = \mathbb{Y}^{ n }, \bm{x}_{} \in \mathcal{X}  $
		\item encoder: $ \phi : \mathcal{X} \to \mathcal{Y}  $
		\item decoder: $ \psi : \mathcal{Y} \to \mathcal{X}  $
		\item training goal: $ \phi^{ * }, \psi^{ * } = \argmin_{\phi, \psi} || \bm{x}_{} \circ (\psi + \phi) \bm{x}_{}||^{ 2 }  $
\end{enumerate}

The samples for the autoencoder are generated by taking random actions in the environment.
Once it's been trained, the decoder is removed, randomly initialized FC layers are 
added to the encoder and this betwork becomes the Q-network train in the standard Q-learning fashion.
Namely, the update function of Q is:
\begin{equation}
		Q (\bm{s}_{t}, \bm{a}_{t} ) \leftarrow 
		Q (\bm{s}_{t}, \bm{a}_{t} ) + \alpha \left( r_{ t+1 } + \gamma \max_{\bm{a}_{}} Q (\bm{s}_{t+1}, \bm{a}_{t} ) - Q (\bm{s}_{t}, \bm{a}_{t} ) \right) 
\end{equation}
wher $ \max_{\bm{a}_{}} Q (\bm{s}_{t+1}, \bm{a}_{t} )  $ is the maximum estimate action-value
for state $ \bm{s}_{t+1}  $.
The loss of the deep q-network is:
\begin{align}
L (\theta) &= E_{ (\bm{s}_{t}, \bm{a}_{t} , r_{ t+1 },\bm{s}_{t+1}  ) \sim \mathcal{D} } 
\left[ (y - Q (\bm{s}_{t}, \bm{a}_{t} ; \theta))^{ 2 } \right] \\
y&= \left\{  
		\begin{array}{ll}
				r_{ t+1 } & \text{terminal}\\
				r_{ t+1 } + \gamma \max_{\bm{a}_{}} Q (\bm{s}_{t+1}, \bm{a}_{}; \theta^{ - }) & \text{non terminal}
\end{array}
		\right.
\end{align}



\subsection{Other stuff}
\subsubsection{Results}
Are totally trash. They got practically no benefits when compared to plain deep Q-learning.
The time they got a better result was for the ``paper-rock-scissors'' game
in which the autoencoder was actually a classifier.
They didn't even use human-baseline scores as a comparisson, but
winning ratios in the game, which is very stupid metric because it's quite imprecise
when comparing well-performing agents.

Also, how the heck did you choose paper-rock-scissors as a game!?!??!?!?
The mixed Nash equilibrium, i.e. the game theoretic solution is to literally play
uniformly randomly!!
So i guess the agent makes its move after the being presented with an image.
But then that's not the paper-rock-scissors game everyone knows!
Absolutely horrible. Who published this?
Even more importantly, how did this get 16 citations?!?!?!?

The paper belongs to the traaaaash.





\end{document}

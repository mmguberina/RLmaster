\BOOKMARK [0][-]{chapter*.3}{List of Figures}{}% 1
\BOOKMARK [0][-]{chapter*.4}{List of Tables}{}% 2
\BOOKMARK [0][-]{chapter.1}{Introduction}{}% 3
\BOOKMARK [1][-]{section.1.1}{What is reinforcement learning?}{chapter.1}% 4
\BOOKMARK [1][-]{section.1.2}{Why is reinforcement learning interesting?}{chapter.1}% 5
\BOOKMARK [1][-]{section.1.3}{Why learn from pixels?}{chapter.1}% 6
\BOOKMARK [1][-]{section.1.4}{Efforts to make reinforcement learning more efficient}{chapter.1}% 7
\BOOKMARK [2][-]{subsection.1.4.1}{Utilizing a world model}{section.1.4}% 8
\BOOKMARK [2][-]{subsection.1.4.2}{Utilizing state representations}{section.1.4}% 9
\BOOKMARK [1][-]{section.1.5}{Goal of the thesis}{chapter.1}% 10
\BOOKMARK [2][-]{subsection.1.5.1}{Hypothesis}{section.1.5}% 11
\BOOKMARK [2][-]{subsection.1.5.2}{Contributions}{section.1.5}% 12
\BOOKMARK [1][-]{section.1.6}{Outline}{chapter.1}% 13
\BOOKMARK [0][-]{chapter.2}{Reinforcement learning}{}% 14
\BOOKMARK [1][-]{section.2.1}{Problem setting}{chapter.2}% 15
\BOOKMARK [2][-]{subsection.2.1.1}{Bandit problems}{section.2.1}% 16
\BOOKMARK [2][-]{subsection.2.1.2}{Markov Decision Processes}{section.2.1}% 17
\BOOKMARK [1][-]{section.2.2}{Key concepts in reinforcement learning}{chapter.2}% 18
\BOOKMARK [2][-]{subsection.2.2.1}{Policy}{section.2.2}% 19
\BOOKMARK [2][-]{subsection.2.2.2}{Goal of reinforcement learning}{section.2.2}% 20
\BOOKMARK [2][-]{subsection.2.2.3}{Value functions}{section.2.2}% 21
\BOOKMARK [1][-]{section.2.3}{Classes of reinforcement learning algorithms}{chapter.2}% 22
\BOOKMARK [2][-]{subsection.2.3.1}{Policy gradients}{section.2.3}% 23
\BOOKMARK [3][-]{subsubsection.2.3.1.1}{Baselines}{subsection.2.3.1}% 24
\BOOKMARK [3][-]{subsubsection.2.3.1.2}{Off-policy gradients}{subsection.2.3.1}% 25
\BOOKMARK [3][-]{subsubsection.2.3.1.3}{Advanced policy gradients}{subsection.2.3.1}% 26
\BOOKMARK [2][-]{subsection.2.3.2}{Actor-critic algorithms}{section.2.3}% 27
\BOOKMARK [2][-]{subsection.2.3.3}{Value function methods}{section.2.3}% 28
\BOOKMARK [3][-]{subsubsection.2.3.3.1}{Dynamic programming}{subsection.2.3.3}% 29
\BOOKMARK [1][-]{section.2.4}{Deep Q-networks and their extensions}{chapter.2}% 30
\BOOKMARK [2][-]{subsection.2.4.1}{Double Deep Q-networks: DDQN}{section.2.4}% 31
\BOOKMARK [2][-]{subsection.2.4.2}{Prioritized replay}{section.2.4}% 32
\BOOKMARK [2][-]{subsection.2.4.3}{Dueling Network}{section.2.4}% 33
\BOOKMARK [2][-]{subsection.2.4.4}{Multi-step learning}{section.2.4}% 34
\BOOKMARK [2][-]{subsection.2.4.5}{Noisy Nets}{section.2.4}% 35
\BOOKMARK [2][-]{subsection.2.4.6}{Integrated Agent:Rainbow}{section.2.4}% 36
\BOOKMARK [2][-]{subsection.2.4.7}{Deep autoencoders}{section.2.4}% 37
\BOOKMARK [1][-]{section.2.5}{Problems with RL}{chapter.2}% 38
\BOOKMARK [0][-]{chapter.3}{State representation learning}{}% 39
\BOOKMARK [1][-]{section.3.1}{Representation models in general}{chapter.3}% 40
\BOOKMARK [2][-]{subsection.3.1.1}{Generative models}{section.3.1}% 41
\BOOKMARK [3][-]{subsubsection.3.1.1.1}{Probabilistic models}{subsection.3.1.1}% 42
\BOOKMARK [3][-]{subsubsection.3.1.1.2}{Directed graphical models}{subsection.3.1.1}% 43
\BOOKMARK [3][-]{subsubsection.3.1.1.3}{Directly learning a parametric map from input to representation}{subsection.3.1.1}% 44
\BOOKMARK [2][-]{subsection.3.1.2}{Discriminative models}{section.3.1}% 45
\BOOKMARK [2][-]{subsection.3.1.3}{Common representation learning approaches}{section.3.1}% 46
\BOOKMARK [3][-]{subsubsection.3.1.3.1}{Deterministic autoencoders}{subsection.3.1.3}% 47
\BOOKMARK [3][-]{subsubsection.3.1.3.2}{Variational autoencoders}{subsection.3.1.3}% 48
\BOOKMARK [3][-]{subsubsection.3.1.3.3}{Deterministic autoencoder regularization}{subsection.3.1.3}% 49
\BOOKMARK [1][-]{section.3.2}{Representation models for control}{chapter.3}% 50
\BOOKMARK [2][-]{subsection.3.2.1}{Autoencoder}{section.3.2}% 51
\BOOKMARK [2][-]{subsection.3.2.2}{Forward model}{section.3.2}% 52
\BOOKMARK [2][-]{subsection.3.2.3}{Inverse model}{section.3.2}% 53
\BOOKMARK [2][-]{subsection.3.2.4}{Using prior knowledge to constrain the state space}{section.3.2}% 54
\BOOKMARK [2][-]{subsection.3.2.5}{Using hybring objectives}{section.3.2}% 55
\BOOKMARK [1][-]{section.3.3}{Model-based reinforcement learning}{chapter.3}% 56
\BOOKMARK [0][-]{chapter.4}{Related Work}{}% 57
\BOOKMARK [1][-]{section.4.1}{Reinforcement learning on Atari}{chapter.4}% 58
\BOOKMARK [1][-]{section.4.2}{Efforts in increasing efficiency in Atari}{chapter.4}% 59
\BOOKMARK [1][-]{section.4.3}{State representation learning for efficient model-free learning}{chapter.4}% 60
\BOOKMARK [2][-]{subsection.4.3.1}{Deterministic generative models}{section.4.3}% 61
\BOOKMARK [2][-]{subsection.4.3.2}{Stochastic generative models}{section.4.3}% 62
\BOOKMARK [2][-]{subsection.4.3.3}{Discriminative models}{section.4.3}% 63
\BOOKMARK [0][-]{chapter.5}{Methods}{}% 64
\BOOKMARK [1][-]{section.5.1}{Formulating our hypotheses}{chapter.5}% 65
\BOOKMARK [1][-]{section.5.2}{Our approach}{chapter.5}% 66
\BOOKMARK [1][-]{section.5.3}{Environment and Preprocessing}{chapter.5}% 67
\BOOKMARK [1][-]{section.5.4}{Implementation}{chapter.5}% 68
\BOOKMARK [2][-]{subsection.5.4.1}{Tianshou}{section.5.4}% 69
\BOOKMARK [2][-]{subsection.5.4.2}{Trainer}{section.5.4}% 70
\BOOKMARK [2][-]{subsection.5.4.3}{Implementing state representation learning in Tianshou}{section.5.4}% 71
\BOOKMARK [1][-]{section.5.5}{Hyperparameters}{chapter.5}% 72
\BOOKMARK [0][-]{chapter.6}{Results}{}% 73
\BOOKMARK [0][-]{chapter.7}{Conclusion}{}% 74
\BOOKMARK [0][-]{chapter.7}{Bibliography}{}% 75
\BOOKMARK [0][-]{appendix.A}{Appendix 1}{}% 76

\BOOKMARK [0][-]{chapter*.3}{List of Figures}{}% 1
\BOOKMARK [0][-]{chapter*.4}{List of Tables}{}% 2
\BOOKMARK [0][-]{chapter.1}{Introduction}{}% 3
\BOOKMARK [0][-]{chapter.2}{Background}{}% 4
\BOOKMARK [1][-]{section.2.1}{Introduction to reinforcement learning}{chapter.2}% 5
\BOOKMARK [2][-]{subsection.2.1.1}{Problem setting}{section.2.1}% 6
\BOOKMARK [2][-]{subsection.2.1.2}{Bandit problems}{section.2.1}% 7
\BOOKMARK [2][-]{subsection.2.1.3}{Markov Decision Processes}{section.2.1}% 8
\BOOKMARK [2][-]{subsection.2.1.4}{Key concepts in reinforcement learning}{section.2.1}% 9
\BOOKMARK [3][-]{subsubsection.2.1.4.1}{Policy}{subsection.2.1.4}% 10
\BOOKMARK [3][-]{subsubsection.2.1.4.2}{Goal of reinforcement learning}{subsection.2.1.4}% 11
\BOOKMARK [3][-]{subsubsection.2.1.4.3}{Value functions}{subsection.2.1.4}% 12
\BOOKMARK [1][-]{section.2.2}{Classes of reinforcement learning algorithms}{chapter.2}% 13
\BOOKMARK [2][-]{subsection.2.2.1}{Policy gradients}{section.2.2}% 14
\BOOKMARK [3][-]{subsubsection.2.2.1.1}{Baselines}{subsection.2.2.1}% 15
\BOOKMARK [3][-]{subsubsection.2.2.1.2}{Off-policy gradients}{subsection.2.2.1}% 16
\BOOKMARK [4][-]{paragraph.2.2.1.2.1}{Advanced policy gradient}{subsubsection.2.2.1.2}% 17
\BOOKMARK [2][-]{subsection.2.2.2}{Actor-critic algorithms}{section.2.2}% 18
\BOOKMARK [2][-]{subsection.2.2.3}{Value function methods}{section.2.2}% 19
\BOOKMARK [3][-]{subsubsection.2.2.3.1}{Dynamic programming}{subsection.2.2.3}% 20
\BOOKMARK [1][-]{section.2.3}{Deep Learning }{chapter.2}% 21
\BOOKMARK [2][-]{subsection.2.3.1}{Auto Encoders : Preliminary location}{section.2.3}% 22
\BOOKMARK [1][-]{section.2.4}{Deep Reinforcement Learning and DQN}{chapter.2}% 23
\BOOKMARK [2][-]{subsection.2.4.1}{Extension of DQN }{section.2.4}% 24
\BOOKMARK [3][-]{subsubsection.2.4.1.1}{Double Deep Q-networks: DDQN}{subsection.2.4.1}% 25
\BOOKMARK [3][-]{subsubsection.2.4.1.2}{Prioritized replay}{subsection.2.4.1}% 26
\BOOKMARK [3][-]{subsubsection.2.4.1.3}{Dueling Network}{subsection.2.4.1}% 27
\BOOKMARK [3][-]{subsubsection.2.4.1.4}{Multi-step learning}{subsection.2.4.1}% 28
\BOOKMARK [3][-]{subsubsection.2.4.1.5}{Noisy Nets}{subsection.2.4.1}% 29
\BOOKMARK [3][-]{subsubsection.2.4.1.6}{Integrated Agent:Rainbow}{subsection.2.4.1}% 30
\BOOKMARK [1][-]{section.2.5}{Problems with RL}{chapter.2}% 31
\BOOKMARK [1][-]{section.2.6}{general computer vision stuff}{chapter.2}% 32
\BOOKMARK [1][-]{section.2.7}{General latent space learning}{chapter.2}% 33
\BOOKMARK [2][-]{subsection.2.7.1}{Auto-encoder}{section.2.7}% 34
\BOOKMARK [2][-]{subsection.2.7.2}{Forward model}{section.2.7}% 35
\BOOKMARK [2][-]{subsection.2.7.3}{Inverse model}{section.2.7}% 36
\BOOKMARK [2][-]{subsection.2.7.4}{Using prior knowledge to constrain the state space}{section.2.7}% 37
\BOOKMARK [2][-]{subsection.2.7.5}{Using hybring objectives}{section.2.7}% 38
\BOOKMARK [2][-]{subsection.2.7.6}{Common neural network architectures}{section.2.7}% 39
\BOOKMARK [3][-]{subsubsection.2.7.6.1}{AE}{subsection.2.7.6}% 40
\BOOKMARK [3][-]{subsubsection.2.7.6.2}{DAE}{subsection.2.7.6}% 41
\BOOKMARK [3][-]{subsubsection.2.7.6.3}{VAE}{subsection.2.7.6}% 42
\BOOKMARK [3][-]{subsubsection.2.7.6.4}{Siamese networks}{subsection.2.7.6}% 43
\BOOKMARK [1][-]{section.2.8}{Model-based reinforcement learning}{chapter.2}% 44
\BOOKMARK [0][-]{chapter.3}{Related Work}{}% 45
\BOOKMARK [1][-]{section.3.1}{Work on top of which we build}{chapter.3}% 46
\BOOKMARK [1][-]{section.3.2}{Current state-of-the-art on Atari, Agent 57}{chapter.3}% 47
\BOOKMARK [1][-]{section.3.3}{Current state-of-the-art on Atari}{chapter.3}% 48
\BOOKMARK [1][-]{section.3.4}{Work whose techniques we share}{chapter.3}% 49
\BOOKMARK [1][-]{section.3.5}{Work achieving same goals as us, but differently}{chapter.3}% 50
\BOOKMARK [1][-]{section.3.6}{Strongly related to our work}{chapter.3}% 51
\BOOKMARK [0][-]{chapter.4}{Methods}{}% 52
\BOOKMARK [1][-]{section.4.1}{Problems to be tackled}{chapter.4}% 53
\BOOKMARK [1][-]{section.4.2}{Hypotheses}{chapter.4}% 54
\BOOKMARK [2][-]{subsection.4.2.1}{Enviroment and Preprocessing}{section.4.2}% 55
\BOOKMARK [2][-]{subsection.4.2.2}{Deep Auto-encoder and Model Architecture}{section.4.2}% 56
\BOOKMARK [2][-]{subsection.4.2.3}{Training the RL Agent}{section.4.2}% 57
\BOOKMARK [0][-]{chapter.5}{Results}{}% 58
\BOOKMARK [1][-]{subsection.5.0.1}{Two Step Training}{chapter.5}% 59
\BOOKMARK [2][-]{subsection.5.0.2}{Parallel Training}{subsection.5.0.1}% 60
\BOOKMARK [0][-]{chapter.6}{Conclusion}{}% 61
\BOOKMARK [1][-]{section.6.1}{Discussion}{chapter.6}% 62
\BOOKMARK [1][-]{section.6.2}{Conclusion}{chapter.6}% 63
\BOOKMARK [0][-]{section.6.2}{Bibliography}{}% 64
\BOOKMARK [0][-]{appendix.A}{Appendix 1}{}% 65

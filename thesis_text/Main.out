\BOOKMARK [0][-]{chapter*.3}{List of Figures}{}% 1
\BOOKMARK [0][-]{chapter*.4}{List of Tables}{}% 2
\BOOKMARK [0][-]{chapter.1}{Introduction}{}% 3
\BOOKMARK [0][-]{chapter.2}{Background}{}% 4
\BOOKMARK [1][-]{section.2.1}{Introduction to reinforcement learning}{chapter.2}% 5
\BOOKMARK [2][-]{subsection.2.1.1}{Problem setting}{section.2.1}% 6
\BOOKMARK [2][-]{subsection.2.1.2}{Bandit problems}{section.2.1}% 7
\BOOKMARK [2][-]{subsection.2.1.3}{Markov Decision Processes}{section.2.1}% 8
\BOOKMARK [2][-]{subsection.2.1.4}{Key concepts in reinforcement learning}{section.2.1}% 9
\BOOKMARK [3][-]{subsubsection.2.1.4.1}{Policy}{subsection.2.1.4}% 10
\BOOKMARK [3][-]{subsubsection.2.1.4.2}{Goal of reinforcement learning}{subsection.2.1.4}% 11
\BOOKMARK [3][-]{subsubsection.2.1.4.3}{Value functions}{subsection.2.1.4}% 12
\BOOKMARK [1][-]{section.2.2}{Classes of reinforcement learning algorithms}{chapter.2}% 13
\BOOKMARK [2][-]{subsection.2.2.1}{Policy gradients}{section.2.2}% 14
\BOOKMARK [3][-]{subsubsection.2.2.1.1}{Baselines}{subsection.2.2.1}% 15
\BOOKMARK [3][-]{subsubsection.2.2.1.2}{Off-policy gradients}{subsection.2.2.1}% 16
\BOOKMARK [4][-]{paragraph.2.2.1.2.1}{Advanced policy gradient}{subsubsection.2.2.1.2}% 17
\BOOKMARK [2][-]{subsection.2.2.2}{Actor-critic algorithms}{section.2.2}% 18
\BOOKMARK [2][-]{subsection.2.2.3}{Value function methods}{section.2.2}% 19
\BOOKMARK [3][-]{subsubsection.2.2.3.1}{Dynamic programming}{subsection.2.2.3}% 20
\BOOKMARK [1][-]{section.2.3}{Deep Reinforcement Learning and DQN}{chapter.2}% 21
\BOOKMARK [2][-]{subsection.2.3.1}{Extension of DQN }{section.2.3}% 22
\BOOKMARK [3][-]{subsubsection.2.3.1.1}{Double Deep Q-networks: DDQN}{subsection.2.3.1}% 23
\BOOKMARK [3][-]{subsubsection.2.3.1.2}{Prioritized replay}{subsection.2.3.1}% 24
\BOOKMARK [3][-]{subsubsection.2.3.1.3}{Dueling Network}{subsection.2.3.1}% 25
\BOOKMARK [3][-]{subsubsection.2.3.1.4}{Multi-step learning}{subsection.2.3.1}% 26
\BOOKMARK [3][-]{subsubsection.2.3.1.5}{Noisy Nets}{subsection.2.3.1}% 27
\BOOKMARK [3][-]{subsubsection.2.3.1.6}{Integrated Agent:Rainbow}{subsection.2.3.1}% 28
\BOOKMARK [1][-]{section.2.4}{Problems with RL}{chapter.2}% 29
\BOOKMARK [1][-]{section.2.5}{general computer vision stuff}{chapter.2}% 30
\BOOKMARK [1][-]{section.2.6}{general latent space learning}{chapter.2}% 31
\BOOKMARK [1][-]{section.2.7}{Related Work}{chapter.2}% 32
\BOOKMARK [2][-]{subsection.2.7.1}{Current state-of-the-art}{section.2.7}% 33
\BOOKMARK [2][-]{subsection.2.7.2}{Related to our work}{section.2.7}% 34
\BOOKMARK [0][-]{chapter.3}{Methods }{}% 35
\BOOKMARK [1][-]{subsection.3.0.1}{Enviroment and Preprocessing}{chapter.3}% 36
\BOOKMARK [2][-]{subsection.3.0.2}{Deep Auto-encoder and Model Architecture}{subsection.3.0.1}% 37
\BOOKMARK [2][-]{subsection.3.0.3}{Training the RL Agent}{subsection.3.0.1}% 38
\BOOKMARK [0][-]{chapter.4}{Results}{}% 39
\BOOKMARK [1][-]{subsection.4.0.1}{Two Step Training}{chapter.4}% 40
\BOOKMARK [2][-]{subsection.4.0.2}{Parallel Training}{subsection.4.0.1}% 41
\BOOKMARK [0][-]{chapter.5}{Conclusion}{}% 42
\BOOKMARK [1][-]{section.5.1}{Discussion}{chapter.5}% 43
\BOOKMARK [1][-]{section.5.2}{Conclusion}{chapter.5}% 44
\BOOKMARK [0][-]{section.5.2}{Bibliography}{}% 45
\BOOKMARK [0][-]{appendix.A}{Appendix 1}{}% 46

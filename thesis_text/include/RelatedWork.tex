\chapter{Related Work}
\label{ch-related-work}
As said in the introduction, the goal of the thesis is to use
state representation learning to increase the efficiency 
and finals results of model-free reinforcement learning.
We are now ready to discuss the specifics of our approach.
Firstly, we limit ourselves to image observations and discrete action spaces.
In particular, we limit ourselves to Atari57 games as they are common benchmarks in the field
for discrete action spaces.
As shall be seen in the following text, a lot of recent work in state-representation learning
for model-free reinforcement learning has been done in robotics problems with 
continous action spaces, for example \cite{sac+ae}. %---> you sure about that fam??
% maybe do drqv2 but with image reconstruction loss as well?
% yarats didn't try that
%Our goal will mostly be to transfer these successes to the discrete action space setting,
%namely to DQN and Rainbow algorithms as the basis, instead of SAC and DDPG.
Importantly, since we are concerned with finding ways to make  reinforcement learning
more sample-efficient, we will be using only off-policy algorithms.

Secondly, we are particularly interested in the problem of simultaneous training
of the state representations and the policy.
The reason for this is that two-step training is often not available because not all
state transitions can be observed beforehand.
This state of affairs is the natural setting for problems where reinforcement learning
is a good solution: the problems where exploration is necessary due to
either the high complexity of the dynamics or unanticipatable events.
Parallel training of the state representations and the policy necessitates
instability in policy training due to the fact the state estimations
change even for same observations as the state representation are learned.
Hence, related work that focuses on solving or at least ameliorating this issue 
is of particular importance to our work.

Finally, we want our method to be robust not just in the sense that it works
across a wide array of problems, but in the sense that it can be 
easily added to a variety of reinforcement learning algorithms
to a positive effect. In other words, it should function as a module
which can be easily added to new algorithms.
Furthermore, it should work well with other improvements as those suggested
in some of the following related work.
%For clarity, we divide our discussion of related work in three categories:
%prior work on top of which we build, work which utilizes some of the same
%techniques we employ for different goal, or which achieves the same goals
%in different, but related ways and work which is strongly related to our own
%from which derived inspiration to our approach.
To set the context, we begin with by discussing prior work in the Atari environment.
\section{Reinforcement learning on Atari}
Started with \cite{mnih2013atari}. We already discussed \cite{rainbow}.
%It has been solved, in the sense that all algorithm reach super-human performance
%by \cite{agent57}.
Agent 57 \cite{agent57}  was the first deep RL agent that out performs 
the standard human benchmark on all 57 Atari games.
It was built on  on top of the Never Give Up (NGU) agent which utilizes a model-based approach.
It combines two ideas: first, 
the curiosity-driven exploration, and second, 
distributed deep RL agents, in particular R2D2.
The agent was able to balance the learning of different 
skills that are required to perform well on such diverse set of games: 
exploration and exploitation and long-term credit assignment.
In order to achieve this a neural network was trained to parameterize 
a family of policies ranging from very exploratory to purely exploitative,
by using adaptive  mechanism polices were prioritized throughout the training process.


However, if we convert simulated time to real time,
these algorithms can take up to 16000 hours to reach their final performance.
Since the goal is not really to solve Atari games, but to find useful general purpose
algorithms, the work is still ongoing.
The new proposed benchmark is Atari100K: solving the games with with only 100000 transitions.
\footnote{This equates to 400000 frames because the standard is to repeat each action 4 times:
		this makes learning easier, but also makes sense because humans do not need
such small reaction time to solve the games.}
This equates to 2.5 hours of real time.

\section{Efforts in increasing efficiency in Atari}
At the moment of writing, to the authors knowledge, the most efficient 
algorithm is \cite{ye2021mastering} which is based on MuZero \cite{schrittwieser2020mastering}
and is a model-based algorithm.
However, the title of the most efficient algorithm often switches
between a model-based algorithm, a model-free algorithm with state representation learning
or similar approaches.
We will not discuss model-based approaches, but will discuss some alternative ones
as their techniques illuminate the problem.

In particular, this concerns using data-augmentation as a means to directly regularize 
reinforcement learning.
This was first employed in \cite{rad} and expanded in \cite{drqv1}
and \cite{drqv2}.
In \cite{rad}, the observations are augmented before they are passed
to the policy networks. As we discussed in \ref{ae-regularization},
data-augmentation or noisifying input data functions as strong regularization
to feature extractors.
The same applies to feature extraction trained just from reinforcement learning.
In \cite{drqv1}, the same observation is copied and augmented several times.
All of these augmented version of the same image are passed through the policy network.
The results are then averaged and provide a better estimates than 
those obtained by a single pass of either non-modified or augmented observation.
Thus we may conclude that data augmentation provides benefits to both 
representation and reinforcement learning.

We now turn to discussing works which utilize unsupervised state representation learning
to increase reinforcement learning efficiency.

\section{State representation learning for efficient model-free learning}
Auxiliary losses may be used in a myriad of different ways to help reinforcement learning.
TODO check first 2.
In for example \cite{lossisitsownreward}, \cite{rlwauxloss} or \cite{icm}
the same models used for state representations as used to help guide exploration.
When, for example, a trained forward predictive model incurs large error,
it is reasonable to assume that this happened because a novel state has been encountered.
This means that the loss can be interpreted as ``intrinsic reward'' and be added to 
``extrinsic reward'' provided by the environment, yielding an algorithm which 
encourages exploration.

Of interest to us is the use of auxiliary losses for state representation learning.
The specific loss and how it's used depends on the chosen state representation model.
In the following subsections some common approaches will be explored.

\subsection{Deterministic generative models}
Perhaps the simplest model to be used for state representation learning on images is 
an autoencoder trained on reconstruction loss.
Using an autoencoder ensures spatial coherence.
This idea has been introduced in \cite{lange2010deep}.
It did not get traction in reinforcement learning more broadly due to the fact
that when the autoencoder is updated, the state representation changes.
Unlike regularizing noise which reduces overfitting and incentivizes learning of
desirable properties, this noise is destructive. It hinders the ability 
of the reinforcement learning algorithm to associate states with their values
due to the fact that what it is given different numbers as the same state 
through the course of autoencoder training.
To solve this problem, regularization needs to used.
In \cite{sac+ae}, this was solved by employing the regularizations
introduced in \cite{ghosh2019variational}, which were already discussed in \ref{ae-regularization}.

A mayor flaw of this approach is the fact that reconstructive loss
incentivizes reconstruction of the entire image which contains information
irrelevant to the agent. This pertains backgrounds and other object which do not effect
state transitions. This does not mean that the obtained representations are not better
than raw images, but that they could be made better.
Knowing this, we still opted for this approach due to its simplicity and easy of debugging.

\subsection{Stochastic generative models}
As discussed in \ref{sub-repr-models-general}, stochastic models can be used to generate 
predictions which can be used to plan and thus be used in model-based reinforcement learning.
However, this does not mean that they can not be used to bolster model-free learning.
As discussed in \ref{subsec-mdps}, the formal setting for reinforcement learning is 
the Markov decision process which is stochastic.
Of course, the degree of stochasticity depends on the problem at hand,
but given even in a fully deterministic setting stochastic models
can be used to deal with epistemic uncertainty.
This is further exacerbated in case of partial observability.
In \cite{slac}, (approximate) variational inference is used to formulate the entire algorithm objective.
First, control is formulated as an inference problem and is thereby embedded into the
MDP graphical model. From this single graphical model of the problem,
the variational distribution of action-dependent state transitions can be factorized
into a product of recognition, dynamics and policy terms.
As with most approaches which employ stochastic generative models, 
a variational autoencoder is used to represent the latent (representation) space.
It should be noted that without this deep integration with the problem,
which enables learning state representation and policies under a single objective,
the stochasticity of state representations would hurt the performance of the algorithm.
A detailed analysis of this issue can be found in \cite{sac+ae}.



\subsection{Discriminative models}
Because we are ultimately only interested in state representations, generative models are not required.
Thus it is natural to opt for a discriminative model.
Discriminative models can be trained in different ways.
In \cite{curl}, contrastive loss is employed.
Another common choice, theoretically investigated in \cite{rakelly2021mutual},
(TODO:check these 2)
is used in \cite{anand2019unsupervised} and \cite{mazoure2020deep} is to use mutual information.
A particularly promising avenue is to learn discriminative representation models
though bootstrapping as introduced in \cite{grill2020bootstrap}.
This has been employed to learn state representations in \cite{schwarzer2020data}, and in \cite{merckling2022exploratory}
where the losses have also been used to incentivize exploration.

These approaches ameliorate problems found in approaches discussed so far: that they avoid both the stochasticity 
of stochastic generative models and the unnecessary features picked up through reconstruction loss.
Learning state representations through bootstrapping is particularly interesting because it is rather
flexible with its formulation. In both papers mentioned, the bootstrapping happens
through self-predictive loss and is aided with inverse dynamics loss.
It would be interesting to integrate this more deeply with an appropriate reinforcement learning algorithm,
akin to how stochastic generative models are integrated in the MDP in \cite{slac}.

%\section{Work on top of which we build}
%\begin{enumerate}
%		\item algs we use: \cite{rainbow}, \cite{sac}
%		\item (us, historic) deep Auto-Encoder Neural Networks in Reinforcement Learning 
%				\cite{firstaeinrl}
%\end{enumerate}

%\subsection{Rainbow stuff}
%TODO: move this to background
%As stated in section 2.4.1.6 Integrated agent  
%was built by integrating the previous extensions of DQN in to one agent.
%Prioritized replay and multi-step learning were the two most crucial components.
%compared to the previous benchmarks rainbow was able to improve both  
%data efficiency and final performance.
%Although it potentially improved the performance of the original
%DQN algorithm; it also inherits the disadvantage of DQN, such as excessive
%memory usage, learning instability, and can only be applied to a discrete
%action space\cite{investigationontheDeepLearningFramework}.
%
%%TODOs: \cite{sac}
% 
%The use of deep Auto-Encoder Neural Networks in Reinforcement Learning is till
%in its early stage. The application of auto-encoders in dimensionality reduction
%has played a major role in reducing training time and data efficiency
%\cite{auto-encoderforEfficientEmbeddedReinforcementLearning}.
%Introducing auto encoders in batch RL resulted in learning from 
%raw pixels with out previously augmenting the data manually or 
%prepossessing \cite{firstaeinrl}; this closes the  
%existing gap between the  between the high dimensionality of 
%visual observations and the low dimensionality of state spaces.
%Deep
%convolutional encoders can learn good representations,they require large
%amounts of training data which makes  there application  in control systems
%limited.
%In the latest work \cite{sac} a successful RL variant called SAC + AE was
%introduced. Prior to this agent, two-step training was proposed by (Lange \&
%Riedmiller, 2010; Munk et al., 2016; Higgins et al., 2017b; Zhang et al.,
%2018a; Nair et al., 2018; Dwibedi et al., 2018) due to suboptimal policies the
%performance of these agents was poor.SAC + AE was designed using parrallel
%training using an off-policy algorithm and add an auxiliary task with an
%unsupervised objective low samole effieciency of most of the previously stated
%arcitectures presented under Section 2.3.1 was implemented. In addition to
%successfully combining autoencoders with model-free RL in the off-policy
%setting ;it was proved that SAC+AE bit the current model-based agents  with
%noisy observations  


%\section{Current state-of-the-art on Atari, Agent 57}
%
%
%
%\section{Current state-of-the-art on Atari}
%%\begin{enumerate}
%%		\item (discuss some state of the art ); \cite{agent57}
%%\end{enumerate}
%
%Agent 57 \cite{agent57}  was the first deep RL agent that out performs the
%standard human benchmark on all 57 Atari games.It was built on  on top of the
%Never Give Up(NGU) agent. which combines two ideas: first, the curiosity-driven
%exploration, and second, distributed deep RL agents, in particular R2D2.The
%agent was able to balance the learning of different skills that are required to
%perform well on such diverse set of games: exploration and exploitation and
%long-term credit assignment.In order to achieve this a neural network was
%trained to parameterize a family of policies ranging from very exploratory to
%purely exploitative,by using adaptive  mechanism polices were prioritized
%throughout the training process.

%\section{Work whose techniques we share}
%\begin{enumerate}
%		\item (not us, but things that can be done with image loss): \
%\end{enumerate}


%\section{Work achieving same goals as us, but differently}
%\begin{enumerate}
%\begin{enumerate}
%		\item (not us, but things that can be done with image loss): \cite{lossisitsownreward}, 
%				\cite{rlwauxloss} is the aux loss paper, also uses rewards for aux goals.
%				point here is you can use other losses and make them intrinsic rewards.
%		\item (us as inspiration) \cite{icm} - here the importance of embedding only
%				observations relevant for the agent are discussed.
%				they used this to improve exploration though.
% \end{enumerate}

% \section{Work achieving same goals as us, but differently}
% \begin{enumerate}
%		\item (not us but same high-level idea): data-augmentation like \cite{drqv1},
%				\cite{drqv2}, \cite{rad},
%\cite{imageaugmentationisallyouneed},
%				contrastive loss like \cite{curl}, 
%				\cite{flow}, 
%				\cite{invariantrepwithoutreconstruction}
%\end{enumerate}
%TODO
%First explain \cite{rad} which just throws image augmentation over sampled observations.
%In \cite{drqv1}, the authors go further by using multiple augmentations over the same image
%to regularize the Q-function. The expected return calculated for each of the augmented 
%versions of an observation. Then the average of those is calculated and passed
%as the predicted expected return. This greatly stabilizes Q-learning
%and allows it to learn with much greater sample-efficiency.
%Since what is developed is a regularization technique, it can
%be seamlessly applied to various reinforcement learning algorithms.
%In \cite{drqv2}, the authors futher improve \cite{drqv1} by introducing several
%changes to their overall algorithm.
%
%In \cite{curl}, contrastive loss is used instead.
%
%
%\section{Strongly related to our work}
%\begin{enumerate}
%		\item 
%		\item (us) yarats improving sample efficiency \cite{sac+ae}, \cite{laser}
%\end{enumerate}
%


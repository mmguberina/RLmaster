% CREATED BY DAVID FRISK, 2016
\chapter{Reinforcement learning}
\label{ch-rl-background}
%\section{Introduction to reinforcement learning}
%\label{sec-rl-intro}
\section{Problem setting}
\label{subsec-problem-setting}
In the usual engineering approach to problems,
prior scientific knowledge is used to first describe the problem
and then to define it mathematically.
Once this is done, unknown variables are measured and
solutions are calculated.
This approach works if the inherent stochasticity of the environment
can be controlled, i.e. if bounds of stochasticity are known
the solution account for them and be designed to be robust to them.
But some problems have circumstances which can not be known in advance,
or which are incredibly hard to hand-engineer.

In those cases, an entirely different approach becomes the only viable one:
designing a system which can produce and refine its own solution,
or in other words, designing a system which, in a way, learn the
solution by itself.
This is the idea behind the learning-based approaches: automating the 
process of learning.
Crucially, now the world and how it operates
is unknown and has to be discovered.  
The schematic \ref{fig:rl-shematic} shows how this process is formulated in reinforcement learning.
\begin{figure}[htpb]
\begin{center}
\begin{tikzpicture}[scale=1, transform shape, node distance=2.0cm]
		\node (agent) [rec] {agent};
		\node (environment) [rec, right of=agent, xshift=1.5cm] {environment};
		\draw [arrow, xshift=0.5cm]  (environment.240) to [bend left=30] node [midway, below, yshift=-0.2cm] (textnode1) {observation, reward}  (agent.300);
		\draw [arrow, xshift=0.5cm]  (agent.70) to [bend left=30] node [midway, above, yshift=0.2cm] (textnode2) {action} (environment.100);
\end{tikzpicture}
\end{center}
\caption{Conceptual schematic of reinforcement learning.}
\label{fig:rl-shematic}
\end{figure}
Reinforcement learning is a 2-step iterative process.
The \textbf{agent}, which represents the computer program, takes \textbf{actions}
in its \textbf{environment}. It then \textbf{observes} the resulting state
of the environment and is also given a \textbf{reward}
which is a function mapping every state of the environment to a number.

To help introduce reinforcement learning formally,
first the simplest possible problem 
to which reinforcement learning is the best solution is described.

\subsection{Bandit problems}
%Reinforcement learning  uses training information that evaluates 
%the actions taken rather than instruct by giving correct actions.
Consider the following problem.
The agent is faced with $k$ different gambling slot machines.
Each of them give random rewards under an unknown distribution.
At each turn, the agents has to select one of the machines and pull its lever.
%The agents is repeatedly faced with a choice of k different actions.
%After each choice the agent receive a numerical reward based on the action selected.
The goal is to maximize the expected total reward over some number of turns.
%This is original form of k-armed bandit problem.
%Depending on the action selected $k$-armed bandit problem formalizes the value function to get the expected or average reward.
If the agent knew the distribution of rewards of each of the slot machines, 
it would simply choose the one with the highest expected reward.
%in number of turns it has been given.
%It is assumed the agent do not know the action value with certainty although the estimate of 
%the action value is known by the agent and we expected it to be close to actual value of the action.
However, the agent does not have access to that information
and hence it can not effectively \textbf{exploit} the environment to obtain the highest rewards.
Instead, it is forced to \textbf{explore} the environment in order to learn the probability distribution of the reward.
In this problem, this amounts to pulling different levers and recording the received payoffs.
%At any time step the agent will be able to select at least one action whose estimated value is greatest.
%When the agent selects on of this actions it is exploiting the current knowledge of the
%values of the actions.
%If the agents keep exploiting the goal of maximizing reward over period of time will be trivial.
%If instead the agent selects one of the non greedy action this will enables it to improve the average expected reward over time.

The key issue is thus to balance exploitation and exploration.
It is compounded by the fact that the agent is given only a finite amount of time,
or a finite number of lever pulls in this case.
While one could leverage Bayesian statistics to construct an optimal solution to this 
simplest formulation of the bandit problem, this quickly becomes intractable as the complexity of
the problem is increased.
Namely, the described problem is stationary, in the sense that past actions do not influence 
the state of the world: the slot machines do not change as different levers are pulled.
Said another way, the $ k  $-bandit problem has a single \textbf{state}.
This is of course not the case in most problems of interest.
%To describe a changing environment in a stochastic setting, Markov chains are used.
To model the agent's effect on the environment, additional structure needs to be introduced.
This is done in the following sections.
%When addressing the canonical problem of sequential decision making under uncertainty,
%the exploitation-exploration trade-off is highlighted.More specifically,
%as depicted in Fig.1, an agent interacts with an unknown environment in a sequential manner to obtain rewards.
%The ultimate goal is to maximize the rewards.
%For one thing, the agent takes advantage of existing knowledge of the environment.
%For another, the agent investigates an unfamiliar environment.

 
\subsection{Markov Decision Processes}
\label{subsec-mdps}
%\cite{gehring2017convolutional}
%The environment of $ k  $-bandit problem is static --- 
%the actions do not change the \textbf{state} of the environment.
To model environments in which states change, Markov chains are used.
They capture the stochastic nature of state transitions, while Markov property 
allows for easier mathematical analysis.
The schematic of a Markov chain is shown in \ref{fig:markov-chain}.

\begin{figure}[htpb]
\begin{center}
\begin{tikzpicture}[scale=1, transform shape, node distance=1.5cm]
		\node (s1) [mcs] {$\bm{s}_{1} $};
		\node (s2) [mcs, right of=s1, xshift=1cm] { $ \bm{s}_{2}  $};
		\draw [arrow] (s1) -- node [below, midway] {$ p(\bm{s}_{t+1}|\bm{s}_{t}) $} (s2);
		\node (s3) [mcs, right of=s2, xshift=1cm] { $ \bm{s}_{3}  $};
		\draw [arrow] (s2) -- node [below, midway] {$ p(\bm{s}_{t+1}|\bm{s}_{t}) $} (s3);
\end{tikzpicture}
\end{center}
\caption{Schematic of a Markov chain.}
\label{fig:markov-chain}
\end{figure}

Formally, a Markov chain $ \mathcal{M}  $ is defined by its state space
$ \mathcal{S}  $ with discrete or continuous state $ \bm{s} \in \mathcal{S}  $
and the transition operator $ \mathcal{T}  $.
The notation $ \bm{s}_{ t }  $ denotes the state at time $ t  $ and it is a vector of real numbers.
The transition operator allows for a succinct description of environment dynamics.
For a transition probability $ p(s_{ t+1 }|s_{ t })  $,
let $ \mu_{ t,i } = p (\bm{s}_{t} = i)  $ and
$ \mathcal{T}_{ i,j } = p (\bm{s}_{t+1} = i|\bm{s}_{t} = j )  $.
Then $\overrightarrow{\mu}_t$ is a vector of probabilities and 
$\overrightarrow{\mu}_{t+1} = \mathcal{T} \overrightarrow{\mu}_t$.
Importantly, $ \mathcal{T}  $ is linear.

To model the agent's actions, 
actions are added as priors to state transition probabilities
in the Markov chain. 
With this and the definition of the reward function,
a Markov decision process is constructed.
It's schematic can be seen in \ref{fig:mdp}.
\begin{figure}[htpb]
\begin{center}
\begin{tikzpicture}[scale=1, transform shape, node distance=1.5cm]
		\node (s1) [mcs] {$\bm{s}_{1} $};
		\node (a1) [mcs, above right of=s1] { $ \bm{a}_{1}  $};
		\node (s2) [mcs, right of=s1, xshift=1.5cm] { $ \bm{s}_{2}  $};
		\draw [arrow] (a1) -- (s2);
		\draw [arrow] (s1) -- node [below, midway] {$ p(\bm{s}_{t+1}|\bm{s}_{t}, \bm{a}_{t}) $} (s2);
		\node (s3) [mcs, right of=s2, xshift=1.5cm] { $ \bm{s}_{3}  $};
		\node (a2) [mcs, above right of=s2] { $ \bm{a}_{2}  $};
		\draw [arrow] (s2) -- node [below, midway] {$ p(\bm{s}_{t+1}|\bm{s}_{t}, \bm{a}_{t}) $} (s3);
		\draw [arrow] (a2) -- (s3);
\end{tikzpicture}
\end{center}
\caption{Schematic of a Markov decision process.}
\label{fig:mdp}
\end{figure}

The Markov decision process is thus defined by the tuple
$ \mathcal{M} = \{\mathcal{S}, \mathcal{A}, \mathcal{T}, r\}  $.
$ \mathcal{A}  $ denotes the action space, where
$ \bm{a}_{} \in \mathcal{A}  $ is a continuous or discrete action and
$ r  $ is the reward function $r : \mathcal{S} \times \mathcal{A} \to \mathbb{R}$.
It should also be noted that the transition operator is now a tensor.
Let $\mu_{t,j} = p(s_t = j), \xi_{t,k} = p(a_t = k), \mathcal{T}_{i,j,k} = p(s_{t+1} = i | s_t =j, a_t =k) $.
Then $\mu_{t+1,i} = \sum_{j,k}^{} \mathcal{T}_{i,j,k} \mu_{t,j} \xi_{t,k}$.
Therefore, $ \mathcal{T}  $ retains its linearity.

Finally, partial observability also needs to be accounted for.
To do so, a partially observable Markov decision process (POMDP) needs to be constructed.
This is done by augmenting the Markov decision process to also include
the observation space $ \mathcal{O}  $, where observations $ \bm{o}_{} \in \mathcal{O} $
denote the discrete or continuous observations
and the emission probability $ \mathcal{E}  $ which describes the probability 
$ p(\bm{o}_{t} | \bm{s}_{t})  $ of getting the observation $ \bm{o}_{t}  $ when in state  $\bm{s}_{t}$.
The schematic can be seen in \ref{fig:pomdp}.

\begin{figure}[htpb]
\begin{center}
\begin{tikzpicture}[scale=1, transform shape, node distance=1.5cm]
		\node (s1) [mcs] {$\bm{s}_{1} $};
		\node (a1) [mcs, above right of=s1] { $ \bm{a}_{1}  $};
		\node (o1) [mcs, above of=s1] { $ \bm{o}_{1}  $};
		\draw [arrow] (s1) -- (o1);
		\node (s2) [mcs, right of=s1, xshift=1.5cm] { $ \bm{s}_{2}  $};
		\node (o2) [mcs, above of=s2] { $ \bm{o}_{2}  $};
		\draw [arrow] (s2) -- (o2);
		\draw [arrow] (a1) -- (s2);
		\draw [arrow] (s1) --  (s2);
		\node (s3) [mcs, right of=s2, xshift=1.5cm] { $ \bm{s}_{3}  $};
		\node (o3) [mcs, above of=s3] { $ \bm{o}_{3}  $};
		\draw [arrow] (s3) -- (o3);
		\node (a2) [mcs, above right of=s2] { $ \bm{a}_{2}  $};
		\draw [arrow] (s2) -- (s3);
		\draw [arrow] (a2) -- (s3);
\end{tikzpicture}
\end{center}
\caption{Schematic of a partially observable Markov decision process.}
\label{fig:pomdp}
\end{figure}
It is important to note that not all elements of POMDP are present in every problem: for example,
the reward may be a deterministic function of the state and so on.
In general through the text, to aid in simplifying notation, only the necessary elements will be explicity referenced
in sketches and written out in the equations (most often using just the Markov decision process).
%However, POMDP fully describes the setting in which reinforcement learning algorithms can be defined.


%Markov decision processes (MDPs) are used to model decision making in discrete, stochastic, sequential environments,
%and are thus a natural choice of model in which reinforcement learning can be defined.
%[9].
%Markov Chain, which works with S, a set of states, and P, the probability of transitioning from one to the next. It also uses the Markov Property, meaning each state depends only on the one immediately prior to it.

%MDPs is a framework that can solve most of reinforcement learning problems with discrete actions.The formal definition of Markov decision process can be summarised in  the tuple $\mathcal{M} = \{\mathcal{S}, \mathcal{A},\mathcal{P(S_0)}, \mathcal{T}, r\}$[8].
%The goal of the model is that an agent inhabits a  stochastic environment in a response to action choice made by the agent[9]. \\

%The environment consists of the the Transition function, $\mathcal{T} : \mathcal{S} \times \mathcal{A} \to \mathcal{(PS)}$.\\  and the reward function ($r(s_t, a_t)$), $r : \mathcal{S} \times \mathcal{A} \to \mathbb{R}$.In MDPs the transition probability and reward only depends on the current state and the action chosen by the agent  but not the past state and action.

%The agent acts in environment according to a policy $\mathcal{\pi} : \mathcal{S} \to \mathcal{(PS)}$.policy learned can be off policy where the behaviour policy is different from the policy used for action selection one common example is Q-learning or on-policy methods which attempts to evaluate or improve the policy that is used to make decision.\\
%The former state-action value function (Q function for short)can be computes recursively with dynamic programming: 


\section{Key concepts in reinforcement learning}
\label{subsec-key-rl-concepts}
\subsection{Policy}
With the problem space being formally defined,
definitions which will allow the construction
of a reinforcement learning algorithm may be introduced.
The reinforcement learning problem can be defined in finite or infinite time horizons.
Different environments usually naturally fall in either category.
In order for the agent to learn, it needs to be able to try out different actions from the same, 
or at least similar states.
This is usually achieved by having the agent return to a set of starting states.
The period between two such returns is called \textbf{an episode}.
The agent selects actions based on its \textbf{policy} $ \pi  $.
The policy is a function which maps states to actions.
The schematic showing it in the context of a Markov decision process
is given in \ref{fig:policy-in-mdp}.
\begin{figure}[htpb]
\begin{center}
\begin{tikzpicture}[scale=1, transform shape, node distance=2.5cm]
		\node (s1) [mcs] {$\bm{s}_{1} $};
		\node (a1) [mcs, above right of=s1] { $ \bm{a}_{1}  $};
		\node (s2) [mcs, right of=s1, xshift=1.5cm] { $ \bm{s}_{2}  $};
		\draw [arrow] (a1) -- (s2);
		\draw [arrow] (s1) -- node [below, midway] {$ p(\bm{s}_{t+1}|\bm{s}_{t}, \bm{a}_{t}) $} (s2);
		\node (s3) [mcs, right of=s2, xshift=1.5cm] { $ \bm{s}_{3}  $};
		\node (a2) [mcs, above right of=s2] { $ \bm{a}_{2}  $};
		\draw [arrow] (s2) -- node [below, midway] {$ p(\bm{s}_{t+1}|\bm{s}_{t}, \bm{a}_{t}) $} (s3);
		\draw [arrow] (a2) -- (s3);
		\draw [arrow] (s1) -- node [above, midway, sloped] {$\pi_{ \theta } (\bm{a}_{t}| \bm{s}_{t} )  $} (a1);
		\draw [arrow] (s2) -- node [above, midway, sloped] {$\pi_{ \theta } (\bm{a}_{t}| \bm{s}_{t} )  $} (a2);
\end{tikzpicture}
\end{center}
\caption{Schematic of a Markov decision process with a policy $ \pi  $.}
\label{fig:policy-in-mdp}
\end{figure}

The policy is a stochastic function. The intensity of stochasticity determines the trade-off
between exploration and exploitation.
To emphasize that the policy depends on some parameters $ \theta  $,
the notation $ \pi_{ \theta } $ is used.

\subsection{Goal of reinforcement learning}
For simpler notation, the finite horizon form
is assumed for the following definitions.
Since the environment is modeled as a Markov decision process,
the probability of observing a trajectory of states
and actions can be written as:

\begin{equation}
\underbrace{p_\theta(\bm{s}_1, \bm{a}_1, \dots, \bm{s}_T, \bm{a}_T)}_{p_\theta(\tau)} = p(\bm{s}_1) \prod^{T}_{t=1} 
\underbrace{\pi_{\theta} (\bm{a}_t | \bm{s}_t) p (\bm{s}_{t+1} | \bm{s}_t, \bm{a}_t)}_{\text{Markov chain on } (\bm{s}, \bm{a})}
\end{equation}
A bit more explicitly, we can a transition probability as:
\begin{equation}
p((\bm{s}_{t+1}, \bm{a}_{t+1}) | (\bm{s}_t, \bm{a}_t)) = 
p((\bm{s}_{t+1}| (\bm{s}_t, \bm{a}_t)) \pi_\theta (\bm{a}_{t+1} | \bm{s}_{t+1})
\end{equation}

With this, a formal definition of the goal of reinforcement learning can be given.
It is to find policy parameters $ \theta^{ \star }  $ such that:
\begin{align}
		\theta^\star &= \argmax_{\theta} \mathbb{E}_{\tau \sim p_\theta(\tau)} \left[ \sum_{t}^{} r(\bm{s}_t, \bm{a}_t) \right] \\
	 &= \argmax_{\theta} \sum_{t}^{T} \mathbb{E}_{(\bm{s}_t, \bm{a}_t) \sim p_\theta(\bm{s}_t, \bm{a}_t)} \left[  r(\bm{s}_t, \bm{a}_t) \right]
\end{align}

To ensure that the expected sum of rewards, also know as the \textbf{return},
is finite in the infinite horizon case, a
\textbf{discount factor} $ 0 < \gamma < 1  $ is introduced in the sum.
The discount factor also plays a role in modelling 
because usually it makes sense to value immediate rewards
more.
It is important to note that the \textit{expected} sum of
rewards is maximized . This makes the goal a smooth and differentiable function of the parameters,
which means gradient descent can be employed to find the optimal parameters.
This leads us to the first class of reinforcement learning algorithms:
policy gradient algorithms.
They will be introduced with the other classes of algorithm in \ref{sub-policy-gradient},
while additional concepts required by other classes of algorithms will be introduced 
in the following subsection.

\subsection{Value functions}
Value functions are functions which map states or state-action pairs
to the expected returns obtained under a fixed policy.
They are a concept from dynamic programming. In fact,
reinforcement learning can be interpreted as an extension of dynamic programming,
as shall be done in the following subsection.
Having that said, value function can be interpreted in other ways as well.
The \textbf{Q-function} maps state-action pairs to the estimated sum of returns
under policy $ \pi_{ \theta } $:
\begin{equation}
		\label{eq:q-function}
		Q^\pi (\bm{s}_t, \bm{a}_t) = \sum_{t'=t}^{T} \mathbb{E}_{\pi_\theta}
		\left[ r(\bm{s}_{t'}, \bm{a}_{t'} )| \bm{s}_t, \bm{a}_t \right] 
\end{equation}
thus denoting the expectred total reward from taking $\bm{a}_t$ in $\bm{s}_t$.
\textbf{Value functions} map states to to the expected sum of rewards
under policy $ \pi_{ \theta } $:
\begin{equation}
		\label{eq:value-function}
		V^\pi (\bm{s}_t) = \sum_{t'=t}^{T} \mathbb{E}_{\pi_\theta}
		\left[ r(\bm{s}_{t'}, \bm{a}_{t'} | \bm{s}_t) \right] 
\end{equation}

The connection between the two is the following:
\begin{equation}
		V^\pi (\bm{s}_t) = \mathbb{E}_{\bm{a}_t \sim \pi(\bm{s}_t, \bm{a}_t)}
		\left[ Q^\pi(\bm{s}_t, \bm{a}_t) \right] 
\end{equation}
With these definitions, the reinforcement learning objective may also be written as:
\begin{equation}
		\mathbb{E}_{\bm{s}_1 \sim p(\bm{s}_1)}
		\left[ V^\pi (\bm{s}_1) \right] 
\end{equation}


\section{Classes of reinforcement learning algorithms}
\label{sec-rl-alg-classes}
\subsection{Policy gradient algorithms}
\label{sub-policy-gradient}
Policy gradients are derived by directly solving for
the reinforcement learning objective using gradient descent,
where the derivative is taken with respect to the policy parameters.
To do so, the reinforcement learning objective needs to be evaluated.
To make this a bit easier to follow, we introduce a notational shorthand:

\begin{equation}
		\theta^\star = \argmax_\theta \underbrace{\mathbb{E}_{\tau \sim p_\theta (\tau)} \left [ \sum_t r(\bm{s}_t, \bm{a}_t) \right ]}_{J(\theta)}
\end{equation}

$J(\theta)$ is estimated by making rollouts from the policy.
Simply put, the agent collects experience under the current policy,
and the average return is used as the estimate.
In the equation below $i$ is the sample index. $i,t$ denotes the $t^{\text{th}}$ timestep
in the $i^{\text{th}}$ sample:
\begin{equation}
		J(\theta) = \mathbb{E}_{\tau \sim p_\theta(\tau)} \left [ \sum_t r(\bm{s}_t, \bm{a}_t) \right ] \approx 
		\frac{1}{N} \sum_i \sum_t r(\bm{s}_{i,t}, \bm{a}_{i,t})
\end{equation}
Simplifying the notation further, we get:
\begin{equation}
		J(\theta) = \mathbb{E}_{\tau \sim p_\theta(\tau)} \underbrace{[r(\tau)]}_{\sum^{T}_{t=1} r(\bm{s}_t, \bm{a}_t)} = 
		\int_{{}}^{} {p_\theta(\tau)r(\tau)} \: d{\tau} {}
\end{equation}
The goal now is to compute the derivative of the estimated reinforcement learning
objective:
\begin{equation}
		\label{eq:derivative-of-estimated-rl-obj}
		\nabla_\theta J(\theta) = \int_{{}}^{{}} {\nabla_\theta p_\theta (\tau) r(\tau)} \: d{\tau} {}
\end{equation}

Since the goal of this text is just to introduce the necessary concepts
and algorithms, the derivation(s) will be ommited.
We encourage the interested reader to consult the literature \cite{suttonrlbook, berkleylectures} 
to find more information.

Here we will just note that it is crucial that the final expression
can be estimated by sampling the agent's experience 
as the other quantities are not available.
The resulting expression for the policy gradient \label{eq:derivative-of-estimated-rl-obj} is:

\begin{equation}
		\label{eq:policy-gradient}
		\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim p_\theta(\tau)} 
		\left [ \left ( \sum_{t=1}^{T} \nabla_\theta \log \pi_\theta (\bm{a}_t | \bm{s}_t ) \right )
		\left ( \sum_{t=1}^{T} r(\bm{s}_t, \bm{a}_t) \right ) \right ]
\end{equation}
It can be evaluated through sampling:
\begin{equation}
		\label{eq:estimated-policy-gradient}
		\nabla_\theta J(\theta) \approx \frac{1}{N}  \sum_{i=1}^{N} 
		\left ( \sum_{t=1}^{T} \nabla_\theta \log \pi_\theta (\bm{a}_{i,t} | \bm{s}_{i,t} ) \right )
		\left ( \sum_{t=1}^{T} r(\bm{s}_{i,t}, \bm{a}_{i,t}) \right )
\end{equation}
The estimated gradient can be used to perform gradient ascent. 
This is the backbone of 
the REINFORCE algorithm, also known as ``vanilla policy gradient'':

\fbox{
		\parbox{\textwidth}{
				\underline{REINFORCE algorithm:}
\begin{enumerate}
		\item sample $\{\tau^i\}$ from $\pi_\theta(\bm{a}_t | \bm{s}_t)$ by running the policy
		\item use the samples to estimate the gradient of the objective: \newline $\nabla_\theta J(\theta) \approx   \sum_{i}^{} 
		\left ( \sum_{t}^{T} \nabla_\theta \log \pi_\theta (\bm{a}_{i,t} | \bm{s}_{i,t} ) \right )
		\left ( \sum_{t}^{} r(\bm{s}_{i,t}, \bm{a}_{i,t}) \right )$
\item update the policy function by performing a step of gradient ascent: \newline 
		$\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta) $
\end{enumerate}
}}

This algorithm does not work well in practice.
The main reason for that is that the variance of returns
is very high. 
However, there are a number of modifications which dramatically improve its performance.
Since the goal of this text is not to outline every reinforcement learning algorithm,
we will introduce only the modifications which outline
general trade-offs and principles in reinforcement learning algorithm design.

\subsubsection{Baselines}
The policy gradient in the REINFORCE algorithm lacks some important properties.
One of them is that it should, ideally, make bad actions less likely
and good actions more likely. 
However, if all rewards are positive, then all actions' probabilities will be increased,
only by different amounts.
This can be changed if a \textbf{baseline } $ b  $ is added to actions:
\begin{align}
		\nabla_\theta J(\theta) &\approx 
		\frac{1}{N} \sum_{i=1}^{N}
		\nabla_\theta \log p_\theta (\tau) [ r(\tau) - b] \\
		b &= \frac{1}{N} \sum_{i=1}^{N} r(\tau)
\end{align}
This addition does not change the gradient in expectation, i.e. it does not
introduce bias,
but it does change its variance.
Although an optimal bias can be calculated, it is rarely used in practice due
to its computational cost.
Using baselines is one of the key ideas in actor-critic algorithms
so they will be discussed further in \ref{sub-ac-algs}.

\subsubsection{Off-policy gradients}
An important property of the REINFORCE algorithm is that it
is an \textbf{on-policy} algorithm.
This means that new samples need to be collected for every gradient step.
The reason behind this is the fact that the expectation of the gradient 
of the return needs to be calculated with respect to the current parameters
of the policy.
In other words, because the policy changes with each gradient step,
old samples are effectively collected under a different policy.
This means that they can not be used to calculate the expected gradient 
of the return with respect to the current policy as it would not
produce those rollouts:
\begin{equation}
		\nabla_\theta J(\theta) = \underbrace{\mathbb{E}_{\tau \sim p_\theta(\tau)}}_{\text{this is the trouble!}} [\nabla_\theta p_\theta(\tau)r(\tau)]
\end{equation}
If the policy is a neural network, which requires small gradient steps,
the cost of generating a large number of samples for every update
could make the algorithm entirely infeasible.
This of course depends on the cost of generating samples,
which is entirely problem dependent ---
policy gradient algorithms are often the best solution when 
the cost of generating samples is low.

However, on-policy algorithms can be turned into off-policy 
algorithms through \textbf{importance sampling},
which is the name given to the following mathematical identity:
\begin{align}
		E_{x \sim p(x)} [f(x)]  
		&= \int_{{}}^{{}} {p(x)f(x)} \: d{x} \\
		&= \int_{{}}^{{}} {\frac{q(x)}{q(x)}  p(x)f(x)} \: d{x} \\
		&= \int_{{}}^{{}} { q(x) \frac{p(x)}{q(x)}  f(x)} \: d{x} \\
		&= E_{x \sim p(x)} \left [ \frac{p(x)}{q(x)} f(x) \right ]
\end{align}
which is exact in expectation.
To use importance sampling to create an off-policy policy gradient algorithm,
certain approximations need to be made. Again, the details of the derivation
are omitted and what follows is just the final result.
\begin{equation}
		\nabla_{\theta'} J(\theta') \approx
		\frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T}
		\frac{\pi_{\theta'}( \bm{a}_{i,t} | \bm{s}_{i,t})}{\pi_{\theta}( \bm{a}_{i,t} | \bm{s}_{i,t})} 
		\nabla_{\theta'} \log \pi_{\theta'} (\bm{s}_{i,t}, \bm{a}_{i,t}) 
		\hat{Q}_{i,t} 
\end{equation}
To get this equation, the factor 
$ \frac{\pi_{\theta'}(\bm{s}_{i,t})}{\pi_{\theta}(\bm{s}_{i,t})}  $
had to be ignored in the expression because it is impossible 
to calculate the state marginal probabilities.
This means that the expression works only if $ \pi_{ \theta' }  $
is not too different from $ \pi_{ \theta }  $.
The justification for this can be found in the previously referenced literature.
What is important in the context of the thesis is that we will be interested only in
off-policy methods as they are inherently more sample-efficient and 
out goal is to increase the sample-efficiency of reinforcement learning.

\subsubsection{Advanced policy gradients}
The basic algorithm we have outlined is essentially just a basic gradient descent method. 
From convex optimization, we know that it can be made much better if second order derivatives
or their approximations are used.
For example, conjugate gradient descent can be used.
Further, there are various ways in which this optimization problem can be better conditioned.
Such improvements led to algorithms such as PPO and TRPO,
which will not be discussed here.

\subsection{Actor-critic algorithms}
\label{sub-ac-algs}
Actor-critic methods can be seen as making a different trade-off between 
variance and bias in policy gradient estimation.
Consider with the following observation:
\footnote{In this equation, the summation of rewards is done from time $t   $
		to $ T  $ because actions and states prior to that time do
		not affect the return from that time onward.
This leveraging of causality reduces the variance of the estimate.}
\begin{equation}
		\nabla_\theta J(\theta) \approx 
		\frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_\theta \log \pi_\theta (\bm{a}_{i,t}| \bm{s}_{i,t})
		\underbrace{\left ( \sum_{t'=t}^{T} r (\bm{s}_{i,t}, \bm{a}_{i,t}) \right )}_{\hat{Q}_{i,t}
		\text{: ``reward to go''}}
\end{equation}
Simply put, in the policy gradient method a single-run Monte-Carlo (MC) estimate is used
to estimate the return.
This causes high variance, while incurring no bias.
Another option is to try to estimate the full expectation
$ \hat{Q}_{i,t} \approx  \sum_{t'=t}^{T} \mathbb{E}_{\pi_\theta} \left[ r(\bm{s}_{t'}, \bm{a}_{t'}) |\bm{s}_{t}, \bm{a}_{t}  \right]     $.
Since the estimate won't be perfect, it will introduce bias.
Of course, using multiple runs from the same state-action pair would 
reduce variance, but this is sometimes impossible to procure
and is certainly more costly.
However, if the ``reward to go'' estimator can generalize between states,
it will be able to produce good estimates regardless.

Like the policy, the return estimator will have to be learned.
In this approach, the policy is also called the \textbf{actor} 
and the return estimator is called the \textbf{critic}.
%We proceed by discussing how the critic can be constructed.
If the correct Q-function was available (i.e. not the estimate, but the actual values),
the policy gradient estimate could be improved by using it
both to estimate the return and as a baseline:
\begin{align}
		\nabla_\theta J(\theta) &\approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_\theta \log \pi_\theta (\bm{a}_{i,t}| \bm{s}_{i,t})
( Q(\bm{s}_{i,t}, \bm{a}_{i,t}) - b) \\
		b_t &= \frac{1}{N} \sum_{i}^{} Q(\bm{s}_{i,t}, \bm{a}_{i,t})
\end{align}
However, having a baseline that depends on actions leads to bias.
Thus a state-dependent baseline is employed: 
\begin{equation}
		V(\bm{s}_t) = E_{\bm{a}_t \sim \pi_\theta (\bm{s}_{t}, \bm{a}_{t})} [Q(\bm{s}_{t}, \bm{a}_{t})]
\end{equation}
Since the value function \ref{eq:value-function} gives the expected return of the average action,
it is possible to calculate how much better a certain action is by substracting
its Q-value \ref{eq:q-function} for the value function.
The result is called the \textbf{advantage function}:
\begin{equation}
A^\pi (\bm{s}_{t}, \bm{a}_{t}) = Q^\pi (\bm{s}_{t}, \bm{a}_{t}) - V^\pi (\bm{s}_t)
\end{equation}
Thus, either the Q-function, the value function or the advantage function can be learned.
Of these, it is best to learn the value function because there are less 
states than state-action pairs.
The advantage function is then calculated in the following way:
\begin{align}
		A^\pi (\bm{s}_{t}, \bm{a}_{t})  &\approx r(\bm{s}_{t}, \bm{a}_{t}) + V^\pi (\bm{s}_{t+1})  - V^\pi(\bm{s}_t)
\end{align}

The value function can be estimated through samples:
\begin{align}
		V^\pi (\bm{s}_t) &\approx \sum_{t'=t}^{T} r(\bm{s}_{t'}, \bm{a}_{t'})
\end{align}
After collecting many such samples
\begin{equation}
		\left\{ \left( \bm{s}_{i,t}, \underbrace{\sum_{t'=t}^{T} r(\bm{s}_{i,t}, \bm{a}_{i,t})}_{y_{i,t}} \right)  \right\} 
\end{equation}
the value function can be fitted through supervised regression with the following loss:
\begin{equation}
		\mathcal{L}(\phi) = \frac{1}{2} \sum_{i}^{} ||\hat{V}^\pi_\phi (\bm{s}_i) - y_i||^2
\end{equation}
This process can be sped up with bootstrapped estimates:
\begin{equation}
		y_{i,t} = \sum_{t'=t}^{T} E_{\pi_\theta} \left[ r(\bm{s}_{t'}, \bm{a}_{t'})|\bm{s}_{i,t}\right] + V^\pi(\bm{s}_{i,t+1})  
		\approx r(\bm{s}_{i,t}, \bm{a}_{i,t}) + \hat{V}^\pi_\phi(\bm{s}_{i,t+1}) 
\end{equation}
This will further reduce variance, but again increase bias.

Fortunately, the trade-off between bias and variance can be tuned.
In the Monte Carlo estimate, the entire trajectory was used
to estimate the return. In the bootstrap estimate,
only a single step in the future was used along with the estimate.
Instead, a \textbf{n-step} return estimator can used:
\begin{equation}
		\label{eq-n-step-return}
		\hat{A}^\pi_n (\bm{s}_{t}, \bm{a}_{t}) =
		\sum_{t'=t}^{t+n} \gamma^{t'-t} r(\bm{s}_{t'}, \bm{a}_{t'})
		- \hat{V}^\pi_\theta(\bm{s}_t) + \gamma^n \hat{V}^\pi_\theta(\bm{s}_{t+n})
\end{equation}
In most cases
the ideal trade-off for $ n  $ lies somewhere between 1 and $\infty$ (the MC estimate).
Finally, an average of all n-step return estimators can be used.
This is called the generalized advantage estimator (GAE):
\begin{equation}
\hat{A}^\pi_{GAE} (\bm{s}_{t}, \bm{a}_{t}) =
\sum_{n=1}^{\infty} (\gamma \lambda)^{t'-t}r(\bm{s}_{t'}, \bm{a}_{t'}) + \gamma \hat{V}^\pi_\theta(\bm{s}_{t'+1})  - \hat{V}^\pi_\theta(\bm{s}_{t'})
\end{equation}
where the factor $ \lambda  $ controls the weight of future values.

Combining this into an iterative algorithm, and fixing the issues
of naiive implementations results in the following algorithm:

\fbox{
\parbox{\textwidth}{
\underline{Actor-critic algorithm template}
\begin{enumerate}
		\item take action $\bm{a} \sim \pi_\theta(\bm{a}|\bm{s})$, observe transition
				$(\bm{s}, \bm{a},\bm{s'},r)$ and store it in the replay buffer $\mathcal{R}$ 
		\item sample a batch $\left\{  (\bm{s}_i, \bm{a}_i,\bm{s'}_i,r_i) \right\} $ from buffer $\mathcal{R}$
		\item update the Q-value estimator $\hat{Q}^\pi_\theta$ by using the target: \newline
				$y_i = r_i + \gamma \hat{Q}^\pi_\theta(\bm{s}_i', \bm{a}_i') \forall \bm{s}_i, \bm{a}_i$
		\item compute the policy gradient estimate with: \newline
				$\nabla_\theta J(\theta) \approx  \frac{1}{N} \sum_{i}^{}  \nabla_{\theta} \log \pi_\theta(\bm{a}^\pi_i|\bm{s}_i)\hat{Q}^\pi(\bm{s}_{i}, \bm{a}^\pi_{i})$,
				where $\bm{a}_i^\pi \sim \pi_\theta(\bm{a} | \bm{s}_i)$
		\item update the policy function by performing a gradient step: \newline
				$\theta \leftarrow \theta  + \alpha \nabla_\theta J(\theta)$
\end{enumerate}
}}

%\begin{equation}
%   	Q^\pi (s,a) = E_s'[r + \gamma E_a'~\pi(s') [Q^\pi(s',a')]|s,a,\pi]
%\end{equation}
 
%optimal q value under deterministic policy
%\begin{equation} 
%a = argmax_a'\epsilon A Q^*(s,a')
%\end{equation} 
%
%\begin{equation}
%	  Q^* (s,a) = \max_\pi  Q^\pi(s,a)
%\end{equation}
%
%if the optimal function satisfies the bellman equation:
%\begin{equation}
%    Q^* (s,a) = E_s' [r + \gamma max(\bm{a'}) Q^* (s',a')| s,a,\pi]
%\end{equation}
%
%The advantage function  describes “how good” the action a is, compared to the expected return when following direct policy pi.
%
%\begin{equation}
%    A^\pi (s,a) = Q^\pi(s,a)- V^\pi(s)
%\end{equation}
%
%The value function V measures the how good it is to be in a particular state s. The Q function, however, measures the the value of choosing a particular action when in this state[6].\\

\subsection{Value function methods}
Value function methods use only the critic from actor-critic algorithms.
Suppose that the advantage function $ A^{ \pi } (\bm{s}_{t}, \bm{a}_{t} )  $ is known.
It tells us how much better the action $ \bm{a}_{t}  $ is than the average action
according to the policy $ \pi  $.
Thus, if provided with the advantage function, a deterministic 
\textbf{greedy policy} could be construct:
\begin{equation}
		\label{eq-greedy-pi}
		\pi_{ \text{greedy} }(\bm{s}_{t}| \bm{a}_{t}) = \left\{ 
\begin{matrix}
		1 & \text{ if } \bm{a}_t = \argmax_{\bm{a}_t} A^\pi (\bm{s}_{t}, \bm{a}_{t}) 		 \\
		0 & \text{ otherwise}
\end{matrix}
		\right.
\end{equation}
which would yield the highest expected return.
In other words, if the advantage function is known, the policy would be
reduced to the argmax operation.
This approach has roots in dynamic programming.

\subsubsection{Dynamic programming}
Dynamic programming refers to a collection of algorithms that can be used
to compute optimal policies given a perfect model of the environment.
They are of limited utility in reinforcement learning due to the perfect model requirement 
and their great computational expense, but are important theoretically:
they provide an essential foundation for understanding the other methods.
Usually a finite Markov decision process is assumed. Dynamic programming can be applied to continuous 
problems as well, but exact solution exist only in special cases.

For brevity, dynamic programming will not be fully introduced here.
The interested reader is referred to \cite{suttonrlbook}.
Here only the broad idea will be introduced in order to make value iteration intuitive in its own right.
A problem is said to have optimal substructure if an optimal solution to it can be constructed from
optimal solutions of its subproblems.
Value functions have this property: for a single well-defined problem,
nearby optimal states are not related to optimality of distant states.
In other words, the value of one state is related only to the value of states to which there are transitions from it.
This principle is used to derive the \textbf{Bellman equation}.
In problem with a finite number of states, it can be shown that iterating between evaluating the value function
with the Bellman equation
and updating the policy based on the value function leads to the optimal value function and policy.

In particular, the bootstrap update for the value function is:
\begin{equation}
		\label{eq-bellman-value-update}
V^\pi(\bm{s}) \leftarrow E_{\bm{a} \sim \pi(\bm{a}|\bm{s})} \left[ r(\bm{s}_{}, \bm{a}_{}) + \gamma E_{\bm{s}' \sim p(\bm{s}' |\bm{a},\bm{s}  )} [V^\pi(\bm{s}') ] \right] 
\end{equation}
where $   V^\pi(\bm{s}') $ is the current estimate (initially set to whatever).
With \ref{eq-greedy-pi} we can construct the policy iteration algorithm:

\fbox{
\parbox{\textwidth}{
\underline{Policy iteration}
\begin{enumerate}
		\item evaluate $ V^{ \pi } (\bm{s}_{})  $ with \ref{eq-bellman-value-update}
\item set $ \pi \leftarrow \pi'  $
\end{enumerate}
}}

Upon inspection of the $ \argmax  $ in the advantage function, by skipping the policy update, this produce can be further simplified 
into the \textbf{value iteration} algorithm:
\footnote{Here the notation was simplified for readability.}
\fbox{
\parbox{\textwidth}{
		\label{alg-finite-value-iteration}
\underline{Value iteration}
\begin{enumerate}
		\item set $Q(\bm{s}, \bm{a}) \leftarrow r (\bm{s}, \bm{a}) + \gamma E[V(\bm{s}')]$
		\item set $V(\bm{s}) \leftarrow \max_{\bm{a}} Q(\bm{s}, \bm{a})$
\end{enumerate}
}}


\subsubsection{Fitted value iteration}
As mentioned, and as should be clear from the equations, \ref{alg-finite-value-iteration} works only on
problems with a finite number of states.
While real-valued problems can be partitioned into discrete ones and limited in scope,
for most interesting problems this results in an intractable algorithm.
However, function approximation, in particular nonlinear function approximation through neural networks
can greatly bolster the capacity of value iteration.
Another benefit of such an approach is that it is naturally adaptable to being off-policy.
This is because after collecting samples the goal is to fit the value function, namely the Q-function, 
to the gathered data.
In particular, the algorithm template is:
\footnote{
A particular implementation called DQN
will be discussed later.}

\fbox{
\parbox{\textwidth}{
		\label{eq:fitted_q_iteration_algorithm}
\underline{Fitted Q-iteration algorithm}
\begin{enumerate}
		\item collect dataset $ \left\{ \left( \bm{s}_i, \bm{a}_i, \bm{s}_i', r_i \right)  \right\}  $ using a 
				policy based on the value function
		\item set $ \bm{y}_i \leftarrow r(\bm{s}_{i}, \bm{a}_{i})+ \gamma \max_{\bm{a}_i'} Q_\phi (\bm{s}_{i}', \bm{a}_{i}')   $
		\item set $\phi \leftarrow \argmin_\phi \frac{1}{2} \sum_{i}^{} || Q_\phi (\bm{s}_{i}, \bm{a}_{i}) - \bm{y}_i||^2 $
\end{enumerate}
}}
By itself, this algorithm does not encourage exploration.
This is usually fixed by using the \textbf{epsilon-greedy} policy:
\begin{equation}
\pi(\bm{a}_{t}| \bm{s}_{t}) = 
\left\{
		\begin{array}{ll}
				1 - \epsilon & \text{if } \bm{a}_t = \argmax_{\bm{a}_t} Q_\phi(\bm{s}_{t}, \bm{a}_{t}) \\
				\frac{\epsilon}{|\mathcal{A}| - 1} & \text{otherwise}
\end{array}
		\right.
\end{equation}
In this case $ \epsilon  $ is often set to be large in the beginning of training
and is decreased over time.

Unfortunately, by using nonlinear function approximators for value functions the convergence 
guarantees from the finite setting are lost.
To see why, we first introduce the Bellman operator:
\begin{equation}
		\mathcal{B}: \mathcal{B}V = \max_{\bm{a}} r_{\bm{a}} + \gamma \mathcal{T}_{\bm{a}}V
\end{equation}
where $r_{\bm{a}}$ is the stacked vector of rewards of all states for action $\bm{a}$,
and $\mathcal{T}_{\bm{a},i,j} = p (\bm{s}' = i | \bm{s} = j, \bm{a}) $ is the matrix of transitions corresponding to the action $\bm{a}$.
$V^\star$ is now a \textit{fixed point} of $\mathcal{B}$, 
meaning that if it is recovered the optimal policy is obtained:
\begin{equation}
		V^\star(\bm{s}) = \max_{\bm{a}} r(\bm{s}_{}, \bm{a}_{}) + \gamma E[V^\star(\bm{s}')], \text{ so } V^\star = \mathcal{B}V^\star
\end{equation}
It's possible to show that $V^\star$ always exists, is unique and corresponds to the optimal policy.
This is because
it can proven that $\mathcal{B}$ is a \textit{contraction}. This means that for any $V$, $\bar{V}$: 
\begin{equation}
		|| \mathcal{B}V - \mathcal{B}\bar{V}||_\infty \leq \underbrace{\gamma}_{\text{gap always gets smaller by } \gamma \text{ w.r.t. } \infty \text{-norm}} || V - \bar{V}||_\infty
\end{equation}

However, if nonlinear function approximation is used the situation changes.
Namely, in the second step of \ref{alg-finite-value-iteration}:
\begin{equation}
		V' \leftarrow \argmin_{V' \in \Omega} \frac{1}{2} \sum_{}^{} ||V'(\bm{s}) - (\mathcal{B}V)(\bm{s})||^2
\end{equation}
where $\Omega$ is the hypothesis space (ex. the space of all weights of employed neural network architectures).
$V'$ will be a projection of $\mathcal{B}V$ back to $\Omega$.
Let us introduce an operator for this projection:
\begin{equation}
		\Pi : \Pi V = \argmin_{V' \in \Omega} \frac{1}{2} \sum_{}^{} ||V'(\bm{s}) - V(\bm{s})||^2
\end{equation}
So the fitter value iteration algorithm is:
\begin{enumerate}
		\item $V \leftarrow \Pi \mathcal{B} V$
\end{enumerate}
and here $\mathcal{B}$ is a contraction (w.r.t. $\infty$-norm (``max'' norm)),
$\Pi$ is a contraction w.r.t. $l_2$-norm (Euclidean distance), but
$\Pi \mathcal{B}$ is not a contraction of any kind!
The same holds for fitted Q-iteration, but we withhold further analysis for 
sake of brevity.
Thus fitted value iteration does not converge.
Additionally, it is interesting that Q-learning is not in fact a 
derivative of the Q-function:
\begin{equation}
	 \phi \leftarrow \phi - \alpha \frac{d Q_\phi}{d\phi} (\bm{s}_{i}, \bm{a}_{i}) \left( Q_\phi(\bm{s}_{i}, \bm{a}_{i}) - 
	 \underbrace{\left[ r(\bm{s}_{i}, \bm{a}_{i}) + \gamma \max_{\bm{a}'} Q_\phi (\bm{s}_{i}', \bm{a}_{i}') \right] }_{\text{no gradient through target value}}
\right)  	
\end{equation}
As can be seen, the target Q-values themselves depend on Q-values.
Despite these sad theoretical results,
fitted value iteration algorithms work surprisingly well in practise.
In fact, this work uses fitted value iteration algorithms,
namely DQN and Rainbow, which will be introduced next.


%============================================e
%\newline
%Value iteration and Q-learning make up two fundamental algorithms of
%Reinforcement Learning.Q learning is an Off policy algorithm, which means it
%uses a different policy for exploring actions from the target policy of being
%optimal.  Many of the amazing feats in RL over the past decade, such as Deep
%Q-Learning for Atari, or AlphaGo, Rainbow were rooted in these foundations. As
%stated in section 2.2.3 value function ,is a measure of the expected reward you
%can receive from any given state  given an MDP and a policy describing which
%actions an agent takes in each state. Value iteration is a computational
%algorithm that provides a means of finding the optimal policy $ \pi^{ * }$ .
%The algorithm works by iteratively determining the value of being in each
%state,  assuming that the agent takes the best possible action in that state
%under the current estimate of the value function.Fitted value iteration
%algorithm  are used for approximating the value function of a continuous state
%MDP.Unlike value iteration over discrete state of states fitted value iteration
%can not always converge.In order for certain dynamic programming algorithms
%(e.g. policy iteration, value iteration) converge to a unique fixed point
%Bellman equation  was rewritten as operator.

%TODO: 
%\begin{enumerate}
%		\item Introduce value iteration in a tabular setting
%		\item introduce fitted value iteration
%		\item show that q-learning is off-policy
%		\item show that q-learning does not converge when using non-linear function
%				approximation (introduce the bellman operator as well)
%\end{enumerate}

\section{Deep reinforcement learning with Q-functions}
\label{sec-drl}
In this section we finally introduce reinforcement learning algorithms
which are built upon in the thesis.
As the section title suggests, these algorithms utilize (deep) neural networks.
While simple feed-forward networks with 2-3 hidden layers suffice for many control problems,
additional preceding convolutional layers are require form problems with image inputs.
For problems with long-term temporal dependencies, networks with recurrent layers are often used.
We are particularly concerned with the DQN algorithm \cite{mnih2013atari} and a series of improvements to it which culminated
in their combination which is named the Rainbow algorithm.
There are multiple reasons why Rainbow was chosen.
Firstly, it and other fitted value iteration algorithms are the best suited ones for
problems with discrete action spaces to which we restricted ourselves.
Secondly, leveraging state representation learning is most suitable to
fitted value iteration algorithms and critics in actor-critic algorithms because 
they are more stable than policy gradients. 
As shall be discussed later, stability of state representation learning and of reinforcement learning
is necessary for successful simultaneous learning of both.

The on-policy version of Q-learning suffers from overfitting to local transitions.
The problem is further exacerbated because the target values change through time.
While this problem can be ameliorated by using parallel workers,
a better solution is to use a \textbf{replay buffer}.
A replay buffer is simply an array which keeps track of sampled transitions.
Minibatch updates can be constructed by sampling independent and identically distributed
transitions from the buffer, thereby remove the problem of overfitting to local transitions.
Continuous replacement of old samples with new ones ensures continual learning as
new transitions are collected (the initial epsilon-greedy policy often can not sample all transitions
because it can not reach states with high rewards).
Q-learning then becomes:

\fbox{
\parbox{\textwidth}{
\label{q-learning-w-replay-buffer}
\underline{Q-learning with a replay buffer}
\begin{enumerate}
		\item[] \textit{repeat until a satisfactory result is reached}:
		\item collect dataset $\left\{ \left( \bm{s}_i, \bm{a}_i, \bm{s}_i', r_i \right)  \right\} $ using some policy, add it to $\mathcal{B}$
		\item[] \textit{repeat K times}:
		\item sample a batch  $\left( \bm{s}_i, \bm{a}_i, \bm{s}_i', r_i \right)$ in i.i.d. fashion from $\mathcal{B}$
		\item update network weights: \newline $  \phi \leftarrow \phi  - \alpha \sum_{i}^{}  \frac{d Q_\phi}{d\phi} (\bm{s}_{i}, \bm{a}_{i}) \left( Q_\phi(\bm{s}_{i}, \bm{a}_{i}) - 
			\left[ r(\bm{s}_{i}, \bm{a}_{i}) + \gamma \max_{\bm{a}'} Q_\phi (\bm{s}_{i}', \bm{a}_{i}') \right] 	\right) $ 
\end{enumerate}
}}
%This is the classic DQN algorithm from \cite{mnih2013atari}.
This algorithm still suffers from the fact that Q-learning is not gradient descent and that 
it tries to converge to a moving target (which is local overfitting).
One technique to ameliorate this is to use \textbf{target networks}.
The idea is to collect transitions with one network, called the target network, 
and apply updates to another.
For practical reasons, the two are the same network, where the target network
is periodically updated with the weights of the other network.
This can be done by simply copying the weights or doing it more smoothly
via ex Polyak averaging.
With this we get the classic DQN algorithm \cite{mnih2013atari}:

\fbox{
\parbox{\textwidth}{
		\label{alg-classic-dqn}
\underline{``Classic'' DQN}
\begin{enumerate}
		\item take some action $\bm{a}_i$,  observe $\left( \bm{s}_i, \bm{a}_i, \bm{s}_i', r_i \right)$ and add it to $\mathcal{B}$
		\item sample a mini-batch  $\left( \bm{s}_j, \bm{a}_j, \bm{s}_j', r_j \right)$  from $\mathcal{B}$ uniformly
		\item compute $y_j = r_j + \gamma \max_{\bm{a}_j'} Q_{\phi'} (\bm{s}_{j}', \bm{a}_{j}')$ using the \textit{target} network $Q_{\phi'}$
		\item $  \phi \leftarrow \phi  - \alpha \sum_{j}^{}  \frac{d Q_\phi}{d\phi} (\bm{s}_{j}, \bm{a}_{j}) \left( Q_\phi(\bm{s}_{i}, \bm{a}_{i}) - 
				y_j 	\right) $ 
		\item update $\phi'$: copy $\phi$ every $N$ steps
\end{enumerate}
}}
Here the other network is updated at every sample, i.e. $   K=1$ because that was the setting
in the original paper, but of course it could have any other value.


\subsection{Double Q-networks (DDQN)}
Empiric evidence shows that Q-networks trained with DQN overestimate returns, i.e.
the true Q-function. The reason for this is the following one:
\begin{equation}
		\text{target value } y_j = r_j + \gamma \underbrace{ \max_{\bm{a}_j'} Q_{\phi'}(\bm{s}_{j}', \bm{a}_{j}')}_{\text{herein lies the problem}}
\end{equation}
The explanation goes as follows.
Consider two random variables, $X_1$ and $X_2$, and let them represent a true value obscured by some noise.
Provably,
\begin{equation}
		E \left[ \max (X_1, X_2) \right] \geq \max	\left( E[X_1], E[X_2] \right) 
\end{equation}
The relation to Q-learning is the following.
If we imagine that $Q_{\phi'}(\bm{s}_{}', \bm{a}_{}')$ is not perfect because it has added noise,
we get exactly the situation in the inequality --- the max over the actions and the expectation
over it will lead to systematic overestimation.
Thus $\max_{\bm{a}'} Q_{\phi'}(\bm{s}_{}', \bm{a}_{}')$ \textit{overestimates} the next value.
Note that $\max_{\bm{a}'} Q_{\phi'}(\bm{s}_{}', \bm{a}_{}') = Q_{\phi'} (\bm{s}', \argmax_{\bm{a}'}Q_{\phi'}(\bm{s}', \bm{a}'))$ .
If noise in the action selection mechanism and noise in the value evaluation
mechanism are decorrelated , the problem will go away. In other words, getting both actions and values from $Q_{\phi'}$
needs to be avoided. This can be done by utilizing another network.
For practical reasons, the target network and the updating network can serve this purpose.
Although, not theoretically ideal, this solution works well in practise.
Thus the only difference to the \ref{alg-classic-dqn} is to change the error calculation in step 3 into
\begin{equation}
		y = r + \gamma Q_{\phi'}\left( \bm{s}', \argmax_{\bm{a}'}Q_{\phi}(\bm{s}_{}', \bm{a}_{}') \right)
\end{equation}


\subsection{Q-learning with multi-step returns}
As discussed in \ref{sub-ac-algs}, n-step returns \ref{eq-n-step-return} offer a better balance better bias and variance 
than either single-step bootstrap
estimates or Monte-Carlo estimates.
They can of course be used in Q-learning as well:
\begin{equation}
		y_{j,t} = \sum_{t'=t}^{t+N-1} \gamma^{t-t'}    r_{j,t'} + \gamma^N  \max_{\bm{a}_{j,t+N}}Q_{\phi'}(\bm{s}_{j,t+N}, \bm{a}_{j,t+N}) 
\end{equation}




\subsection{Prioritized replay}
As mentioned in \ref{q-learning-w-replay-buffer}, the samples for minibatches are sampled 
uniformly from the buffer. Depending on the size of the buffer, this slows down progress
as it takes time for newer transitions to be incorporated into Q-function estimation.
Alternatively, newer samples could be given priority by being sampled with
a higher probability, or, better still, also proportionally
to the size of the TD error:
\begin{equation}
	p_{ t } \propto \left| 
r_{ t+1 } + \gamma_{ t+1 } \max_{a'} Q_{ \phi' } (\bm{s}_{t+1}, a') -
Q_{ \phi } (\bm{s}_{t}, \bm{a}_{t} )
	\right|^{ \omega } 	
\end{equation}
where $ \omega  $ is a hyper-parameter determining the shape of the distribution.
%The main use of replay buffer is to sample transitions with maximum probability.Both DQN and DDQN samples experiences uniformly.Prioritized replay [20] samples transitions using the maximum priority ,providing a bias towards recent transitions and stochastic transitions  even when there is little left to learn about them.
%TODO:
%\begin{enumerate}
%		\item Introduce to deep learning and advances in computer vision
%	\end{enumerate}

%The success of a deep neural network in computer vision has introduced a new
%paradigm in learning from raw pixels.   The goal of Deep Reinforcement Learning
%is to connect the reinforcement learning algorithm to a deep neural network
%that operates directly on RGB images and efficiently processes training data
%using stochastic gradient updates \cite{mnih2013atari}. 
%
%
%The use of label data and the assumption that the distribution of data is
%identical and independent throughout the deep learning training process makes
%it complex to use it directly in reinforcement learning algorithms, which must
%be able to learn from sparse,noisy,delayed reward and highly correlated data.
%To address these issues and successfully apply deep learning to reinforcement
%learning \cite{mnih2013atari} used the experience replay mechanism
%\cite{mnih2015humanlevel} which randomly samples previous transitions and
%thereby smooths the distribution of training over many past behaviors. A
%convolutional neural network was used to learn successful control policies from
%raw video data in a complex reinforcement learning environment. The network is
%trained with a variant of Q-learning algorithm,with stochastic gradient descent
%to update weights.\\

%This architecture was based on the Tesauro TD-Gammon architecture [17] that
%updates the parameters of the network that estimate the value function directly
%from the sampled experience of the policy. Similarly to this approach, the
%online network in DQN uses a technique known as experience replay [18] where
%the  agent’s experiences at each time-step, $\mathcal e_t =
%(s_t,a_t,r_t,s_t+1)$ is stored in a data set $\mathcal D = e1 , ..., e_N $
%pooled over many  episodes in a replay memory. By drawing random samples from
%this pool of stored experiences, the Q-learning is updated.After performing the
%experience replay, the agent selects and executes an action according to a
%$\epsilon$-greedy policy.
%The use of experience replay and target networks enables 
%relatively stable learning of Q values, 
%and led to super-human performance on several Atari games.

%The advantage of using deep Q-learning over Q-learning  
%includes allowing  to have greater sample efficiency,
%reduced variance by randomizing the sample bias, and avoiding being 
%stuck in local minimum. 
%The drawbacks of deep Q-learning is that it only handle discrete, 
%low-dimensional action spaces.


%\section{Extension of DQN }

%Although the initial architecture of deep Q learning introduced by
%\cite{mnih2013atari} paves the way for the use of deep neural network in
%reinforcement learning, it comes with its own drawbacks;this leads to more
%studies and an improved DQN architecture.In this section, we will discuss these
%extensions of DQN architectures.



\subsection{Dueling Network}

Dueling Network was designed for  value based learning,this architecture separates the representation of sate-value and state-dependent action advantages without supervision[6].its consists of two streams that represents the value and advantage functions,while sharing a common convolutional feature learning module.This network has a single Q-learning network with two streams that replace DQN architecture[3]. 

\begin{equation}
    Q(s,a; \theta,\alpha,\beta) = V(s,\theta,\beta) + A(s,a; \theta,\alpha)
\end{equation}

%\textbf{if needed TODO ADD EQUATION:factorization of action values}

  

\subsection{Noisy Nets}

The one limitation of $\mathcal{\epsilon}$-greedy policy  is many actions must be executed to collect the first reward.Noisy Nets proposed a noisy linear layer that combines  a deterministic and noisy stream.Depending on the learning rate the network ignores to learn the noisy stream.

\subsection{Integrated Agent:Rainbow}


In the Rainbow architecture \cite{rainbow} 
several architecture changes included the one stated above where applied to DQN.
Distributional loss was replaced by a multi-step variant. The target
distribution was constructed  by contracting the value distribution
in St+n according to the cumulative discount, 
and shifting it by the truncated n-step discounted return. 
multi-step distributional loss with double Q-learning by using the greedy action in St+n 
selected according to the online network as the bootstrap action $a \cdot t+n,$ and evaluating such action using the target network.

%\textbf{TODO ADD EQUATION:target distribution}



\subsection{Deep autoencoders}


Reinforcement learning requires learning from large  high-dimensional image
data-set.For example, In Atari games the environment is composed of images with
210 * 160 pixels and 128 color palette. Each image is made up of hundreds of
pixels, so each data point has hundreds of dimensions. The manifold hypothesis
states that real-world high-dimensional data actually consists of
low-dimensional data that is embedded in the high-dimensional space. This is
the motivation behind dimensionality reduction techniques, which try to take
high-dimensional data and project it onto a lower-dimensional surface.

Autoencoders are a special kind of neural network used to perform
dimensionality reduction. They act as an identity function, such that an auto
encoder learns to output whatever is the input.They are  composed of two
networks, an encoder e and a decoder d.  

The encoder learns a non-linear
transformation that projects the data from the original high-dimensional input
space X to a lower-dimensional latent space Z. 
This is called latent  vector $z=e(x) $. 
A latent vector is a low-dimensional representation of a data point
that contains information about x.
This is commonly known  as latent space representation,
it contains all the important information needed to represent
raw data points.Auto encoders manipulates the ``closeness'' of data in the latent
space.  

A decoder learns a non-linear transformation d:Z→X that projects the
latent vectors back into the original high-dimensional input space X. This
transformation takes the latent vector and reconstruct the original input data
: 
\begin{equation}
     z = e(x)  	\rightarrow  \hat{x}=d(z)=d(e(x))
\end{equation}

The autoencoder is trained to minimize the difference between the input x and
the reconstruction $\hat{x}$ using a kind of reconstruction loss.\\ 

In traditional autoencoders, the latent vector should be easily decoded back to
the original image as a result the latent space z can become disjoint and
non-continuous. Variational autoencoders try to solve this problem.

In variational autoencoders, inputs are mapped to a probability distribution
over latent vectors, and a latent vector is then sampled from that
distribution. As a result the decoder becomes more robust at decoding latent
vectors.

Specifically, instead of mapping the input $x$ to a latent vector $z=e(x)$, 
we  instead map it to a mean vector $\mu(x)$ 
and a vector of standard deviations $\sigma(x)$. 
These parametrize a diagonal Gaussian distribution $\cal{N}(\mu, \sigma)$, 
from which we then sample a latent vector $z \sim \cal{N}(\mu, \sigma)$.


This is generally accomplished by replacing the last layer of a traditional
autoencoder with two layers, each of which output $\mu(x)$ and $\sigma(x)$. An
exponential activation is often added to $\sigma(x)$ to ensure the result is
positive.

However, this does not completely solve the problem. 
There may still be gaps in the latent space because the outputted means may 
be significantly different and the standard deviations may be small. 
To reduce that, an auxiliary loss is added that penalizes 
the distribution $p(z|x)$ for being too far from the standard normal distribution 
$\cal{N}(0,1)$. This penalty term is the 
Kullback-Leibler(KL) divergence between $p(z|x)$ and $\cal{N}(0,1)$, 
which is given by\(\mathbb{KL}\left( \mathcal{N}(\mu, \sigma) \parallel \mathcal{N}(0, 1) \right) = \sum_{x \in X} \left( \sigma^2 + \mu^2 - \log \sigma - \frac{1}{2} \right)\).This expression applies to two univariate Gaussian distributions by summing KL divergence for each dimension we are able to  extend it to our diagonal Gaussian distributions.

This loss is useful for two reasons. First, we cannot train the encoder network by gradient descent without it, since gradients cannot flow through sampling (which is a non-differentiable operation). Second, by penalizing the KL divergence in this manner, we can encourage the latent vectors to occupy a more centralized and uniform location. In essence, we force the encoder to find latent vectors that approximately follow a standard Gaussian distribution that the decoder can then effectively decode.

%TODO:
%\begin{enumerate}
%		\item Discuss the latent rep embedding diff with traditional AE and Varational AE with marko and chechk the recostrauction loss for both
%		
%		\item Include argument on the Architecture  of our final AE used for RL
%	\end{enumerate}


\section{Problems with RL}
\label{sec-rl-problems}
%ex. drl that matters paper
%\cite{drlthatmatters}

In previous studies presented in Chapter 3 we see some successful application
of unsupervised learning techniques applied to improve the performance of the
underlying RL algorithms, even though these and other studies conducted
previously have shown remarkable results. RL comes with the following
challenges. \\

One of the most difficult aspects of RL is learning efficiently with little
data. The term "sample efficiency" refers to an algorithm that makes the most
of a given sample. To put it another way, it's the amount of experience the
algorithm has to gain during training in order to achieve efficient
performance. The difficulty is that the RL system takes a long time to become
efficient.

Neural networks are opaque black boxes whose workings are mysteries to even the
creators. They are also increasing in size and complexity, backed by huge data
sets, computing power and hours of training.This referred to as Reproducibility
crisis, These factors make RL models very difficult to replicate.  

Another major challenge in Rl is agents are trained in simulated environment
;In this environment they can fail and learn, but they do not have the
opportunity to fail and learn in real-life scenarios. Usually, in real
environments, the agent lacks the space to observe the environment well enough
to use past training data to decide on a winning strategy. This also includes
the reality gap, where the agent cannot gauge the difference between the
learning simulation and the real world.\\

The reward technique discussed in the previous sections is not foolproof. Since
the rewards are sparsely distributed in the environment, a possible issue is an
agent not observing the situation enough to notice the reward signals and
maximise specific actions. This also occurs when the environment cannot provide
reward signals in time; for instance, in many situations, the agent receives a
green flag only when it is close enough to the target. 

Curiosity-driven methods are widely used to encourage the agent to explore the
environment and learn to tackle tasks in it. The researchers in the paper
‘Curiosity-driven exploration by self-supervised prediction’ proposed an
Intrinsic Curiosity Module (ICM) to support the agent in exploration and prompt
it to choose actions based on reduced errors. Another approach is curriculum
learning, where the agent is presented with various tasks in ascending order of
complexity. This imitates the learning order of humans. 

=============================================
\newline

%\section{Unsupervised learning on images}
%\ref{sec-unsupervised-learning-on-images}
%TBD - citing papers rl papers cite.
%Point here is pretraining is good and stuff like contrastive loss is introduced.

%Classifiers can't directly help RL, but things like object detectors might.
%Also learned models.
%But then we don't have end-to-end learning (it's important to explain why).


\chapter{State representation learning}
\label{ch-srl-background}
%\section{Introduction to state learning learning}
%TODO: throw in citations (ex. pilco, embed2control etc)

%BIG TODO:\\
%============================================\\
%clearly deliniate generative and discriminative models:
%--- generative models are AEs, VAEs, GANs,
%and are trained to learn the distribution of inputs
%by somehow using reconstruction loss and have different
%levels of statistical modelling complexity.
%--- discriminative models don't have decoders
%because they are not trained to learn the distribution of inputs.
%instead, they are only trained to learn a coherent mapping
%into a latent space. this can be done for example
%by contrastive learning.\\
%there are also bisimulation metrics and you better find out
%what those are.
%===============================================

As discussed in \ref{sec-why-pixels}, learning control from images is 
very desirable. Images, and observations in general, only implicitly 
provide information about the underlying state. 
Finding a good policy from observations, especially images,
is much more difficult than finding a policy with direct state access
because the state first needs to be inferred from those observations.
Reinforcement learning algorithms can by themselves implicitly extract
the relevant information from observations, but this at best results
in much less sample-efficient training and at worst results
in complete failure.
Often a problem which a reinforcement learning algorithm can solve
with direct state access, can not achieve any progress when
provided only image observations. \\

Extracting lower-dimensional information from data in order to extract
meaningful information from it is an established problem in machine learning broadly,
and is referred to as \textbf{representation learning} in that context.
Clearly, learning to extract stateful information from image observations
can be viewed as a subset of representation learning
and in this context it is referred to as \textbf{state representation learning}.
One important aspect of representation learning is that the representations
can be abstract and only implicitly represent the data in question.
Such representations can be learned in an unsupervised manner:
the goal is to learn latent representations which have useful features,
and as such they can be directly optimized with regards to these features
and not to conform to some explicit semantic form.
The purpose of this chapter is to briefly discuss representation learning
in general terms, and then to investigate how it can be applied
to the problem of learning state representations from images for control problems. 
Importantly, the problem of learning a model 
which can be used to achieve control through planning will not be discussed,
although there are similarities between the two.\\

The fundamental reason why this is a promising proposition is
the fact that the learning signal generated from for example
image reconstruction loss is substantially stronger than the reward signal,
especially in settings with sparse rewards where the reward signal is 
not present most of the time.
This thus means that the state representation will be learned quickly
compared to policy learning. Hence they can be leveraged to
aid the sample-efficiency in of reinforcement learning, 
despite their training starting at the same time as that of the policy.

\section{Representation learning in general}
\label{sec-repr-models-general}
%In general, representation learning algorithms are designed to learn
%abstract features that characterize data.
In general, representation learning refers to the process of learning 
a parametric mapping from raw input data domain
to a feature vector or tensor, in the hope of capturing and extracting more abstract and useful concepts
that can improve performance of downstream tasks.
%In the simplest forms it includes methods such as $ k  $ nearest neighbors.
%Usually, this includes dimensionality reduction.
This mapping should also meaningfully generalize well on new data.
%Before introducing types of representation models,
%we first need to define the characteristics a good representation of data needs
%to have in general. 
%These principles and trade-offs between their relative priorities guide the model design.
The following list, introduced in \cite{bengio2013representation},
summarizes different assumptions that can be made on the data to be represented.
These priors thus translate themselves as
desirable characteristics of learned representations.
\begin{enumerate}
		\item \textit{Smoothness}: the learned mapping $ f  $ is such that $ x \approx y  $ generally implies
				$ f(x) \approx f (y)  $.
		\item \textit{Multiple explanatory factors}: generally, there are several different underlying factors
				which are the cause of the observed data. The learned representations should be able
				to distinguish between these different factors. In other words,
				these causes should ideally be disentangled features in representation space.
				The existence of different underlying causes is, by itself, an assumption made on the observed data.
		\item \textit{A hierarchical organizations of explanatory factors}: The learned abstractions should relate
				to each other in a hierarchical fashion. This concerns, for example, the assumption that different
				layers of convolutional neural networks should embed progressively finer features of images.
		\item \textit{Semi-supervised learning}: for inputs $ X  $ and target $ Y  $ to be predicted,
				learning $ P (X)  $ should help learning $ P (Y|X)  $ because
				features of $ X  $ should help explain $ Y  $. This implies that unsupervised pre-training
				of networks should benefit supervised learning tasks because the features learned through unsupervised
				learning should help explain the supervised learning task.
		\item \textit{Shared factors across tasks}: moreover, the features learned on $ X  $ should
				help in learning different supervised predictions $ Y'  $.
		\item \textit{Manifolds}: if it is assumed that the probability mass concentrates
				in regions with much smaller dimensionality than the data itself,
				then the learned representations should have smaller dimensionality 
				to exploit this assumption.
		\item \textit{Natural clustering}: further, it is assumed that different values of categorical variables
				are associated with separate manifolds. This too should be evident in the learned representations.
		\item \textit{Temporal and spatial coherence}:
				consecutive or spatially nearby observations
				tend to be associated with the same value of relevant categorical
				concepts or result in small surface move on the surface of the manifold
		\item \textit{Sparsity}: implies either that many features are 0, or 
				that the features are insensitive to small changes in $ x  $
		\item \textit{Simplicity of factor dependencies}: ideally factors are related
				to each other linearly, or otherwise in a simple fashion.
\end{enumerate} 


The process of extracting representations from observations,
or inferring latent variables in a probabilistic view of a dataset,  is often called \textbf{inference}.
Models used for representation learning can be categorized as either \textbf{generative} or as \textbf{discriminative} models. \\

Generative models learn representations by modelling the data distribution
$ p(\bm{x}_{})  $. Such models can therefore generate realistic examples of the data they represent.
They can be used for downstream tasks by
evaluating the conditional distribution $ p (y | \bm{x}_{})  $. This is done via Bayes rule.

Discriminative models instead model the conditional distribution $ p (y | \bm{x}_{})  $
directly.
Discriminative modelling consists first of inference
that extracts latent variables
$ p(\bm{v}_{}| \bm{x}_{})  $,
which are then used to make downstream decision
from those variables $ p (y|\bm{v}_{})  $.

The benefit of discriminative models is that 
the expensive process of learning $ p (\bm{x}_{})  $ is avoided.
That however makes them harder to evaluate.
This is especially evident if you just want a lower dimensional distribution.
In the context of reinforcement learning, the model-based approach 
benefits from generative models as they can be used to generate predictions
which can then be used for planning.
In the model-free approach, both discriminative and generative models may be used
as predictions are not used.


\subsection{Generative models}
\subsubsection{Probabilistic models}
\label{subsub-probabilistic-models}
From the probabilistic modeling perspective, feature learning
can be interpreted as an attempt to recover a parsimonious set of latent random
variables that describe a distribution over the observed data.
$ p (x,h)  $ is the probabilistic model over the joint space
of latent variables $ h  $ and observed data $ x  $.
Feature values are then the result of an inference process to determine the probability
distribution of the latent variables given the data, i.e. $ p (h|x)  $,
a.k.a posterior probability.
Learning is the finding the parameters
that (locally) maximize the regularized likelihood of the training data.

\subsubsection{Directed graphical models}
\textit{Directed latent factor models} separately parametrize
$ p (x|h)  $ and the prior $ p (h)  $ to construct
$ p (x,h) = p (x|h) p (h)  $.
They can explain away: a priori independent causes of an event
can become nonindependent given the observation of the event.
Hence, they can be conceived them as causal models, where $ h  $ activations
cause the observed $ x  $, making $ h  $ nonindependent.
This makes recovering the posterior $ p (h|x)  $ intractable.

\subsubsection{Directly learning a parametric map from input to representation}
The posterior distribution becomes complicated quickly.
Thus approximate inference becomes necessary, which is not ideal.
Also, depending on the problem, one needs to derive feature vectors
from the distribution.
If we want deterministic feature values in the end, we might as well
go ahead and use a nonprobabilistic feature learning paradigm.
Doing so is particularly desirable for representations for model-free 
reinforcement learning algorithms:
since the data distribution is not explicitly used to make plans,
the stochasticity inherent in statistical modelling hinders the ability
of the reinforcement learning algorithm to use those representations.


\subsection{Discriminative models}
In discriminative modelling the data distribution is not directly represented.
Instead, it is implicit in the representation space. 
One way to learn discriminative models is through contrastive representation learning.
Intuitively, it's learning by comparing.
So instead of needing data labels $ y  $ for datapoints $ \bm{x}$,
you need to define a similarity distribution which allows you to
sample a positive input $ \bm{x}_{}^{ + } \sim p^{ + } (\cdot | \bm{x}_{})  $
and a data distribution for a negative input $ \bm{x}_{}^{ - } \sim p^{ - } (\cdot | \bm{x}_{})  $,
with respect to an input sample $ \bm{x}_{}  $.
``Similar'' inputs should be mapped close together, and ``dissimilar'' samples
should be mapped further away in the embedding space.

Let's explain how this would work with the example of image-based instance discrimination.
The goal is to learn a representation by maximizing agreement of the encoded features (embeddings)
between two differently augmented views of the same images,
while simultaneously minimizing the agreement between different images.
To avoid model maximizing agreement through low-level visual cues, views
from the same image are generated through a series of strong image augmentation methods.
Let $ \mathcal{T}  $ be a set of image transformation operations where
$ t, t' \sim \mathcal{T}  $ are two different transformations sampled independently from $ \mathcal{T}  $.
There transformations include for example cropping, resizing, blurring, 
color distortion or perspective distortion and their combinations.
A $ (\bm{x}_{q}, \bm{x}_{k})  $ pair of query and key views is positive when these 2 views
are created with different transformations on the same image,
i.e. $ \bm{x}_{q} = t (\bm{x}_{})  $ and $ \bm{x}_{k} = t' (\bm{x}_{})  $,
and is negative otherwise.
A feature encoder $ e (\cdot)  $ then extracts feature vectors from all augmented data samples 
$ \bm{v}_{} = e (\bm{x}_{})  $. This is usually ResNet, in which case 
$ \bm{v}_{} \in \mathcal{R}^{ d }  $ is the output of the average pooling layer.
Each $ v  $ is then fed into a projection head $ h (\cdot)  $ made up of
a small multi-layer perceptron to obtain a metric embedding $ \bm{z}_{} = h (\bm{v}_{})  $,
where $ \bm{z}_{} \in \mathcal{R}^{ d' }  $ with $ d' < d  $.
All vectors are then normalized to be unit vectors.
Then you take a batch of these metric embedding pairs $ \left\{ (\bm{z}_{i}, \bm{z}_{i}') \right\}   $,
with $ (\bm{z}_{i}, \bm{z}_{i}')  $ being the metric embeddings of
$ (\bm{x}_{q}, \bm{x}_{k})  $ of the same image
are fed into the contrastive loss function which does what we said 3 times already.
The general form of popular loss function such as InfoNCE and NT-Xent
is:
\begin{equation}
		\mathcal{L}_{ i } = - \log \frac{\exp (\bm{z}_{i}^{ T }\bm{z}_{i}'/\tau)}{\sum_{j=0}^{K} \exp (\bm{z}_{i} \cdot \bm{z}_{j}')/\tau} 
\end{equation}
where $ \tau  $ is the temperature parameter.
The sum is over one positive and $ K  $ negative pairs in the same minibatch.


\subsection{Common representation learning approaches}
\subsubsection{Deterministic autoencoders}
Deterministic autoencoders are generative models.
%TODO: move text about them here.
\begin{equation}
		h^{ (t) } = f_{ \theta } (x^{ (t) })
\end{equation}
There's also the reconstruction $ r = g_{ \theta } (h)  $,
used for the reconstruction error $ L (x,r)  $ over the
training examples.
Autoencoder training boils down to finding $ \theta  $
which minimizes:
\begin{equation}
		\mathcal{J}_{ AE } (\theta) =
		\sum_{t}^{} L (x^{ (t) }, g_{ \theta } (f_{ \theta } (x^{ (t) })))
\end{equation}
One can tie the weights between the encoder and the decoder (i.e. make the same ones,
just reversed).




\subsubsection{Variational autoencoders}
%TODO: delete half of this or add the other half
Variational autoencoders marry graphical models and deep learning.
The generative model is a Bayesian network of form
$ p (\bm{x}_{}| \bm{z}_{})p (\bm{z}_{})  $,
or in the case of multiple stochastic layers, a hierarchy such as:
$ p (\bm{x}_{}| \bm{z}_{L})p (\bm{z}_{L}|\bm{z}_{L-1})\cdots p (\bm{z}_{1}|\bm{z}_{0})  $.
Similarly, the recognition model is also a conditional Bayesian network of
form $ p (\bm{z}_{}|\bm{x}_{})  $ which can also be a hierarchy of
stochastic layers.
Inside each conditional may be a deep neural network,
e.g. $ \bm{z}_{}|\bm{x}_{} \sim f (\bm{x}_{}, \bm{\epsilon}_{})  $
with $ f  $ being the neural network mapping and $ \bm{\epsilon}_{}  $ a
noise random variable.
Its learning algorithm is a mix of classical (amortized, variational)
expectation maximization, but with the reparametrization trick
ends up backpropagating through the many layers of the deep neural networks
embedded inside it.


We can parametrize conditional distributions with neural networks.
VAEs in particular work with \textit{directed} probabilistic models,
also know as \textit{probabilistic graphical models} (PGMs)
or \textit{Bayesian networks}.
The joint distribution over the variables of such models
factorizes as a product of prior and conditional distributions:
\begin{equation}
p_{ \bm{\theta}_{} } (\bm{x}_{1}, \dots, \bm{x}_{M}) =
\prod_{j=1}^{M} p_{ \bm{\theta}_{} } (\bm{x}_{j}| Pa (\bm{x}_{j})) 
\end{equation}
where $ P a (\bm{x}_{j})  $ is the set of parent variables of node $ j  $ in
the directed graph. For root nodes the parents are an empty set,
i.e. that distribution is unconditional.
Before you'd parametrize each conditional distribution with
ex. a linear model, and now we do it with neural networks:
\begin{align}
		\bm{\eta}_{} &= \text{NeuralNet} (P a (\bm{x}_{}))\\
		p_{ \bm{\theta}_{} } (\bm{x}_{}|Pa (\bm{x}_{})) &= p_{ \bm{\theta}_{} } (\bm{x}_{}|\bm{\eta}_{})
\end{align}

To solve intractabilities, we introduce
a parametric \textit{inference model} $ q_{ \bm{\phi}_{}} (\bm{z}_{}|\bm{x}_{})  $.
This model is called the \textit{encoder} or \textit{recognition model}/
$ \bm{\phi}_{}  $ are called the \textit{variational parameters}.
They are optimized s.t.:
\begin{equation}
		 q_{ \bm{\phi}_{}} (\bm{z}_{}|\bm{x}_{})  \approx
p_{ \bm{\theta}_{} } (\bm{z}_{}|\bm{x}_{})
\end{equation}
Like a DLVM, the inference model can be almost any directed graphical model:
\begin{equation}
		q_{ \bm{\phi}_{}} (\bm{z}_{}|\bm{x}_{}) = 
		q_{ \bm{\phi}_{}} (\bm{z}_{1}, \dots, \bm{z}_{M}|\bm{x}_{}) =
		\prod_{j=1}^{M} q_{ \\bm{\phi}_{} (\bm{z}_{j}| P a (\bm{z}_{j}), \bm{x}_{}) } 
\end{equation}
This can also be a neural network.
In this case, parameters $ \bm{\phi}_{}  $ include the weights and biases, ex.
\begin{align}
		(\bm{\mu}_{}, \log \bm{\sigma}_{}) &= \text{EncoderNeuralNet}_{ \bm{\phi}_{} } (\bm{x}_{})\\
		q_{ \bm{\phi}_{} } (\bm{z}_{}|\bm{x}_{}) &=
		\mathcal{N} (\bm{z}_{}; \bm{\mu}_{}, \text{diag} (\bm{\sigma}_{}))
\end{align}
Typically, one encoder is used to perform posterior inference
over all of the datapoints in the dataset.
The strategy used in VAEs of sharing variational parameters across datapoints is also called
\textit{amortized variational inference}.


\subsubsection{Deterministic autoencoder regularization}
\label{ae-regularization}
%Sparsity penalty, adding data-augmentation to it.
Autoencoders may be employed not only just to learn representations, 
but to perform additional auxiliary tasks.
One such task is denoising: provided a noisified input at the encoder,
the decoder outputs a denoisified image as output.
Importantly, while this training process results in a denoising autoencoder,
it also regularizes the autoencoder.
Regularization not only helps with preventing overfitting, but also
produces better representations as it encourages smoothness
and spatial coherence of when learning.
The same result can be accomplished by other data augmentation techniques
like random cropping.

%TODO: reword so that the spice flows.

Learning VAEs from data poses unanswered theoretical questions and considerable practical challenges.
This work proposes a generative model that is simpler, deterministic, easier to train,
while retaining some VAE advantages.
Namely, the observation is that sampling a stochastic encoder in Gaussian VAE can be interpreted as injecting
noise into the input of a deterministic decoder.

The encoder deterministically maps a data point $ \bm{x}_{}  $ to
the mean $ \mu_{ \phi } (\bm{x}_{})  $ and variance $ \sigma_{ \phi } ( \bm{x}_{})  $
in the latent space.
The input to $ D_{ \theta }  $ is then the mean $ \mu_{ \phi } (\bm{x}_{})  $
augmented with Gaussian noise scaled by $ \sigma_{ \phi } (\bm{x}_{})  $
via the reparametrizing trick.
Authors argue that this noise injection is a key factor in having a regularized decoder (
noise injection as a mean to regularize neural networks is a well-known technique).
Thus training the RAE involves minimizing the simplified loss:
\begin{equation}
		\mathcal{L}_{ \text{RAE} } = 
\mathcal{L}_{ \text{REC} } + \beta \mathcal{L}^{ \text{RAE} }_{ \bm{z}_{} } 
+ \lambda \mathcal{L}_{ \text{REG} }
\end{equation}
where $ \mathcal{L}_{ \text{REG} }  $ represents the explicit regularizer for $ D_{ \theta }  $,
and $ \mathcal{L}^{ \text{RAE} }_{ \bm{z}_{} } = \frac{1}{2} ||\bm{z}_{}||_{ 2 }^{ 2 }  $,
which is equivalent to constraining the size of the learned latent space, which is needed
to prevent unbounded optimization.
One option for $ \mathcal{L}_{ \text{REG} }  $ is Tikhonov regularization
since it is known to be related to the addition of low-magnitude input noise.
In this framework this equates to 
$ \mathcal{L}_{ \text{REG} } = \mathcal{L}_{ L_{ 2 } } = ||\theta||^{ 2 }_{ 2 } $.
There's also the \textbf{gradient penalty} and
\textbf{spectral normalization}.


%\subsubsection{Siamese networks}
%Networks that share parameters.


\section{Representation models for control}
\label{sec-srl-for-control}
In state representation learning the learned features
are of low dimension, evolve through time and are depended
on actions of an agent.
The last point is particularly important because in reinforcement learning,
features that do not influence the agent and that can not be influenced
by the agent are not relevant for the problem of optimally controlling the agent.
Also, simply reducing the dimensionality of the input to a reinforcement learning
agent results in a computationally easier learning problem,
which can make a difference between the solution being feasible or infeasible.
Ideally, state  representation learning should be done in an without explicit supervision
as it can then be done in tandem with the likewise unsupervised reinforcement learning.

While we assume that state-transitions have the Markov property,
partial observability denies the possibility of having a one-to-one
correspondence between each observation and state ---
an object whose position is required may be occluded by another.
Thus prior observations have affect the mapping to the current state.
Images in particular also do not encode kinematic or dynamic information:
to get that crucial information a sequence of images is required.
Hence we define the SRL task as learning
a representation $ \tilde{\bm{s}}_{t} \in \tilde{\cal{S}}  $ of dimension $ K  $
with characteristics similar to those of true states $ \bm{s}_{t} \in \mathcal{S} $.
In particular, the representation is a mapping of the history of 
observation to the current state: $ \tilde{\bm{s}}_{t} = \phi(\bm{o}_{1:t}  $.
Actions $ \bm{a}_{1:t}  $ and rewards $ r_{ 1:t }  $ can also be added
to the parameters of $ \phi  $.
This can help in extracting only the information relevant for the agent and its task.
Often the representation is learned by using the reconstruction loss;
$ \hat{\bm{o}_{t}}  $ denotes the reconstruction of $ \bm{o}_{t}  $.

In the context of reinforcement learning, state representations should
ideally have the following properties:
\begin{itemize}
		\item have the Markov property 
		\item be able to represent the current state well enough for policy improvement
		\item be able to generalize to unseen states with similar features
		\item be low dimensional
\end{itemize}
We now discuss different types of models and learning strategies which 
can be used to learn state representations.

One way to do this is to explicitly use such methods to learn 
a function which maps from observations to states
and then use reinforcement learning methods these learned state
representations.
This approach is explored in this section, mainly with the help
of the 
\cite{srloverview}
overview paper.
In this section  state representation learning for control in general is discussed.
as this will allow for a broader contextualization of our own work.


With representation in learning introduced in general,
we can now introduce four different strategies for learning latent space models for control:
the autoencoder, the forward model, the inverse model and the model
with prior.
These models refer to portions of the control problem they are modelling.\footnote{The term
autoencoder is overloaded in this case.}
They can be both discriminative and generative models.
In the figures below, the white nodes are inputs and the gray nodes are outputs.
The dashed rectangles are fitted around variables with which the loss is calculated.

\subsection{Autoencoder}
The idea behind the autoencoder is to just learn a lower-dimensional embedding
of the observation space. This should make the learning problem easier due to the
dimensionality reduction.
The auto-encoder may be trained to denoise the observations by passing an observation
with artificially added noise to the encoder, but then calculating the reconstruction
loss on the image without the added noise.
Formally this can be written as
\begin{align}
		\bm{s}_{t} &= \phi (\bm{o}_{t}; \theta_{ \phi }) \\
		\hat{\bm{o}_{t}} &= \phi^{ -1 } (\bm{s}_{t}; \theta_{ \phi^{ -1 } })
\end{align}
where $ \theta_{ \phi }  $ and
$ \theta_{ \phi^{ -1 } }  $ are the parameters learned for the encoder and decoder respectively.

\begin{figure}[htpb]
\begin{center}
\begin{tikzpicture}[scale=1, transform shape, node distance=1.5cm]
		\node (st) [mcsb] {$\bm{s}_{t} $};
		\node (ot) [mcs, below of=st] {$\bm{o}_{t}$};
		\node (othat) [mcsb, right of=ot] {$\hat{\bm{o}}_{t} $};
		\draw [arrow] (ot) -- (st);
		\draw [arrow] (st) -- (othat);
		\node[draw,dashed,inner sep=1.5mm,fit=(ot) (othat) ] {};
\end{tikzpicture}
\end{center}
		\caption{Auto-encoder: learned by reconstructing the observation (one-to-one).
				The observation is the input and the computed state is the vector at
				the auto-encoder's bottleneck layer, i.e. is the output of the encoder
				part of the auto-encoder network.
		The loss is calculated between the true observation and the reconstructing observation (which
		is obtained by passing the observation though both the encoder and the decoder).}
\end{figure}

\subsection{Forward model}
The auto-encoder does not encode dynamic information.
Since that information is necessary for control, usually a few consequtive
observations (or their embeddings) are stacked and passed to the reinforcement learning algorithm.
This way the information about the dynamics is implicitly provided.
While doing so works, it could be made more efficient by embedding the dynamic
information as well.
One way to achieve this is to trained a model to predict future state representations.
A model can also be observations directly, 
of course provided that the network in question has a bottleneck layer from which
the learned representations can be extracted.
Since learning on sequential information is difficult and would also benefit from
lowering the dimensionality, learning a forward model can be done in two steps:
first, learning an auto-encoder to embed individual frames and then 
learning a predictive model in the embedded space.
In the schematic we show the case where predictions are learned from embeddings
because it is the structurally more complex scheme.
Formally, we have
\begin{equation}
		\hat{\tilde{\bm{s}}}_{ t+1 } = f (\tilde{\bm{s}_{t}}, \bm{a}_{t}; \theta_{ \text{forward} })
\end{equation}
%This however supposes that all states (and the corresponding observations) are accessible 
%prior to the beggining of the training process.
The forward model can be constrained to have linear transition between 
$ \tilde{\bm{s}}_{t}  $ and $ \tilde{\bm{s}}_{t+1}  $, thereby
imposing simple linear dynamics in the learned state space.
Depending on the problem, if this is done well enough, learning a control law can be avoided and instead
schemes like model-predictive control can be employed.

\begin{figure}[htpb]
\begin{center}
\begin{tikzpicture}[scale=1, transform shape, node distance=1.5cm]
		\node (at) [mcs] {$\bm{a}_{t} $};
		\node (st) [mcsb, below of=at] {$\tilde{\bm{s}}_{t} $};
		\node (sthatplus1) [mcsb, right of=at] {$\hat{\tilde{\bm{s}}}_{t+1} $};
		\node (stplus1) [mcsb, right of=st] {$\tilde{\bm{s}}_{t+1} $};
		\node (ot) [mcs, below of=st] {$\bm{o}_{t}$};
		\node (otplus1) [mcs, right of=ot] {$\hat{\bm{o}}_{t} $};
		\draw [arrow] (at) -- (sthatplus1);
		\draw [arrow] (st) -- (sthatplus1);
		\draw [arrow] (ot) -- (st);
		\draw [arrow] (otplus1) -- (stplus1);
		\node[draw,dashed,inner sep=1.5mm,fit=(sthatplus1) (stplus1) ] {};
\end{tikzpicture}
\end{center}
		\caption{Forward model: predicting the future state from the state-action pair.
				The loss is computer from comparing the predicted state against the true next state
				(the states being the learned states).
				This can also be done directly by predicting the next observation and comparing against it.
				}
\end{figure}



\subsection{Inverse model}
The introducing predictions solves the problem of not embedding the dynamic
information.
However, not all information in the observation is relevant for control.
Consider a computer game where images feature decorative backgrounds ---
those decorations are irrelevant for playing the game well.
If the reconstruction loss is computed from entire observation,
that information is also carried over into the embedded space.
However, if the model is trained to predict actions,
it is only incentivised to use information which the agent can affect.
Thus, due to less information being required,
the inverse model should produce a more compact embedding.
Formally, we can write this as:
\begin{equation}
		\hat{\bm{a}_{t}} = g (\tilde{\bm{s}_{t}}, \tilde{\bm{s}_{t+1}}; \theta_{ \text{inverse} })
\end{equation}
If the inverse model is neural network, we can recover the embedding by discarding
the last few layers and use their outputs to produce the embeddings.

\begin{figure}[htpb]
\begin{center}
\begin{tikzpicture}[scale=1, transform shape, node distance=1.5cm]
		\node (at) [mcs] {$\bm{a}_{t} $};
		\node (athat) [mcsb, below of=at] {$\hat{\bm{a}}_{t} $};
		\node (nothing) [below of=st] {};
		\node (sttilde) [mcsb, left of=nothing] {$\tilde{\bm{s}}_{t} $};
		\node (stildetplus1) [mcsb, right of=nothing] {$\tilde{\bm{s}}_{t+1} $};
		\node (ot) [mcs, below of=sttilde] {$\bm{o}_{t}$};
		\node (otplus1) [mcs, below of=stildetplus1] {$\bm{o}_{t+1} $};
		\draw [arrow] (ot) -- (sttilde);
		\draw [arrow] (otplus1) -- (stildetplus1);
		\draw [arrow] (sttilde) -- (athat);
		\draw [arrow] (stildetplus1) -- (athat);
		\node[draw,dashed,inner sep=1.5mm,fit=(at) (athat) ] {};
\end{tikzpicture}
\end{center}
		\caption{Inverse model: predicting the action between two consecutive states.
				The loss is computer from comparing the predicted action between two consecutive states
				against the true action that
				was taken by the agent between those two states.
				(the states being the learned states).
				}
\end{figure}

\subsection{Using prior knowledge to constrain the state space}
Of course, not everything need be learned in every problem.
While in general hand-engineered features are worse than learned ones,
there are other ways to provide prior knowledge to the learning system.
For example, convolutional neural network by their architecture encode
the fact that nearby pixels are related.
In the SRL context we already mention the possibility of constraining 
the model to linear transitions, but there are other available techniques
like for example
constraining temporal continuity or the principle of causality.
Furthermore, priors can be defined as additional objectives or loss functions.
For example, additional loss can be provided if embeddings from
consequtive observation are drastically different.
This is called the slowness principle.

\subsection{Using hybring objectives}
The approaches outlined thus far can be combined into hybrid
approaches, for example \cite{watter2015embed}.
%TODO: throw a reference or two from the overview paper you're going over,
%for example embed2control.


\section{Model-based reinforcement learning}
Like the name suggest, in model-based reinforcement learning a ``world model''
is learned.
While there exists a whole spectrum of methods between pure
model-free and model-based ones, the key distinguishing feature
of model-based methods is that the learned model is used to \textit{plan} actions.
In this case, the task of reinforcement learning in the narrow sense
is to learn the values of different states.
This then enables calculation of trajectories toward states with high rewards.
In model-free methods, only the following action is selected at on iteration of the process
because only the transition reward is learned and states these transitions
lead to are unknown (not explicitly modelled).
%TODO
%Introduce just the idea for the sole purpose of
%showing why we aren't doing model-based reinforcement learning,
%but instead opting for model-free with state representation learning.



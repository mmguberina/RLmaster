% CREATED BY DAVID FRISK, 2016
\chapter{Reinforcement learning}
\label{ch-rl-background}
%\section{Introduction to reinforcement learning}
%\label{sec-rl-intro}
\section{Problem setting}
\label{subsec-problem-setting}
In the usual engineering approach to problems,
prior scientific knowledge is used to first describe the problem
and then to define it mathematically.
Once this is done, unknown variables are measured and
solutions are calculated.
This approach works if the inherent stochasticity of the environment
can be controlled, i.e. if bounds of stochasticity are known
the solution account for them and be designed to be robust to them.
But some problems have circumstances which can not be known in advance,
or which are incredibly hard to hand-engineer.

In those cases, an entirely different approach becomes the only viable one:
designing a system which can produce and refine its own solution,
or in other words, designing a system which, in a way, learn the
solution by itself.
This is the idea behind the learning-based approach: automating the 
process of learning.
Crucially, now the world and how it operates
is unknown and has to be discovered.  
The schematic \ref{fig:rl-shematic} shows how this process is formulated in reinforcement learning.
\begin{figure}[htpb]
\begin{center}
\begin{tikzpicture}[scale=1, transform shape, node distance=2.0cm]
		\node (agent) [rec] {agent};
		\node (environment) [rec, right of=agent, xshift=1.5cm] {environment};
		\draw [arrow, xshift=0.5cm]  (environment.240) to [bend left=30] node [midway, below, yshift=-0.2cm] (textnode1) {observation, reward}  (agent.300);
		\draw [arrow, xshift=0.5cm]  (agent.70) to [bend left=30] node [midway, above, yshift=0.2cm] (textnode2) {action} (environment.100);
\end{tikzpicture}
\end{center}
\caption{Conceptual schematic of reinforcement learning.}
\label{fig:rl-shematic}
\end{figure}
Reinforcement learning is a 2-step iterative process.
The \textbf{agent}, which represents the computer program, takes \textbf{actions}
in its \textbf{environment}. It then \textbf{observes} the resulting state
of the environment and is also given a \textbf{reward}
which is a function mapping every state of the environment to a number.

To introduce reinforcement learning more formally,
we first describe the simplest possible problem 
to which reinforcement learning is the best solution.

\subsection{Bandit problems}
Reinforcement learning  uses training information that evaluates 
the actions taken rather than instruct by giving correct actions.
Consider this learning problem.
The agent is faced with $k$ different gambling slot machines.
Each of them give random rewards under an unknown distribution.
At each turn, the agents has to select one of the machines and pull its lever.
%The agents is repeatedly faced with a choice of k different actions.
%After each choice the agent receive a numerical reward based on the action selected.
The goal is to maximize the expected total reward over some number of turns.
%This is original form of k-armed bandit problem.
%Depending on the action selected $k$-armed bandit problem formalizes the value function to get the expected or average reward.
If the agent knew the distribution of rewards of each of the slot machines, 
it would simply choose the one with the highest expected reward in number of turns 
it has been given.
%It is assumed the agent do not know the action value with certainty although the estimate of 
%the action value is known by the agent and we expected it to be close to actual value of the action.
At any time step the agent will be able to select at least one action whose estimated value is greatest.
When the agent selects on of this actions it is exploiting the current knowledge of the
values of the actions.
If the agents keep exploiting the goal of maximizing reward over period of time will be trivial.
If instead the agent selects one of the non greedy action this will enables it to improve the average expected reward over time.
 
When addressing the canonical problem of sequential decision making under uncertainty,
the exploitation-exploration trade-off is highlighted.More specifically,
as depicted in Fig.1, an agent interacts with an unknown environment in a sequential manner to obtain rewards.
The ultimate goal is to maximize the rewards.
For one thing, the agent takes advantage of existing knowledge of the environment.
For another, the agent investigates an unfamiliar environment.

 
\subsection{Markov Decision Processes}
\label{subsec-mdps}
%\cite{gehring2017convolutional}
The environment of $ k  $-bandit problem is static --- 
the actions do not change the \textbf{state} of the environment.
To model environments in which states change, Markov chains are used.
They capture the stochastic nature of state transitions, while Markov property 
allows for easier mathematical analysis.
The schematic of a Markov chain is shown in \ref{fig:markov-chain}.

\begin{figure}[htpb]
\begin{center}
\begin{tikzpicture}[scale=1, transform shape, node distance=1.5cm]
		\node (s1) [mcs] {$\bm{s}_{1} $};
		\node (s2) [mcs, right of=s1, xshift=1cm] { $ \bm{s}_{2}  $};
		\draw [arrow] (s1) -- node [below, midway] {$ p(\bm{s}_{t+1}|\bm{s}_{t}) $} (s2);
		\node (s3) [mcs, right of=s2, xshift=1cm] { $ \bm{s}_{3}  $};
		\draw [arrow] (s2) -- node [below, midway] {$ p(\bm{s}_{t+1}|\bm{s}_{t}) $} (s3);
\end{tikzpicture}
\end{center}
\caption{Schematic of a Markov chain.}
\label{fig:markov-chain}
\end{figure}

Formally, a Markov chain $ \mathcal{M}  $is defined by its state space
$ \mathcal{S}  $ with discrete or continuous state $ \bm{s} \in \mathcal{S}  $
and the transition operator $ \mathcal{T}  $.
The notation $ \bm{s}_{ t }  $ denotes the state at time $ t  $ and it is a vector of real numbers.
The transition operator allows for a succinct description of environment dynamics.
For a transition probability $ p(s_{ t+1 }|s_{ t })  $,
let $ \mu_{ t,i } = p (\bm{s}_{t} = i)  $ and
$ \mathcal{T}_{ i,j } = p (\bm{s}_{t+1} = i|\bm{s}_{t} = j )  $.
Then $\overrightarrow{\mu}_t$ is a vector of probabilities and 
$\overrightarrow{\mu}_{t+1} = \mathcal{T} \overrightarrow{\mu}_t$.
Importantly, $ \mathcal{T}  $ is linear.

To model the agent's actions, we simply augment the Markov chain by adding
actions as priors to state transition probabilities and defining the reward function, 
thereby constructing a Markov decision process.
It's schematic can be seen in \ref{fig:mdp}.
\begin{figure}[htpb]
\begin{center}
\begin{tikzpicture}[scale=1, transform shape, node distance=1.5cm]
		\node (s1) [mcs] {$\bm{s}_{1} $};
		\node (a1) [mcs, above right of=s1] { $ \bm{a}_{1}  $};
		\node (s2) [mcs, right of=s1, xshift=1.5cm] { $ \bm{s}_{2}  $};
		\draw [arrow] (a1) -- (s2);
		\draw [arrow] (s1) -- node [below, midway] {$ p(\bm{s}_{t+1}|\bm{s}_{t}, \bm{a}_{t}) $} (s2);
		\node (s3) [mcs, right of=s2, xshift=1.5cm] { $ \bm{s}_{3}  $};
		\node (a2) [mcs, above right of=s2] { $ \bm{a}_{2}  $};
		\draw [arrow] (s2) -- node [below, midway] {$ p(\bm{s}_{t+1}|\bm{s}_{t}, \bm{a}_{t}) $} (s3);
		\draw [arrow] (a2) -- (s3);
\end{tikzpicture}
\end{center}
\caption{Schematic of a Markov decision process.}
\label{fig:mdp}
\end{figure}

The Markov decision process is thus defined by the tuple
$ \mathcal{M} = \{\mathcal{S}, \mathcal{A}, \mathcal{T}, r\}  $.
$ \mathcal{A}  $ denotes the action space, where
$ \bm{a}_{} \in \mathcal{A}  $ is a continuous or discrete action and
$ r  $ is the reward function $r : \mathcal{S} \times \mathcal{A} \to \mathbb{R}$.
It should also be noted that the transition operator is now a tensor.
Let $\mu_{t,j} = p(s_t = j), \xi_{t,k} = p(a_t = k), \mathcal{T}_{i,j,k} = p(s_{t+1} = i | s_t =j, a_t =k) $.
Then $\mu_{t+1,i} = \sum_{j,k}^{} \mathcal{T}_{i,j,k} \mu_{t,j} \xi_{t,k}$.
Therefore, $ \mathcal{T}  $ retains its linearity.

Finally, partial observability also needs to be accounted for.
To do so, a partially observable Markov decision process (POMDP) needs to be constructed.
This is done by augmenting the Markov decision process to also include
the observation space $ \mathcal{O}  $, where observations $ \bm{o}_{} \in \mathcal{O} $
denote the discrete or continuous observations
and the emission probability $ \mathcal{E}  $ which describes the probability 
$ p(\bm{o}_{t} | \bm{s}_{t})  $ of getting the observation $ \bm{o}_{t}  $ when in state  $\bm{s}_{t}$.
The schematic can be seen in \ref{fig:pomdp}.

\begin{figure}[htpb]
\begin{center}
\begin{tikzpicture}[scale=1, transform shape, node distance=1.5cm]
		\node (s1) [mcs] {$\bm{s}_{1} $};
		\node (a1) [mcs, above right of=s1] { $ \bm{a}_{1}  $};
		\node (o1) [mcs, above of=s1] { $ \bm{o}_{1}  $};
		\draw [arrow] (s1) -- (o1);
		\node (s2) [mcs, right of=s1, xshift=1.5cm] { $ \bm{s}_{2}  $};
		\node (o2) [mcs, above of=s2] { $ \bm{o}_{2}  $};
		\draw [arrow] (s2) -- (o2);
		\draw [arrow] (a1) -- (s2);
		\draw [arrow] (s1) --  (s2);
		\node (s3) [mcs, right of=s2, xshift=1.5cm] { $ \bm{s}_{3}  $};
		\node (o3) [mcs, above of=s3] { $ \bm{o}_{3}  $};
		\draw [arrow] (s3) -- (o3);
		\node (a2) [mcs, above right of=s2] { $ \bm{a}_{2}  $};
		\draw [arrow] (s2) -- (s3);
		\draw [arrow] (a2) -- (s3);
\end{tikzpicture}
\end{center}
\caption{Schematic of a partially observable Markov decision process.}
\label{fig:pomdp}
\end{figure}
It is important to note that not all elements of POMDP are present in every problem: for example,
the reward may be a deterministic function of the state and so on.
In general through the text, to aid in simplifying notation, only the necessary elements will be explicity referenced
in sketches and written out in the equations (most often using just the Markov decision process).
%However, POMDP fully describes the setting in which reinforcement learning algorithms can be defined.


%Markov decision processes (MDPs) are used to model decision making in discrete, stochastic, sequential environments,
%and are thus a natural choice of model in which reinforcement learning can be defined.
%[9].
%Markov Chain, which works with S, a set of states, and P, the probability of transitioning from one to the next. It also uses the Markov Property, meaning each state depends only on the one immediately prior to it.

%MDPs is a framework that can solve most of reinforcement learning problems with discrete actions.The formal definition of Markov decision process can be summarised in  the tuple $\mathcal{M} = \{\mathcal{S}, \mathcal{A},\mathcal{P(S_0)}, \mathcal{T}, r\}$[8].
%The goal of the model is that an agent inhabits a  stochastic environment in a response to action choice made by the agent[9]. \\

%The environment consists of the the Transition function, $\mathcal{T} : \mathcal{S} \times \mathcal{A} \to \mathcal{(PS)}$.\\  and the reward function ($r(s_t, a_t)$), $r : \mathcal{S} \times \mathcal{A} \to \mathbb{R}$.In MDPs the transition probability and reward only depends on the current state and the action chosen by the agent  but not the past state and action.

%The agent acts in environment according to a policy $\mathcal{\pi} : \mathcal{S} \to \mathcal{(PS)}$.policy learned can be off policy where the behaviour policy is different from the policy used for action selection one common example is Q-learning or on-policy methods which attempts to evaluate or improve the policy that is used to make decision.\\
%The former state-action value function (Q function for short)can be computes recursively with dynamic programming: 


\section{Key concepts in reinforcement learning}
\label{subsec-key-rl-concepts}
\subsection{Policy}
With the problem space being formally defined,
we may introduce definitions which will allow the construction
of a reinforcement learning algorithm.
The reinforcement learning problem can be defined in finite or infinite time horizons.
Different environments usually naturally fall in either category.
For the agent to learn, it needs to be able to try out different actions from the same, 
or at least similar states.
This is usually achieved by having the agent return to a set of starting states.
The period between two such returns is called \textbf{an episode}.
The agent selects actions based on its \textbf{policy} $ \pi  $.
The policy is a function which maps states to actions.
The schematic showing it in the context of a Markov decision process
is given in \ref{fig:policy-in-mdp}.
\begin{figure}[htpb]
\begin{center}
\begin{tikzpicture}[scale=1, transform shape, node distance=2.5cm]
		\node (s1) [mcs] {$\bm{s}_{1} $};
		\node (a1) [mcs, above right of=s1] { $ \bm{a}_{1}  $};
		\node (s2) [mcs, right of=s1, xshift=1.5cm] { $ \bm{s}_{2}  $};
		\draw [arrow] (a1) -- (s2);
		\draw [arrow] (s1) -- node [below, midway] {$ p(\bm{s}_{t+1}|\bm{s}_{t}, \bm{a}_{t}) $} (s2);
		\node (s3) [mcs, right of=s2, xshift=1.5cm] { $ \bm{s}_{3}  $};
		\node (a2) [mcs, above right of=s2] { $ \bm{a}_{2}  $};
		\draw [arrow] (s2) -- node [below, midway] {$ p(\bm{s}_{t+1}|\bm{s}_{t}, \bm{a}_{t}) $} (s3);
		\draw [arrow] (a2) -- (s3);
		\draw [arrow] (s1) -- node [above, midway, sloped] {$\pi_{ \theta } (\bm{a}_{t}| \bm{s}_{t} )  $} (a1);
		\draw [arrow] (s2) -- node [above, midway, sloped] {$\pi_{ \theta } (\bm{a}_{t}| \bm{s}_{t} )  $} (a2);
\end{tikzpicture}
\end{center}
\label{fig:policy-in-mdp}
\end{figure}

The policy is a stochastic function. The intensity of stochasticity determines the trade-off
between exploration and exploitation.
To emphasize that the policy depends on some parameters $ \theta  $,
we usually write $ \pi_{ \theta } $.

\subsubsection{Goal of reinforcement learning}
For simpler notation, the finite horizon form
is assumed for the following definitions.
Since the environment is modeled as a Markov decision process,
we can write the probability of observing a trajectory of states
and actions as:

\begin{equation}
\underbrace{p_\theta(\bm{s}_1, \bm{a}_1, \dots, \bm{s}_T, \bm{a}_T)}_{p_\theta(\tau)} = p(\bm{s}_1) \prod^{T}_{t=1} 
\underbrace{\pi_{\theta} (\bm{a}_t | \bm{s}_t) p (\bm{s}_{t+1} | \bm{s}_t, \bm{a}_t)}_{\text{Markov chain on} (\bm{s}, \bm{a})}
\end{equation}
A bit more explicitly, we can a transition probability as:
\begin{equation}
p((\bm{s}_{t+1}, \bm{a}_{t+1}) | (\bm{s}_t, \bm{a}_t)) = 
p((\bm{s}_{t+1}| (\bm{s}_t, \bm{a}_t)) \pi_\theta (\bm{a}_{t+1} | \bm{s}_{t+1})
\end{equation}

With this, we may now formally define the goal of reinforcement learning.
It is to find policy parameters $ \theta^{ \star }  $ such that:
\begin{align}
		\theta^\star &= \argmax_{\theta} E_{\tau \sim p_\theta(\tau)} \left[ \sum_{t}^{} r(\bm{s}_t, \bm{a}_t) \right] \\
	 &= \argmax_{\theta} \sum_{t}^{T} E_{(\bm{s}_t, \bm{a}_t) \sim p_\theta(\bm{s}_t, \bm{a}_t)} \left[  r(\bm{s}_t, \bm{a}_t) \right]
\end{align}

To ensure that the expected sum of rewards, also know as the \textbf{return},
is finite in the infinite horizon case, a
\textbf{discount factor} $ 0 < \gamma < 1  $ is introduced in the sum.
The discount factor also play a role in modelling 
because often times it makes sense to value immediate rewards
more.
It is important to note that we are maximizing the \textit{expected} sum of
rewards. This makes the goal a smooth and differentiable function of the parameters,
which means we can employ gradient descent to find the optimal parameters.
This leads us to the first class of reinforcement learning algorithms:
policy gradient algorithms.
They will be introduced with the other classes of algorithm in the next subsection,
while here additional concepts required by other classes of algorithms will be introduced here.

\subsection{Value functions}
Value functions are functions which map states or state-action pairs
to the expected returns obtained under a fixed policy.
They are a concept from dynamic programming. In fact,
reinforcement learning can be interpreted as an extension of dynamic programming,
as shall be done in the following subsection.
Having that said, value function can be interpreted in other ways as well.
The \textbf{Q-function} maps state-action pairs to the estimated sum of returns
under policy $ \pi_{ \theta } $:
\begin{equation}
		\label{eq:q-function}
		Q^\pi (\bm{s}_t, \bm{a}_t) = \sum_{t'=t}^{T} E_{\pi_\theta}
		\left[ r(\bm{s}_{t'}, \bm{a}_{t'} )| \bm{s}_t, \bm{a}_t \right] 
\end{equation}
thus denoting the total reward from taking $\bm{a}_t$ in $\bm{s}_t$.
\textbf{Value functions} map states to to the estimated sum of returns
under policy $ \pi_{ \theta } $:
\begin{equation}
		\label{eq:value-function}
		V^\pi (\bm{s}_t) = \sum_{t'=t}^{T} E_{\pi_\theta}
		\left[ r(\bm{s}_{t'}, \bm{a}_{t'} | \bm{s}_t) \right] 
\end{equation}

The connection between the two is the following:
\begin{equation}
		V^\pi (\bm{s}_t) = E_{\bm{a}_t \sim \pi(\bm{s}_t, \bm{a}_t)}
		\left[ Q^\pi(\bm{s}_t, \bm{a}_t) \right] 
\end{equation}
And we can also write the RL objective as:
\begin{equation}
		E_{\bm{s}_1 \sim p(\bm{s}_1)}
		\left[ V^\pi (\bm{s}_1) \right] 
\end{equation}


\section{Classes of reinforcement learning algorithms}
\label{sec-rl-alg-classes}
\subsection{Policy gradients}
Policy gradients are derived by directly solving for
the reinforcement learning objective with gradient descent
with respect to the policy parameters.
To do so, the reinforcement learning objective needs to be evaluated.
We begin by introducing a notational shorthand:

\begin{equation}
		\theta^\star = \argmax_\theta \underbrace{E_{\tau \sim p_\theta (\tau)} \left [ \sum_t r(\bm{s}_t, \bm{a}_t) \right ]}_{J(\theta)}
\end{equation}

We estimate $J(\theta)$ by making rollouts from the policy (below $i$ is the sample index and $i,t$ is the $t^{th}$ timestep
in the $i^{th}$ sample):
\begin{equation}
		J(\theta) = E_{\tau \sim p_\theta(\tau)} \left [ \sum_t r(\bm{s}_t, \bm{a}_t) \right ] \approx 
		\frac{1}{N} \sum_i \sum_t r(\bm{s}_{i,t}, \bm{a}_{i,t})
\end{equation}
Simplifying the notation further, we get:
\begin{equation}
		J(\theta) = E_{\tau \sim p_\theta(\tau)} \underbrace{[r(\tau)]}_{\sum^{T}_{t=1} r(\bm{s}_t, \bm{a}_t)} = 
		\int_{{}}^{} {p_\theta(\tau)r(\tau)} \: d{\tau} {}
\end{equation}
The goal now is to compute the derivative of the estimated reinforcement learning
objective:
\begin{equation}
		\label{eq:derivative-of-estimated-rl-obj}
		\nabla_\theta J(\theta) = \int_{{}}^{{}} {\nabla_\theta p_\theta (\tau) r(\tau)} \: d{\tau} {}
\end{equation}

Since the goal of this text is just to introduce the necessary concepts
and algorithms, the derivation(s) will be ommited.
We encourage the interested reader to consult the literature \cite{suttonrlbook}
and \cite{berkleylectures} to find them.

Here we will just note that it is crucial that the final expression
can be estimated by sampling the agent's experience 
as the other quantities are not available.
The resulting expression for the policy gradient \label{eq:derivative-of-estimated-rl-obj} is:

\begin{equation}
		\label{eq:policy-gradient}
		\nabla_\theta J(\theta) = E_{\tau \sim p_\theta(\tau)} 
		\left [ \left ( \sum_{t=1}^{T} \nabla_\theta \log \pi_\theta (\bm{a}_t | \bm{s}_t ) \right )
		\left ( \sum_{t=1}^{T} r(\bm{s}_t, \bm{a}_t) \right ) \right ]
\end{equation}
To evaluate the policy gradient we can sample:
\begin{equation}
		\label{eq:estimated-policy-gradient}
		\nabla_\theta J(\theta) \approx \frac{1}{N}  \sum_{i=1}^{N} 
		\left ( \sum_{t=1}^{T} \nabla_\theta \log \pi_\theta (\bm{a}_{i,t} | \bm{s}_{i,t} ) \right )
		\left ( \sum_{t=1}^{T} r(\bm{s}_{i,t}, \bm{a}_{i,t}) \right )
\end{equation}
With the gradient we can do a step of gradient ascent and use it to form
the REINFORCE algorithm, also known as ``vanilla policy gradient'':

\fbox{
		\parbox{\textwidth}{
				\underline{REINFORCE algorithm:}
\begin{enumerate}
		\item sample $\{\tau^i\}$ from $\pi_\theta(\bm{a}_t | \bm{s}_t)$ by running the policy
		\item $\nabla_\theta J(\theta) \approx   \sum_{i}^{} 
		\left ( \sum_{t}^{T} \nabla_\theta \log \pi_\theta (\bm{a}_{i,t} | \bm{s}_{i,t} ) \right )
		\left ( \sum_{t}^{} r(\bm{s}_{i,t}, \bm{a}_{i,t}) \right )$
\item $\theta \leftarrow \theta + \alpha \nabla_\theta J(\theta) $
\end{enumerate}
}}

This algorithm does not work well in practice.
The main reason for that is that the variance of returns
is very high. 
However, there are a number of modifications which dramatically improve its performance.
Since the goal of this text is not to outline every reinforcement learning algorithm,
we will introduce only the modifications which outline
general trade-offs and principles in reinforcement learning algorithm design.

\subsubsection{Baselines}
The policy gradient in the REINFORCE algorithm lacks some important properties.
One of them is that it should, ideally, make bad actions less likely
and good actions more likely. 
However, if all rewards are positive, then all actions' probabilities will be increased,
only by different amounts.
This can be changed if a \textbf{baseline} $ b  $is added to actions:
\begin{align}
		\nabla_\theta J(\theta) &\approx 
		\frac{1}{N} \sum_{i=1}^{N}
		\nabla_\theta \log p_\theta (\tau) [ r(\tau) - b] \\
		b &= \frac{1}{N} \sum_{i=1}^{N} r(\tau)
\end{align}
This addition does not change the gradient in expectation, i.e. it does not
introduce bias,
but it does change its variance.
Although an optimal bias can be calculated, it is rarely used in practice due
to its computational cost.
Using baselines is one of the key ideas in actor-critic algorithms
so they will be discussed further there.

\subsubsection{Off-policy gradients}
An important property of the REINFORCE algorithm is that it
is an \textbf{on-policy} algorithm.
This means that new samples need to be collected for every gradient step.
The reason behind this is the fact that the expectation of the gradient 
of the return needs to be calculated with respect to the current parameters
of the policy.
In other words, because the policy changes with each gradient step,
old samples are effectively collected under a different policy.
This means that they can not be used to calculate the expected gradient 
of the return with respect to the current policy --- it would not
produce those trajectories.
In mathematical notation:
\begin{equation}
		\nabla_\theta J(\theta) = \underbrace{E_{\tau \sim p_\theta(\tau)}}_{\text{this is the trouble!}} [\nabla_\theta p_\theta(\tau)r(\tau)]
\end{equation}
If the policy is a neural network, which requires small gradient steps,
the cost of generating a large number of samples for every update
could make the algorithm entirely infeasible.
This of course depends on the cost of generating samples,
which is entirely problem dependent ---
policy gradient algorithms are often the best solution when 
the cost of generating samples is low.

However, on-policy algorithms can be turned into off-policy 
algorithms through \textbf{importance sampling},
which is the name given to the following mathematical identity:
\begin{align}
		E_{x \sim p(x)} [f(x)]  
		&= \int_{{}}^{{}} {p(x)f(x)} \: d{x} \\
		&= \int_{{}}^{{}} {\frac{q(x)}{q(x)}  p(x)f(x)} \: d{x} \\
		&= \int_{{}}^{{}} { q(x) \frac{p(x)}{q(x)}  f(x)} \: d{x} \\
		&= E_{x \sim p(x)} \left [ \frac{p(x)}{q(x)} f(x) \right ]
\end{align}
which is exact in expectation.
To use importance sampling to create an off-policy policy gradient algorithm,
certain approximations need to be made. Again, the details of the derivation
are omitted and what follows is just the final result.
\begin{equation}
		\nabla_{\theta'} J(\theta') \approx
		\frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T}
		\frac{\pi_{\theta'}( \bm{a}_{i,t} | \bm{s}_{i,t})}{\pi_{\theta}( \bm{a}_{i,t} | \bm{s}_{i,t})} 
		\nabla_{\theta'} \log \pi_{\theta'} (\bm{s}_{i,t}, \bm{a}_{i,t}) 
		\hat{Q}_{i,t} 
\end{equation}
To get this equation, the factor 
$ \frac{\pi_{\theta'}(\bm{s}_{i,t})}{\pi_{\theta}(\bm{s}_{i,t})}  $
had to be simple ignored in the expression because it is impossible 
to calculate the state marginal probabilities.
This means that the expression works only if $ \pi_{ \theta' }  $
is not too different from $ \pi_{ \theta }  $.

\subsubsection{Advanced policy gradients}
The basic algorithm we have outlined is essentially just a basic gradient descent method. 
From convex optimization, we know that it can be made much better if second order derivatives
or their approximations are used.
For example, conjugate gradient descent can be used.
Further, there are various ways in which this optimization problem can be better conditioned.
Such improvements led to algorithms such as PPO and TRPO,
which will not be discussed here.

\subsection{Actor-critic algorithms}
Actor-critic methods can be seen as making a different trade-off between 
variance and bias in policy gradient estimation.
We begin with the following observation:
\footnote{In this equation, the summation of rewards is done from time $t   $
		to $ T  $ because actions and states prior to that time do
		not affect the return from that time onward.
This leveraging of causality reduces the variance of the estimate.}
\begin{equation}
		\nabla_\theta J(\theta) \approx 
		\frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_\theta \log \pi_\theta (\bm{a}_{i,t}| \bm{s}_{i,t})
		\underbrace{\left ( \sum_{t'=t}^{T} r (\bm{s}_{i,t}, \bm{a}_{i,t}) \right )}_{\hat{Q}_{i,t}
		\text{: ``reward to go''}}
\end{equation}
Simply put, in the policy gradient method a single-run Monte-Carlo (MC) is used
to estimate the return.
This causes high variance, while incurring no bias.
Another option is to try to estimate the full expectation
$ \hat{Q}_{i,t} \approx  \sum_{t'=t}^{T} E_{\pi_\theta} \left[ r(\bm{s}_{t'}, \bm{a}_{t'}) |\bm{s}_{t}, \bm{a}_{t}  \right]     $.
Since the estimate won't be perfect, it will introduce bias.
Of course, using multiple runs from the same state-action pair would 
reduce variance, but this is sometimes impossible to procure
and is certainly more costly.
However, if our estimator of ``reward to go'' can generalize between states,
we will be able to get good estimates regardless.

Like the policy, the return estimator will have to be learned.
In this approach, the policy is also called the \textbf{actor} 
and the return estimator is called the \textbf{critic}.
We proceed by discussing how the critic can be constructed.
If we had the correct Q-function (i.e. not the estimate, but the actual values),
we could improve the policy gradient estimate by using it
both to estimate the return and as a baseline:
\begin{align}
		\nabla_\theta J(\theta) &\approx \frac{1}{N} \sum_{i=1}^{N} \sum_{t=1}^{T} \nabla_\theta \log \pi_\theta (\bm{a}_{i,t}| \bm{s}_{i,t})
( Q(\bm{s}_{i,t}, \bm{a}_{i,t}) - b) \\
		b_t &= \frac{1}{N} \sum_{i}^{} Q(\bm{s}_{i,t}, \bm{a}_{i,t})
\end{align}
However, having a baseline that depends on actions leads to bias.
Thus we employ a baseline dependent on the state:
\begin{equation}
		V(\bm{s}_t) = E_{\bm{a}_t \sim \pi_\theta (\bm{s}_{t}, \bm{a}_{t})} [Q(\bm{s}_{t}, \bm{a}_{t})]
\end{equation}
Since the value function \ref{eq:value-function} tells us the expected return of the average action,
we can calculate how much better a certain action is by substracting
its Q-value \ref{eq:q-function} for the value function.
The result is called the \textbf{advantage function}:
\begin{equation}
A^\pi (\bm{s}_{t}, \bm{a}_{t}) = Q^\pi (\bm{s}_{t}, \bm{a}_{t}) - V^\pi (\bm{s}_t)
\end{equation}
Thus we can fit either the Q-function, the value function or the advantage function.
Of these, it is best to learn the value function because there are less 
states than state-action pairs.
We then calculate the advantage function in the following way:
\begin{align}
		A^\pi (\bm{s}_{t}, \bm{a}_{t})  &\approx r(\bm{s}_{t}, \bm{a}_{t}) + V^\pi (\bm{s}_{t+1})  - V^\pi(\bm{s}_t)
\end{align}

The value function can be estimated through samples
\begin{align}
		V^\pi (\bm{s}_t) &\approx \sum_{t'=t}^{T} r(\bm{s}_{t'}, \bm{a}_{t'})
\end{align}
After collecting many such samples
\begin{equation}
		\left\{ \left( \bm{s}_{i,t}, \underbrace{\sum_{t'=t}^{T} r(\bm{s}_{i,t}, \bm{a}_{i,t})}_{y_{i,t}} \right)  \right\} 
\end{equation}
we can fit the value function through supervised regresion with the loss being:
\begin{equation}
		\mathcal{L}(\phi) = \frac{1}{2} \sum_{i}^{} ||\hat{V}^\pi_\phi (\bm{s}_i) - y_i||^2
\end{equation}
However, this process can be sped up with bootstraped estimates:
\begin{equation}
		y_{i,t} = \sum_{t'=t}^{T} E_{\pi_\theta} \left[ r(\bm{s}_{t'}, \bm{a}_{t'})|\bm{s}_{i,t}\right] + V^\pi(\bm{s}_{i,t+1})  
		\approx r(\bm{s}_{i,t}, \bm{a}_{i,t}) + \hat{V}^\pi_\phi(\bm{s}_{i,t+1}) 
\end{equation}
This will further reduce variance, but again increase bias.

Fortunatelly, we can tune the trade-off between bias and variance.
In the Monte Carlo estimate, the entire trajectory was used
to estimate the return. In the bootstrap estimate,
only a single step in the future was used along with the estimate.
Instead, a \textbf{n-step} return estimator can used:
\begin{equation}
		\hat{A}^\pi_n (\bm{s}_{t}, \bm{a}_{t}) =
		\sum_{t'=t}^{t+n} \gamma^{t'-t} r(\bm{s}_{t'}, \bm{a}_{t'})
		- \hat{V}^\pi_\theta(\bm{s}_t) + \gamma^n \hat{V}^\pi_\theta(\bm{s}_{t+n})
\end{equation}
In most cases
the ideal trade-off for $ n  $ lies somewhere between 1 and $\infty$ (the MC estimate).
Finally, an average of all n-step return estimators can be used.
This is called the generalized advantage estimator (GAE):
\begin{equation}
\hat{A}^\pi_{GAE} (\bm{s}_{t}, \bm{a}_{t}) =
\sum_{n=1}^{\infty} (\gamma \lambda)^{t'-t}r(\bm{s}_{t'}, \bm{a}_{t'}) + \gamma \hat{V}^\pi_\theta(\bm{s}_{t'+1})  - \hat{V}^\pi_\theta(\bm{s}_{t'})
\end{equation}
where the factor $ \lambda  $ controls the weight of future values.

Combining this into an iterative algorithm, and fixing the issues
of naiive implementations results in the following algorithm:

\fbox{
\parbox{\textwidth}{
\underline{Actor-critic algorithm template}
\begin{enumerate}
		\item take action $\bm{a} \sim \pi_\theta(\bm{a}|\bm{s})$, get $(\bm{s}, \bm{a},\bm{s'},r)$, store in $\mathcal{R}$ (replay buffer)
		\item sample a batch $\left\{  (\bm{s}_i, \bm{a}_i,\bm{s'}_i,r_i) \right\} $ from buffer $\mathcal{R}$
		\item update $\hat{Q}^\pi_\theta$ using target $y_i = r_i + \gamma \hat{Q}^\pi_\theta(\bm{s}_i', \bm{a}_i') \forall \bm{s}_i, \bm{a}_i$
		\item $\nabla_\theta J(\theta) \approx  \frac{1}{N} \sum_{i}^{}  \nabla_{\theta} \log \pi_\theta(\bm{a}^\pi_i|\bm{s}_i)\hat{Q}^\pi(\bm{s}_{i}, \bm{a}^\pi_{i})$,
				where $\bm{a}_i^\pi \sim \pi_\theta(\bm{a} | \bm{s}_i)$
		\item $\theta \leftarrow \theta  + \alpha \nabla_\theta J(\theta)$
\end{enumerate}
}}

%\begin{equation}
%   	Q^\pi (s,a) = E_s'[r + \gamma E_a'~\pi(s') [Q^\pi(s',a')]|s,a,\pi]
%\end{equation}
 
%optimal q value under deterministic policy
%\begin{equation} 
%a = argmax_a'\epsilon A Q^*(s,a')
%\end{equation} 
%
%\begin{equation}
%	  Q^* (s,a) = \max_\pi  Q^\pi(s,a)
%\end{equation}
%
%if the optimal function satisfies the bellman equation:
%\begin{equation}
%    Q^* (s,a) = E_s' [r + \gamma max(\bm{a'}) Q^* (s',a')| s,a,\pi]
%\end{equation}
%
%The advantage function  describes “how good” the action a is, compared to the expected return when following direct policy pi.
%
%\begin{equation}
%    A^\pi (s,a) = Q^\pi(s,a)- V^\pi(s)
%\end{equation}
%
%The value function V measures the how good it is to be in a particular state s. The Q function, however, measures the the value of choosing a particular action when in this state[6].\\

\subsection{Value function methods}
Value function methods use only the critic from actor-critic algorithms.
Suppose that the advantage function $ A^{ \pi } (\bm{s}_{t}, \bm{a}_{t} )  $ is known.
It tells us how much better the action $ \bm{a}_{t}  $ is than the average action
according to the policy $ \pi  $.
Thus, if we knew the advantage function, we could construct a deterministic 
\textbf{greedy policy}:
\begin{equation}
		\pi_{ \text{greedy} }(\bm{s}_{t}| \bm{a}_{t}) = \left\{ 
\begin{matrix}
		1 & \text{ if } \bm{a}_t = \argmax_{\bm{a}_t} A^\pi (\bm{s}_{t}, \bm{a}_{t}) 		 \\
		0 & \text{ otherwise}
\end{matrix}
		\right.
\end{equation}
which would yield the highest expected return.
In other words, if we knew the advantage function, the policy would be
reduced to the argmax operation.

\subsubsection{Dynamic programming}
Dynamic programming refers to a collection of algorithms that can be used
to compute optimal policies given a perfect model of the environment as an MDP.
They are of limited utility in reinforcement learning due to the perfect model requirement 
and their great computational expense, but are important theoretically ---
they provide an essential foundation for understanding the other methods.
Usually a finite MDP is assumed. DP can be applied to continous problems as well,
but exact solution exist only in special cases.

Value iteration and Q-learning make up two fundamental algorithms of
Reinforcement Learning.Q learning is an Off policy algorithm, which means it
uses a different policy for exploring actions from the target policy of being
optimal.  Many of the amazing feats in RL over the past decade, such as Deep
Q-Learning for Atari, or AlphaGo, Rainbow were rooted in these foundations. As
stated in section 2.2.3 value function ,is a measure of the expected reward you
can receive from any given state  given an MDP and a policy describing which
actions an agent takes in each state. Value iteration is a computational
algorithm that provides a means of finding the optimal policy $ \pi^{ * }$ .
The algorithm works by iteratively determining the value of being in each
state,  assuming that the agent takes the best possible action in that state
under the current estimate of the value function.Fitted value iteration
algorithm  are used for approximating the value function of a continuous state
MDP.Unlike value iteration over discrete state of states fitted value iteration
can not always converge.In order for certain dynamic programming algorithms
(e.g. policy iteration, value iteration) converge to a unique fixed point
Bellman equation  was rewritten as operator.

%TODO: 
%\begin{enumerate}
%		\item Introduce value iteration in a tabular setting
%		\item introduce fitted value iteration
%		\item show that q-learning is off-policy
%		\item show that q-learning does not converge when using non-linear function
%				approximation (introduce the bellman operator as well)
%\end{enumerate}

\section{Deep Q-networks and their extensions}
\label{sec-drl}

%TODO:
%\begin{enumerate}
%		\item Introduce to deep learning and advances in computer vision
%	\end{enumerate}

The success of a deep neural network in computer vision has introduced a new
paradigm in learning from raw pixels.   The goal of Deep Reinforcement Learning
is to connect the reinforcement learning algorithm to a deep neural network
that operates directly on RGB images and efficiently processes training data
using stochastic gradient updates \cite{mnih2013atari}. 


The use of label data and the assumption that the distribution of data is
identical and independent throughout the deep learning training process makes
it complex to use it directly in reinforcement learning algorithms, which must
be able to learn from sparse,noisy,delayed reward and highly correlated data.
To address these issues and successfully apply deep learning to reinforcement
learning \cite{mnih2013atari} used the experience replay mechanism
\cite{mnih2015humanlevel} which randomly samples previous transitions and
thereby smooths the distribution of training over many past behaviors. A
convolutional neural network was used to learn successful control policies from
raw video data in a complex reinforcement learning environment. The network is
trained with a variant of Q-learning algorithm,with stochastic gradient descent
to update weights.\\

This architecture was based on the Tesauro TD-Gammon architecture [17] that
updates the parameters of the network that estimate the value function directly
from the sampled experience of the policy. Similarly to this approach, the
online network in DQN uses a technique known as experience replay [18] where
the  agent’s experiences at each time-step, $\mathcal e_t =
(s_t,a_t,r_t,s_t+1)$ is stored in a data set $\mathcal D = e1 , ..., e_N $
pooled over many  episodes in a replay memory. By drawing random samples from
this pool of stored experiences, the Q-learning is updated.After performing the
experience replay, the agent selects and executes an action according to a
$\epsilon$-greedy policy.
The use of experience replay and target networks enables 
relatively stable learning of Q values, 
and led to super-human performance on several Atari games.

The advantage of using deep Q-learning over Q-learning  
includes allowing  to have greater sample efficiency,
reduced variance by randomizing the sample bias, and avoiding being 
stuck in local minimum. 
The drawbacks of deep Q-learning is that it only handle discrete, 
low-dimensional action spaces.


%\section{Extension of DQN }

Although the initial architecture of deep Q learning introduced by
\cite{mnih2013atari} paves the way for the use of deep neural network in
reinforcement learning, it comes with its own drawbacks;this leads to more
studies and an improved DQN architecture.In this section, we will discuss these
extensions of DQN architectures.

\subsection{Double Deep Q-networks: DDQN}

DQN suffer from overestimation bias due to due to the maximization step in optimisation function in Q-learning.Q-learning can overestimate actions that have been tried often and the estimations can be higher than any realistic optimistic estimate .Double Q-learning [19], addresses this overestimation by decoupling, in the maximization performed for the bootstrap target, the selection of the action from its evaluation.Double Q-learning stores two Q-functions,The average of the two Q values for each action and then performed $\mathcal{\epsilon}$-greedy exploration with the resulting average Q values.It was successfully combined with DQN to reduce  overestimations.
%\textbf{ TODOS: DDQN  target equation}

\subsection{Prioritized replay}

The main use of replay buffer is to sample transitions with maximum probability.Both DQN and DDQN samples experiences uniformly.Prioritized replay [20] samples transitions using the maximum priority ,providing a bias towards recent transitions and stochastic transitions  even when there is little left to learn about them.

\subsection{Dueling Network}

Dueling Network was designed for  value based learning,this architecture separates the representation of sate-value and state-dependent action advantages without supervision[6].its consists of two streams that represents the value and advantage functions,while sharing a common convolutional feature learning module.This network has a single Q-learning network with two streams that replace DQN architecture[3]. 

\begin{equation}
    Q(s,a; \theta,\alpha,\beta) = V(s,\theta,\beta) + A(s,a; \theta,\alpha)
\end{equation}

%\textbf{if needed TODO ADD EQUATION:factorization of action values}

\subsection{Multi-step learning}

Previously stated extension of DQN have indicated that the use deep learning has enhanced the learning capability of Q-learning.The performance of  Q-learning is still limited by greedy action selection after accumulating a single reward.An alternative approach was multi-step targets:
 
 \begin{equation}
		y_{j,t} = \sum_{t'=t}^{t+N-1} \gamma^{t-t'}    r_{j,t'} + \gamma^N  \max_{\bm{a}_{j,t+N}}Q_{\phi'}(\bm{s}_{j,t+N}, \bm{a}_{j,t+N}) 
\end{equation}

A multi-step variant of DQN is then defined by minimizing
the alternative loss[16],

\begin{equation}
	    R_t^{(n)} +  \gamma_t^{(n)}  \max_{a'} q_\theta^{-}(S_{t+n},a') - q_{\theta}(S_t,A_t)
\end{equation}
  

\subsection{Noisy Nets}

The one limitation of $\mathcal{\epsilon}$-greedy policy  is many actions must be executed to collect the first reward.Noisy Nets proposed a noisy linear layer that combines  a deterministic and noisy stream.Depending on the learning rate the network ignores to learn the noisy stream.

\subsection{Integrated Agent:Rainbow}


In the Rainbow architecture \cite{rainbow} 
several architecture changes included the one stated above where applied to DQN.
Distributional loss was replaced by a multi-step variant. The target
distribution was constructed  by contracting the value distribution
in St+n according to the cumulative discount, 
and shifting it by the truncated n-step discounted return. 
multi-step distributional loss with double Q-learning by using the greedy action in St+n 
selected according to the online network as the bootstrap action $a \cdot t+n,$ and evaluating such action using the target network.

%\textbf{TODO ADD EQUATION:target distribution}



\subsection{Deep autoencoders}


Reinforcement learning requires learning from large  high-dimensional image
data-set.For example, In Atari games the environment is composed of images with
210 * 160 pixels and 128 color palette. Each image is made up of hundreds of
pixels, so each data point has hundreds of dimensions. The manifold hypothesis
states that real-world high-dimensional data actually consists of
low-dimensional data that is embedded in the high-dimensional space. This is
the motivation behind dimensionality reduction techniques, which try to take
high-dimensional data and project it onto a lower-dimensional surface.

Autoencoders are a special kind of neural network used to perform
dimensionality reduction. They act as an identity function, such that an auto
encoder learns to output whatever is the input.They are  composed of two
networks, an encoder e and a decoder d.  

The encoder learns a non-linear
transformation that projects the data from the original high-dimensional input
space X to a lower-dimensional latent space Z. 
This is called latent  vector $z=e(x) $. 
A latent vector is a low-dimensional representation of a data point
that contains information about x.
This is commonly known  as latent space representation,
it contains all the important information needed to represent
raw data points.Auto encoders manipulates the ``closeness'' of data in the latent
space.  

A decoder learns a non-linear transformation d:Z→X that projects the
latent vectors back into the original high-dimensional input space X. This
transformation takes the latent vector and reconstruct the original input data
: 
\begin{equation}
     z = e(x)  	\rightarrow  \hat{x}=d(z)=d(e(x))
\end{equation}

The autoencoder is trained to minimize the difference between the input x and
the reconstruction $\hat{x}$ using a kind of reconstruction loss.\\ 

In traditional autoencoders, the latent vector should be easily decoded back to
the original image as a result the latent space z can become disjoint and
non-continuous. Variational autoencoders try to solve this problem.

In variational autoencoders, inputs are mapped to a probability distribution
over latent vectors, and a latent vector is then sampled from that
distribution. As a result the decoder becomes more robust at decoding latent
vectors.

Specifically, instead of mapping the input $x$ to a latent vector $z=e(x)$, 
we  instead map it to a mean vector $\mu(x)$ 
and a vector of standard deviations $\sigma(x)$. 
These parametrize a diagonal Gaussian distribution $\cal{N}(\mu, \sigma)$, 
from which we then sample a latent vector $z \sim \cal{N}(\mu, \sigma)$.


This is generally accomplished by replacing the last layer of a traditional
autoencoder with two layers, each of which output $\mu(x)$ and $\sigma(x)$. An
exponential activation is often added to $\sigma(x)$ to ensure the result is
positive.

However, this does not completely solve the problem. 
There may still be gaps in the latent space because the outputted means may 
be significantly different and the standard deviations may be small. 
To reduce that, an auxiliary loss is added that penalizes 
the distribution $p(z|x)$ for being too far from the standard normal distribution 
$\cal{N}(0,1)$. This penalty term is the 
Kullback-Leibler(KL) divergence between $p(z|x)$ and $\cal{N}(0,1)$, 
which is given by\(\mathbb{KL}\left( \mathcal{N}(\mu, \sigma) \parallel \mathcal{N}(0, 1) \right) = \sum_{x \in X} \left( \sigma^2 + \mu^2 - \log \sigma - \frac{1}{2} \right)\).This expression applies to two univariate Gaussian distributions by summing KL divergence for each dimension we are able to  extend it to our diagonal Gaussian distributions.

This loss is useful for two reasons. First, we cannot train the encoder network by gradient descent without it, since gradients cannot flow through sampling (which is a non-differentiable operation). Second, by penalizing the KL divergence in this manner, we can encourage the latent vectors to occupy a more centralized and uniform location. In essence, we force the encoder to find latent vectors that approximately follow a standard Gaussian distribution that the decoder can then effectively decode.

%TODO:
%\begin{enumerate}
%		\item Discuss the latent rep embedding diff with traditional AE and Varational AE with marko and chechk the recostrauction loss for both
%		
%		\item Include argument on the Architecture  of our final AE used for RL
%	\end{enumerate}


\section{Problems with RL}
\label{sec-rl-problems}
%ex. drl that matters paper
%\cite{drlthatmatters}

In previous studies presented in Chapter 3 we see some successful application
of unsupervised learning techniques applied to improve the performance of the
underlying RL algorithms, even though these and other studies conducted
previously have shown remarkable results. RL comes with the following
challenges. \\

One of the most difficult aspects of RL is learning efficiently with little
data. The term "sample efficiency" refers to an algorithm that makes the most
of a given sample. To put it another way, it's the amount of experience the
algorithm has to gain during training in order to achieve efficient
performance. The difficulty is that the RL system takes a long time to become
efficient.

Neural networks are opaque black boxes whose workings are mysteries to even the
creators. They are also increasing in size and complexity, backed by huge data
sets, computing power and hours of training.This referred to as Reproducibility
crisis, These factors make RL models very difficult to replicate.  

Another major challenge in Rl is agents are trained in simulated environment
;In this environment they can fail and learn, but they do not have the
opportunity to fail and learn in real-life scenarios. Usually, in real
environments, the agent lacks the space to observe the environment well enough
to use past training data to decide on a winning strategy. This also includes
the reality gap, where the agent cannot gauge the difference between the
learning simulation and the real world.\\

The reward technique discussed in the previous sections is not foolproof. Since
the rewards are sparsely distributed in the environment, a possible issue is an
agent not observing the situation enough to notice the reward signals and
maximise specific actions. This also occurs when the environment cannot provide
reward signals in time; for instance, in many situations, the agent receives a
green flag only when it is close enough to the target. 

Curiosity-driven methods are widely used to encourage the agent to explore the
environment and learn to tackle tasks in it. The researchers in the paper
‘Curiosity-driven exploration by self-supervised prediction’ proposed an
Intrinsic Curiosity Module (ICM) to support the agent in exploration and prompt
it to choose actions based on reduced errors. Another approach is curriculum
learning, where the agent is presented with various tasks in ascending order of
complexity. This imitates the learning order of humans. 

%\section{Unsupervised learning on images}
%\ref{sec-unsupervised-learning-on-images}
%TBD - citing papers rl papers cite.
%Point here is pretraining is good and stuff like contrastive loss is introduced.

%Classifiers can't directly help RL, but things like object detectors might.
%Also learned models.
%But then we don't have end-to-end learning (it's important to explain why).

\chapter{State representation learning}
\label{ch-srl-background}
%\section{Introduction to state learning learning}
%TODO: throw in citations (ex. pilco, embed2control etc)

%BIG TODO:\\
%============================================\\
%clearly deliniate generative and discriminative models:
%--- generative models are AEs, VAEs, GANs,
%and are trained to learn the distribution of inputs
%by somehow using reconstruction loss and have different
%levels of statistical modelling complexity.
%--- discriminative models don't have decoders
%because they are not trained to learn the distribution of inputs.
%instead, they are only trained to learn a coherent mapping
%into a latent space. this can be done for example
%by contrastive learning.\\
%there are also bisimulation metrics and you better find out
%what those are.
%===============================================

As discussed in the introduction, learning control from images is 
very desirable. Images, and observations in general, only implicitly 
provide information about the underlying state. 
Finding a good policy from observations, especially images,
is much more difficult than finding a policy with direct state access
because the state first needs to be inferred from those observations.
Reinforcement learning algorithms can by themselves implicitly extract
the relevant information from observations, but this at best results
in much less sample-efficient training and at worst results
in complete failure.
Often a problem which a reinforcement learning algorithm can solve
with direct state access, can not achieve any progress when
provided only image observations.

It is clear from the previous section that
amazing results were achieved in the field of computer vision.
However, to leverage these results for the purposes of reinforcement
learning, the methods in question need to be applied for state estimation.
This is a drastically different problem than for example image classification.
The key difference is that now dynamics need to be inferred.
While neural network architectures like the convolutional neural network
are able to achieve great successes in timeless problems, 
neural architectures like LSTMs aimed at learning from sequential data
comparatively perform much worse.
Results in problems such as video prediction or action classification leave much to be desired.

Having that said, the learning signal generated from for example
image reconstruction loss is substantially stronger than the reward signal,
especially in settings with sparse rewards where it is 
not present most of the time.
Thus it stands to reason that somehow leveraging the learning
signal from some computer vision method should aid the reinforcement learning 
process. 
One way to do this is to explicitly use such methods to learn 
a function which maps from observations to states
and then use reinforcement learning methods these learned state
representations.
This approach is explored in this section, mainly with the help
of the 
\cite{srloverview}
overview paper.
In this section  state representation learning for control in general is discussed.
%as this will allow for a broader contextualization of our own work.
Importantly, this does not concern learning a model 
which can be used to achieve control through planning,
although there are similarities between these approaches.

In general, representation learning algorithm are designed to learn
abstract features that characterize data.
In the simplest forms they include methods such as k nearest neighbors.
In state representation learning (SRL) the learned features
are of low dimension, evolve through time and are depended
on actions of an agent.
The last point is particularly important because in reinforcement learning,
features that do not influence the agent and that can not be influenced
by the agent are not relevant for the problem of optimally controlling the agent.
Also, simply reducing the dimensionality of the input to a reinforcement learning
agent results in a computationally easier learning problem,
which can make a difference between the solution being feasible or infeasible.
Ideally, state  representation learning should be done in an without explicit supervision
as it can then be done in tandem with the likewise unsupervised reinforcement learning.

While we assume that state-transitions have the Markov property,
partial observability denies the possibility of having a one-to-one
correspondence between each observation and state ---
an object whose position is required may be occluded by another.
Thus prior observations have affect the mapping to the current state.
Images in particular also do not encode kinematic or dynamic information:
to get that crucial information a sequence of images is required.
Hence we define the SRL task as learning
a representation $ \tilde{\bm{s}}_{t} \in \tilde{\cal{S}}  $ of dimension $ K  $
with characteristics similar to those of true states $ \bm{s}_{t} \in \mathcal{S} $.
In particular, the representation is a mapping of the history of 
observation to the current state: $ \tilde{\bm{s}}_{t} = \phi(\bm{o}_{1:t}  $.
Actions $ \bm{a}_{1:t}  $ and rewards $ r_{ 1:t }  $ can also be added
to the parameters of $ \phi  $.
This can help in extracting only the information relevant for the agent and its task.
Often the representation is learned by using the reconstruction loss;
$ \hat{\bm{o}_{t}}  $ denotes the reconstruction of $ \bm{o}_{t}  $.

In the context of reinforcement learning, state representations should
ideally have the following properties:
\begin{itemize}
		\item have the Markov property 
		\item be able to represent the current state well enough for policy improvement
		\item be able to generalize to unseen states with similar features
		\item be low dimensional
\end{itemize}
We now discuss different types of models and learning strategies which 
can be used to learn state representations.

\section{Representation models in general}
\label{sec-repr-models-general}
In general, representation learning refers to the process of learning 
a parametric mapping from raw input data domain
to a feature vector or tensor, in the hope of capturing and extracting more abstract and useful concepts
that can improve performance of downstream tasks.
Often this includes dimensionality reduction.
The goal of representation learning is for this mapping to meaningfully generalize well
on new data.
Before introducing types of representation models,
we first need to define the characteristics a good representation of data needs
to have in general. 
These principles and trade-offs between their relative priorities guide the model design.
The following list summarizes different desirable characteristics:
\begin{enumerate}
		\item \textit{smoothness}: $ f  $ s.t. $ x \approx y  $ implies
				$ f(x) \approx f (y)  $
		\item \textit{multiple explanatory factors} a.k.a. disentangling 
				features
		\item \textit{semi-supervised learning}: for input $ Z  $ and target $ Y  $,
				learning $ P (X)  $ helps learning $ P (Y|X)  $ because
				features of $ X  $ help explain $ Y  $
		\item \textit{shared factors across tasks}: like previous point,
				but also works for different $ Y  $s
		\item \textit{manifolds}: probability mass concentrates
				in regions with much smaller dimensionality than data itself
		\item \textit{natural clustering}: different values of categorical variables
				are associated with separate manifolds.
		\item \textit{temporal and spatial coherence}:
				consecutive or spatially nearby observations
				thend to be associated with the same value of relevant categorical
				concepts or result in small surface move on the surface of the manifold
		\item \textit{sparsity}: could mean often many features are 0. could also be
				that the features are insensitive to small changes in $ x  $
		\item \textit{simplicity of factor dependencies}: ideally factors are related
				to each other linearly, or otherwise simply
\end{enumerate}

The process of extracting representations from observations,
or inferring latent variables in a probabilistic view of a dataset,  is often called \textbf{inference}.
There are \textbf{generative} and \textbf{discriminative} models.

Generative models learn representations by modelling the data distribution
$ p(\bm{x}_{})  $. Such a model can generate realistic examples.
Evaluating the conditional distribution $ p (y | \bm{x}_{})  $
it done via Bayes rule.

Discriminative models model the conditional distribution $ p (y | \bm{x}_{})  $
directly.
Discriminative modelling consists of first the inference
that extracts latent variables
$ p(\bm{v}_{}| \bm{x}_{})  $
which are then used to make downstream decision
from those variables $ p (y|\bm{v}_{})  $.

The benefit of discriminative models are that you don't have to go through
an expensive process of learning $ p (\bm{x}_{})  $.
That's also harder to evaluate.
This is especially evident if you just want a lower dimensional distribution.
In the context of reinforcement learning, the model-based approach 
benefits from generative models as they can be used to generate predictions
which can then be used for planning.
In the model-free approach, both discriminative and generative models may be used
as predictions are not used.


\subsection{Generative models}
\subsubsection{Probabilistic models}
\label{subsub-probabilistic-models}
From the probabilistic modeling perspective, feature learning
can be interpreted as an attempt to recover a parsimonious set of latent random
variables that describe a distribution over the observed data.
$ p (x,h)  $ is the probabilistic model over the joint space
of latent variables $ h  $ and observed data $ x  $.
Feature values are then the result of an inference process to determine the probability
distribution of the latent variables given the data, i.e. $ p (h|x)  $,
a.k.a posterior probability.
Learning is the finding the parameters
that (locally) maximize the regularized likelihood of the training data.

\subsubsection{Directed graphical models}
\textit{Directed latent factor models} separately parametrize
$ p (x|h)  $ and the prior $ p (h)  $ to construct
$ p (x,h) = p (x|h) p (h)  $.
They can explain away: a priori independent causes of an event
can become nonindependent given the observation of the event.
Can conceive them as cause models, where $ h  $ activations
cause the observed $ x  $, making $ h  $ nonindependent.
This makes recovering the posterior $ p (h|x)  $ intractable.

\subsubsection{Directly learning a parametric map from input to representation}
The posterior distribution becomes complicated quickly.
Thus approximate inference becomes necessary, which is not ideal.
Also, depending on the problem, one needs to derive feature vectors
from the distribution.
If we want deterministic feature values in the end, we might as well
go ahead and use a nonprobabilistic feature learning paradigm.
Doing so is particularly desirable for representations for model-free 
reinforcement learning algorithms:
since the data distribution is not explicitly used to make plans,
the stochasticity inherent in statistical modelling hinders the ability
of the reinforcement learning algorithm to use those representations.


\subsection{Discriminative models}
In discriminative modelling the data distribution is not directly represented.
Instead, it is implicit in the representation space. 
One way to learn discriminative models is through contrastive representation learning.
Intuitively, it's learning by comparing.
So instead of needing data labels $ y  $ for datapoints $ \bm{x}$,
you need to define a similarity distribution which allows you to
sample a positive input $ \bm{x}_{}^{ + } \sim p^{ + } (\cdot | \bm{x}_{})  $
and a data distribution for a negative input $ \bm{x}_{}^{ - } \sim p^{ - } (\cdot | \bm{x}_{})  $,
with respect to an input sample $ \bm{x}_{}  $.
``Similar'' inputs should be mapped close together, and ``dissimilar'' samples
should be mapped further away in the embedding space.

Let's explain how this would work with the example of image-based instance discrimination.
The goal is to learn a representation by maximizing agreement of the encoded features (embeddings)
between two differently augmented views of the same images,
while simultaneously minimizing the agreement between different images.
To avoid model maximizing agreement through low-level visual cues, views
from the same image are generated through a series of strong image augmentation methods.
Let $ \mathcal{T}  $ be a set of image transformation operations where
$ t, t' \sim \mathcal{T}  $ are two different transformations sampled independently from $ \mathcal{T}  $.
There transformations include ex. cropping, resizing, blurring, color distortion or perspective distortion.
A $ (\bm{x}_{q}, \bm{x}_{k})  $ pair of query and key views is positive when these 2 views
are created with different transformations on the same image,
i.e. $ \bm{x}_{q} = t (\bm{x}_{})  $ and $ \bm{x}_{k} = t' (\bm{x}_{})  $,
and is negative otherwise.
A feature encoder $ e (\cdot)  $ then extracts feature vectors from all augmented data samples 
$ \bm{v}_{} = e (\bm{x}_{})  $. This is usually ResNet, in which case 
$ \bm{v}_{} \in \mathcal{R}^{ d }  $ is the output of the average pooling layer.
Each $ v  $ is then fed into a projection head $ h (\cdot)  $ made up of
a small multi-layer perceptron to obtain a metric embedding $ \bm{z}_{} = h (\bm{v}_{})  $,
where $ \bm{z}_{} \in \mathcal{R}^{ d' }  $ with $ d' < d  $.
All vectors are then normalized to be unit vectors.
Then you take a batch of these metric embedding pairs $ \left\{ (\bm{z}_{i}, \bm{z}_{i}') \right\}   $,
with $ (\bm{z}_{i}, \bm{z}_{i}')  $ being the metric embeddings of
$ (\bm{x}_{q}, \bm{x}_{k})  $ of the same image
are fed into the contrastive loss function which does what we said 3 times already.
The general form of popular loss function such as InfoNCE and NT-Xent
is:
\begin{equation}
		\mathcal{L}_{ i } = - \log \frac{\exp (\bm{z}_{i}^{ T }\bm{z}_{i}'/\tau)}{\sum_{j=0}^{K} \exp (\bm{z}_{i} \cdot \bm{z}_{j}')/\tau} 
\end{equation}
where $ \tau  $ is the temperature parameter.
The sum is over one positive and $ K  $ negative pairs in the same minibatch.


\subsection{Common representation learning approaches}
\subsubsection{Deterministic autoencoders}
Deterministic autoencoders are generative models.
%TODO: move text about them here.
\begin{equation}
		h^{ (t) } = f_{ \theta } (x^{ (t) })
\end{equation}
There's also the reconstruction $ r = g_{ \theta } (h)  $,
used for the reconstruction error $ L (x,r)  $ over the
training examples.
Autoencoder training boils down to finding $ \theta  $
which minimizes:
\begin{equation}
		\mathcal{J}_{ AE } (\theta) =
		\sum_{t}^{} L (x^{ (t) }, g_{ \theta } (f_{ \theta } (x^{ (t) })))
\end{equation}
One can tie the weights between the encoder and the decoder (i.e. make the same ones,
just reversed).




\subsubsection{Variational autoencoders}
%TODO: delete half of this or add the other half
Variational autoencoders marry graphical models and deep learning.
The generative model is a Bayesian network of form
$ p (\bm{x}_{}| \bm{z}_{})p (\bm{z}_{})  $,
or in the case of multiple stochastic layers, a hierarchy such as:
$ p (\bm{x}_{}| \bm{z}_{L})p (\bm{z}_{L}|\bm{z}_{L-1})\cdots p (\bm{z}_{1}|\bm{z}_{0})  $.
Similarly, the recognition model is also a conditional Bayesian network of
form $ p (\bm{z}_{}|\bm{x}_{})  $ which can also be a hierarchy of
stochastic layers.
Inside each conditional may be a deep neural network,
e.g. $ \bm{z}_{}|\bm{x}_{} \sim f (\bm{x}_{}, \bm{\epsilon}_{})  $
with $ f  $ being the neural network mapping and $ \bm{\epsilon}_{}  $ a
noise random variable.
Its learning algorithm is a mix of classical (amortized, variational)
expectation maximization, but with the reparametrization trick
ends up backpropagating through the many layers of the deep neural networks
embedded inside it.


We can parametrize conditional distributions with neural networks.
VAEs in particular work with \textit{directed} probabilistic models,
also know as \textit{probabilistic graphical models} (PGMs)
or \textit{Bayesian networks}.
The joint distribution over the variables of such models
factorizes as a product of prior and conditional distributions:
\begin{equation}
p_{ \bm{\theta}_{} } (\bm{x}_{1}, \dots, \bm{x}_{M}) =
\prod_{j=1}^{M} p_{ \bm{\theta}_{} } (\bm{x}_{j}| Pa (\bm{x}_{j})) 
\end{equation}
where $ P a (\bm{x}_{j})  $ is the set of parent variables of node $ j  $ in
the directed graph. For root nodes the parents are an empty set,
i.e. that distribution is unconditional.
Before you'd parametrize each conditional distribution with
ex. a linear model, and now we do it with neural networks:
\begin{align}
		\bm{\eta}_{} &= \text{NeuralNet} (P a (\bm{x}_{}))\\
		p_{ \bm{\theta}_{} } (\bm{x}_{}|Pa (\bm{x}_{})) &= p_{ \bm{\theta}_{} } (\bm{x}_{}|\bm{\eta}_{})
\end{align}

To solve intractabilities, we introduce
a parametric \textit{inference model} $ q_{ \bm{\phi}_{}} (\bm{z}_{}|\bm{x}_{})  $.
This model is called the \textit{encoder} or \textit{recognition model}/
$ \bm{\phi}_{}  $ are called the \textit{variational parameters}.
They are optimized s.t.:
\begin{equation}
		 q_{ \bm{\phi}_{}} (\bm{z}_{}|\bm{x}_{})  \approx
p_{ \bm{\theta}_{} } (\bm{z}_{}|\bm{x}_{})
\end{equation}
Like a DLVM, the inference model can be almost any directed graphical model:
\begin{equation}
		q_{ \bm{\phi}_{}} (\bm{z}_{}|\bm{x}_{}) = 
		q_{ \bm{\phi}_{}} (\bm{z}_{1}, \dots, \bm{z}_{M}|\bm{x}_{}) =
		\prod_{j=1}^{M} q_{ \\bm{\phi}_{} (\bm{z}_{j}| P a (\bm{z}_{j}), \bm{x}_{}) } 
\end{equation}
This can also be a neural network.
In this case, parameters $ \bm{\phi}_{}  $ include the weights and biases, ex.
\begin{align}
		(\bm{\mu}_{}, \log \bm{\sigma}_{}) &= \text{EncoderNeuralNet}_{ \bm{\phi}_{} } (\bm{x}_{})\\
		q_{ \bm{\phi}_{} } (\bm{z}_{}|\bm{x}_{}) &=
		\mathcal{N} (\bm{z}_{}; \bm{\mu}_{}, \text{diag} (\bm{\sigma}_{}))
\end{align}
Typically, one encoder is used to perform posterior inference
over all of the datapoints in the dataset.
The strategy used in VAEs of sharing variational parameters across datapoints is also called
\textit{amortized variational inference}.


\subsubsection{Deterministic autoencoder regularization}
\label{ae-regularization}
%Sparsity penalty, adding data-augmentation to it.
Autoencoders may be employed not only just to learn representations, 
but to perform additional auxiliary tasks.
One such task is denoising: provided a noisified input at the encoder,
the decoder outputs a denoisified image as output.
Importantly, while this training process results in a denoising autoencoder,
it also regularizes the autoencoder.
Regularization not only helps with preventing overfitting, but also
produces better representations as it encourages smoothness
and spatial coherence of when learning.
The same result can be accomplished by other data augmentation techniques
like random cropping.

%TODO: reword so that the spice flows.

Learning VAEs from data poses unanswered theoretical questions and considerable practical challenges.
This work proposes a generative model that is simpler, deterministic, easier to train,
while retaining some VAE advantages.
Namely, the observation is that sampling a stochastic encoder in Gaussian VAE can be interpreted as injecting
noise into the input of a deterministic decoder.

The encoder deterministically maps a data point $ \bm{x}_{}  $ to
the mean $ \mu_{ \phi } (\bm{x}_{})  $ and variance $ \sigma_{ \phi } ( \bm{x}_{})  $
in the latent space.
The input to $ D_{ \theta }  $ is then the mean $ \mu_{ \phi } (\bm{x}_{})  $
augmented with Gaussian noise scaled by $ \sigma_{ \phi } (\bm{x}_{})  $
via the reparametrizing trick.
Authors argue that this noise injection is a key factor in having a regularized decoder (
noise injection as a mean to regularize neural networks is a well-known technique).
Thus training the RAE involves minimizing the simplified loss:
\begin{equation}
		\mathcal{L}_{ \text{RAE} } = 
\mathcal{L}_{ \text{REC} } + \beta \mathcal{L}^{ \text{RAE} }_{ \bm{z}_{} } 
+ \lambda \mathcal{L}_{ \text{REG} }
\end{equation}
where $ \mathcal{L}_{ \text{REG} }  $ represents the explicit regularizer for $ D_{ \theta }  $,
and $ \mathcal{L}^{ \text{RAE} }_{ \bm{z}_{} } = \frac{1}{2} ||\bm{z}_{}||_{ 2 }^{ 2 }  $,
which is equivalent to constraining the size of the learned latent space, which is needed
to prevent unbounded optimization.
One option for $ \mathcal{L}_{ \text{REG} }  $ is Tikhonov regularization
since it is known to be related to the addition of low-magnitude input noise.
In this framework this equates to 
$ \mathcal{L}_{ \text{REG} } = \mathcal{L}_{ L_{ 2 } } = ||\theta||^{ 2 }_{ 2 } $.
There's also the \textbf{gradient penalty} and
\textbf{spectral normalization}.


%\subsubsection{Siamese networks}
%Networks that share parameters.


\section{Representation models for control}
\label{sec-srl-for-control}
With representation in learning introduced in general,
we can now introduce four different strategies for learning latent space models for control:
the autoencoder, the forward model, the inverse model and the model
with prior.
These models refer to portions of the control problem they are modelling.\footnote{The term
autoencoder is overloaded in this case.}
They can be both discriminative and generative models.
In the figures below, the white nodes are inputs and the gray nodes are outputs.
The dashed rectangles are fitted around variables with which the loss is calculated.

\subsection{Autoencoder}
The idea behind the autoencoder is to just learn a lower-dimensional embedding
of the observation space. This should make the learning problem easier due to the
dimensionality reduction.
The auto-encoder may be trained to denoise the observations by passing an observation
with artificially added noise to the encoder, but then calculating the reconstruction
loss on the image without the added noise.
Formally this can be written as
\begin{align}
		\bm{s}_{t} &= \phi (\bm{o}_{t}; \theta_{ \phi }) \\
		\hat{\bm{o}_{t}} &= \phi^{ -1 } (\bm{s}_{t}; \theta_{ \phi^{ -1 } })
\end{align}
where $ \theta_{ \phi }  $ and
$ \theta_{ \phi^{ -1 } }  $ are the parameters learned for the encoder and decoder respectively.

\begin{figure}[htpb]
\begin{center}
\begin{tikzpicture}[scale=1, transform shape, node distance=1.5cm]
		\node (st) [mcsb] {$\bm{s}_{t} $};
		\node (ot) [mcs, below of=st] {$\bm{o}_{t}$};
		\node (othat) [mcsb, right of=ot] {$\hat{\bm{o}}_{t} $};
		\draw [arrow] (ot) -- (st);
		\draw [arrow] (st) -- (othat);
		\node[draw,dashed,inner sep=1.5mm,fit=(ot) (othat) ] {};
\end{tikzpicture}
\end{center}
		\caption{Auto-encoder: learned by reconstructing the observation (one-to-one).
				The observation is the input and the computed state is the vector at
				the auto-encoder's bottleneck layer, i.e. is the output of the encoder
				part of the auto-encoder network.
		The loss is calculated between the true observation and the reconstructing observation (which
		is obtained by passing the observation though both the encoder and the decoder).}
\end{figure}

\subsection{Forward model}
The auto-encoder does not encode dynamic information.
Since that information is necessary for control, usually a few consequtive
observations (or their embeddings) are stacked and passed to the reinforcement learning algorithm.
This way the information about the dynamics is implicitly provided.
While doing so works, it could be made more efficient by embedding the dynamic
information as well.
One way to achieve this is to trained a model to predict future state representations.
A model can also be observations directly, 
of course provided that the network in question has a bottleneck layer from which
the learned representations can be extracted.
Since learning on sequential information is difficult and would also benefit from
lowering the dimensionality, learning a forward model can be done in two steps:
first, learning an auto-encoder to embed individual frames and then 
learning a predictive model in the embedded space.
In the schematic we show the case where predictions are learned from embeddings
because it is the structurally more complex scheme.
Formally, we have
\begin{equation}
		\hat{\tilde{\bm{s}}}_{ t+1 } = f (\tilde{\bm{s}_{t}}, \bm{a}_{t}; \theta_{ \text{forward} })
\end{equation}
%This however supposes that all states (and the corresponding observations) are accessible 
%prior to the beggining of the training process.
The forward model can be constrained to have linear transition between 
$ \tilde{\bm{s}}_{t}  $ and $ \tilde{\bm{s}}_{t+1}  $, thereby
imposing simple linear dynamics in the learned state space.
Depending on the problem, if this is done well enough, learning a control law can be avoided and instead
schemes like model-predictive control can be employed.

\begin{figure}[htpb]
\begin{center}
\begin{tikzpicture}[scale=1, transform shape, node distance=1.5cm]
		\node (at) [mcs] {$\bm{a}_{t} $};
		\node (st) [mcsb, below of=at] {$\tilde{\bm{s}}_{t} $};
		\node (sthatplus1) [mcsb, right of=at] {$\hat{\tilde{\bm{s}}}_{t+1} $};
		\node (stplus1) [mcsb, right of=st] {$\tilde{\bm{s}}_{t+1} $};
		\node (ot) [mcs, below of=st] {$\bm{o}_{t}$};
		\node (otplus1) [mcs, right of=ot] {$\hat{\bm{o}}_{t} $};
		\draw [arrow] (at) -- (sthatplus1);
		\draw [arrow] (st) -- (sthatplus1);
		\draw [arrow] (ot) -- (st);
		\draw [arrow] (otplus1) -- (stplus1);
		\node[draw,dashed,inner sep=1.5mm,fit=(sthatplus1) (stplus1) ] {};
\end{tikzpicture}
\end{center}
		\caption{Forward model: predicting the future state from the state-action pair.
				The loss is computer from comparing the predicted state against the true next state
				(the states being the learned states).
				This can also be done directly by predicting the next observation and comparing against it.
				}
\end{figure}



\subsection{Inverse model}
The introducing predictions solves the problem of not embedding the dynamic
information.
However, not all information in the observation is relevant for control.
Consider a computer game where images feature decorative backgrounds ---
those decorations are irrelevant for playing the game well.
If the reconstruction loss is computed from entire observation,
that information is also carried over into the embedded space.
However, if the model is trained to predict actions,
it is only incentivised to use information which the agent can affect.
Thus, due to less information being required,
the inverse model should produce a more compact embedding.
Formally, we can write this as:
\begin{equation}
		\hat{\bm{a}_{t}} = g (\tilde{\bm{s}_{t}}, \tilde{\bm{s}_{t+1}}; \theta_{ \text{inverse} })
\end{equation}
If the inverse model is neural network, we can recover the embedding by discarding
the last few layers and use their outputs to produce the embeddings.

\begin{figure}[htpb]
\begin{center}
\begin{tikzpicture}[scale=1, transform shape, node distance=1.5cm]
		\node (at) [mcs] {$\bm{a}_{t} $};
		\node (athat) [mcsb, below of=at] {$\hat{\bm{a}}_{t} $};
		\node (nothing) [below of=st] {};
		\node (sttilde) [mcsb, left of=nothing] {$\tilde{\bm{s}}_{t} $};
		\node (stildetplus1) [mcsb, right of=nothing] {$\tilde{\bm{s}}_{t+1} $};
		\node (ot) [mcs, below of=sttilde] {$\bm{o}_{t}$};
		\node (otplus1) [mcs, below of=stildetplus1] {$\bm{o}_{t+1} $};
		\draw [arrow] (ot) -- (sttilde);
		\draw [arrow] (otplus1) -- (stildetplus1);
		\draw [arrow] (sttilde) -- (athat);
		\draw [arrow] (stildetplus1) -- (athat);
		\node[draw,dashed,inner sep=1.5mm,fit=(at) (athat) ] {};
\end{tikzpicture}
\end{center}
		\caption{Inverse model: predicting the action between two consecutive states.
				The loss is computer from comparing the predicted action between two consecutive states
				against the true action that
				was taken by the agent between those two states.
				(the states being the learned states).
				}
\end{figure}

\subsection{Using prior knowledge to constrain the state space}
Of course, not everything need be learned in every problem.
While in general hand-engineered features are worse than learned ones,
there are other ways to provide prior knowledge to the learning system.
For example, convolutional neural network by their architecture encode
the fact that nearby pixels are related.
In the SRL context we already mention the possibility of constraining 
the model to linear transitions, but there are other available techniques
like for example
constraining temporal continuity or the principle of causality.
Furthermore, priors can be defined as additional objectives or loss functions.
For example, additional loss can be provided if embeddings from
consequtive observation are drastically different.
This is called the slowness principle.

\subsection{Using hybring objectives}
The approaches outlined thus far can be combined into hybrid
approaches, for example \cite{watter2015embed}.
%TODO: throw a reference or two from the overview paper you're going over,
%for example embed2control.


\section{Model-based reinforcement learning}
Like the name suggest, in model-based reinforcement learning a ``world model''
is learned.
While there exists a whole spectrum of methods between pure
model-free and model-based ones, the key distinguishing feature
of model-based methods is that the learned model is used to \textit{plan} actions.
In this case, the task of reinforcement learning in the narrow sense
is to learn the values of different states.
This then enables calculation of trajectories toward states with high rewards.
In model-free methods, only the following action is selected at on iteration of the process
because only the transition reward is learned and states these transitions
lead to are unknown (not explicitly modelled).
%TODO
%Introduce just the idea for the sole purpose of
%showing why we aren't doing model-based reinforcement learning,
%but instead opting for model-free with state representation learning.



% CREATED BY DAVID FRISK, 2016
\chapter{Introduction}

 Before 1980s Reinforcement learning(RL) has two main threads.One thread associated with learning by trail and error in animal psychology and the second thread concerned with  optimal control and its solution in value function and dynamic programming[1].A third  less common type of reinforcement learning is temporal difference method which is used in tic-tac-toe.After 1980s by combining these three threads modern reinforcement learning has emerged.\\
 
Reinforcement learning is learning what to do—how to map situations to actions—so as to maximize a numerical reward signal[1].It is a discipline that encompass learning problem,a class of solutions related to learning and field of study of problems in learning and solution methods. It is most of time confused with unsupervised learning because it does not rely on examples of correct behavior or labeled data. unlike unsupervised learning  reinforcement learning is trying to maximize a reward signal instead of learning to find hidden structure.\\

All reinforcement learning agents have explicit goals, can sense aspects of their environments, and can choose actions to influence their environments.One of the two successful approaches to solve sequential decision making commonly known as  Markov Decision Process(MDP) optimisation problem is reinforcement learning[7].Markov decision processes are intended to include just three aspects  sensation,action and goal  in their simplest possible forms without trivializing any of them[1].we seek actions that brings about state of highest value not highest reward,because these actions obtains the greatest amount of reward for us over the long run.\\


Existing Reinforcement learning algorithms could be categorised as being either model based or model-free.Currently, the best performing algorithms are of the model-free type, they do not explicitly model the dynamics of the environment.one of the benefits of using model-free algorithms is its generality and applicability to a wide range of problems.In addition to not modeling the dynamics of the environment,the lack of interpretability of model -free  algorithm limits there application in safety-critical control systems.\\

Many recent success in advancing reinforcement learning   through deep learning was kick-stared by Deep Q-Networks algorithm [2][3].Its is a combination of convulational neural network trained with a variant of Q-learning from raw pixels has improved agents to play many atari games at human-level performance.One of major problem of this algorithm is  overestimation of  action values under certain conditions, to address this   double Q-learning algorithm(DDQN)  [4] was introduced ,which was initially proposed in tabluar setting [5].By decoupling selection and evaluation of the bootstrap action the overestimation bias of DQN were resolved.later the dueling network architecture [6] helps to generalize across actions by separately representing state values and action advantage.Since then, many extensions have been proposed that enhance the speed or stability of DQN.\\

Model based reinforcement learning can be defined as any MDP approach that uses a model(known or learned) and uses learning to approximate a global value or policy function[7].model-based approaches [6] offer an avenue to address the two most common issues related to model-free algorithms   modeling the dynamics of the environment and the lack of interpretability. Model-based reinforcement learning  converge  faster than their model-free algorithms.\\

In model-based reinforcement learning both the model and the policy are learned.In order to successfully learn the two  model-based algorithm needs to  balance between exploration and exploitation to find the optimal policy (like a model-free algorithm) and also balance between learning the model and learning the policy itself. Recent studies on model based reinforcement learning have shown a promise in this paradigm by obtaining a state of the art performance on offline reinforcement learning benchmarks [10][11],improving a powerful model-free approach (e.g.[12]).\\

The main problem with model based reinforcement learning is balancing between learning the model and policy.if the learned model is not a sufficiently good representation of the environment, or if it models only certain parts of the environment well, the learned policy will in effect over-fit on that model, resulting in poor performance. On the other hand, if a policy is not performing well during training, the agent won’t be able to access new parts of the environment (for example the agent will not be able to progress to a new level in a game). Often this results in model-based algorithms converging to a poorer policy than their model-free counterparts.\\

In recent years we have seen the use of meta-learning in reinforcement learning.which is a form of learning how to learn. Meta-learning algorithms aim to learn models that can adapt to new scenarios or tasks with few data
points.Meta-learning, in the context of RL, aims to learn a policy that adapts fast to new tasks or environments [ 12,13,14].Another extension of model based reinforcement learning is offline learning ,which utilizes  previously collected data with out additional online data collection.In this learning the stochasticity of the environment is maintained by limiting the interaction of the agent  to collect additional transition by interacting with the environment by using the behaviour policy.\\

In Practical application,Although the above stated have shown remarkable adavance in the field,it is still difficult to learn a precise environment dynamic models.In previous studies[2,3,4,6,16] deep Q-learning was trained using raw pixel image,which remarks one of greater advantages of using deep learning in reinforcement learning.Training reinforcement learning agents from high dimensional image representations can be very computationally expensive.By using  Deep auto-encoders for reinforcement learning to compress high dimensional data such as pixelated images into small  latent representation [21]  were able to achieve improved performance compared to a baseline reinforcement learning  agent with a convolutional feature extractor, while using less than 2W of power.While it is not a novel idea to use auto-encoders to accelerate reinforcement learning we have not found a substantial work focused  on the use of compressed representation by auto-encoders to learn to model the dynamic on an Atari  environment and subsequently train a policy completely in a simulation.




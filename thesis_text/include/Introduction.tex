% CREATED BY DAVID FRISK, 2016
\chapter{Introduction}
\label{ch-introduction}
\section{What is reinforcement learning?}
\label{sec-what-is-rl}
In computer science, reinforcement learning is the formalization of trial-and-error learning.
While this is not the only legitimate interpretation of the concept, it is the most straightforward one:
``trial'' implies existence of an agent which observes its environment and interacts with it 
though its own actions.
``Error'' implies that the agent has a goal it tries to achieve and
that it does not know how to achieve it (in the most effective manner). 
What it can do is take different actions and appraise them
according to how closely they lead the agent toward its goal, thereby observing
the quality of those actions.
By repeatedly exploring the effects of various sequences of actions, the agent
can find, i.e. learn, the sequence of actions which lead to its goal.

Here, it is important to discuss what a goal is.
To formalize the process outlined above, one needs to describe it
in purely mathematical terms.
Thus, among other things, the goal needs to be described numerically.
To achieve that, the notion of a reward function is used:
it maps every state of the environment to a number which denotes
its value called the reward. 
The state of the environment to which the highest reward is ascribed
is then the goal.
A more general description of the goal of reinforcement learning
is to maximize the sum of rewards over time.
The formalization of the entire process will be carried out later in the text,
while here only the most important concepts will be outlined.

%One of these is the trade-off between ``exploration''
%and ``exploitation.''
%To learn just by trial and error implies learning from experience.
%This means that the agent can not know 
%how a certain strategy fares unless it collects experiences 
%which come by following said strategy.
%Thus in order to find a good strategy,
%usually referred to as a ``policy'',
%the agent needs to produce various different strategies and observe their results
%until it find a promising one.
%The process of finding different strategies and experimenting with new random
%behavior is called exploration.
%Likewise, the process of repeating a good strategy is called exploitation.
%Due to the curse of dimensionality, 
%\footnote{The curse of dimensionality refers to the exponential rise of 
%possible configurations of the problem with the number of dimensions.}
%in complex multi-dimensional domains it is impossible to test but a miniscule proportion 
%of all possible strategies.
%Because of this, the problem of effective exploration and the trade-off between
%it and exploitation is a fundamental problem in reinforcement learning.

 

Due to its generality, reinforcement learning is studied in many different disciplines: 
control theory, game theory,
information theory, simulation-based optimization, multi-agent systems etc..
Of these, control theory is of particular importance because it
often enables clear analysis of various reinforcement learning algorithms.
This foremost concerns the usage of dynamic programming which
provides a basis for a large class of reinforcement learning algorithms.
Reinforcement learning is also considered to be one of the pillars of modern data-driven machine learning.


In the context of machine learning, reinforcement learning can be view as a combination 
of supervised and unsupervised learning:
the ``trial'' portion of the trial-and-error learning can be interpreted as unsupervised or as self-supervised learning
because in it the agent collects its own dataset without any explicit labels to guide its way.
This process is referred to as ``exploration''.
The dataset created by exploration is labelled by the reward function.
Thus the agent can learning from ``past experience'' in a supervised manner.
This text will introduce concepts from both control theory
and machine learning which are necessary to formalize the reinforcement learning objective
and to develop algorithms to achieve it.
It will not concern itself with other disciplines.

\section{Why is reinforcement learning interesting?}
\label{sec-why-is-rl-interesting}
Interest in reinforcement learning has grown tremendously over the past decade.
It has been fueled by successes of deep machine learning in fields such as computer vision.
The subsequent utilization of neural networks in reinforcement learning,
dubbed deep reinforcement learning,
led to impressive results such as achieving better-than-human
performance on Atari games, in the game of go and in many others.
Because large amounts of data are required for neural network training
and thus for reinforcement learning algorithms which utilize them,
most of these results are achieved in computer-simulated environments.
\footnote{Simulated environments run as fast as the computers they run on,
		which enables generating thousands of trials in seconds.}
These recent success were kick-started by Deep Q-Network (DQN) algorithm \cite{mnih2013atari} which crucially,
by utilizing convolutional neural network, enabled the agents to successfully learn from raw pixels.
Learning from pixels is incredibly important for many practical applications,
such as those in robotics
where it is often impossible to get full access to the state of the environment.
The state then needs to be inferred from observations such as those from cameras.
\footnote{Here the state refers to the underlying physical parameters of the environment:
the positions and velocities of objects, the friction coefficients and so on.
Observations from sensors such as cameras do not explicitly provide such information.
However, since humans and animals are able to utilize such observations to achieve their
goals, we know that they implicitly hold enough information about the true state
of the world for successful goal completion.}
Due to incredible results achieved in simulated environments,
reinforcement learning holds the promise of solving
many incredibly important engineering problems, for example robotic manipulation
and grasping.
Having that said, there exists a large gap between simple simulated environments and
the real world,
and many improvements to the current state-of-the-art algorithms are required to
bridge that gap.
To explain the approach investigated in this thesis,
a bit more context is needed.

\section{Efforts in making reinforcement learning more efficient}
\label{efforts-in-making-rl-efficient}
An important classification of reinforcement learning algorithm is the one between
model-based and model-free algorithms.
As the name suggests, model-free algorithms do not form an explicit model of the environment.
Instead, they function as black-box optimization algorithms, simply finding actions which maximize
reward without other concerns such as predicting the states resulting from those actions.
In other words, they only predict the reward of actions in given states.
Model-based algorithms on the other hand learn an explicit model of the environment
and use it to plan their actions.
They thus learn the dynamics of the environment and use that knowledge to choose actions
which lead the agent to states with high reward.
Both classes have their benefits and their drawbacks.
Since model-free algorithms do not require any knowledge of environment dynamics
to operate, they are more widely applicable and usually achieve better performance.
But the fact that they can not leverage environment dynamics to create plans implies
a harder learning problem: they need to implicitly learn those dynamics
while only being provided the reward signal. 
This makes them much less sample-efficient.\\

By contrast, model-based algorithms are of course more sample-efficient.
Furthermore, the plan generated from the learned model can be utilized to interpret the
agent's actions which in turn leads to many further benefits such as
the ability to guarantee outcomes in safety-critical operations.
Unfortunately, the twin learning objective of learning the best action-choosing policy 
to maximize the reward over time, and the learning of the model results
in fundamental training instabilities which usually results in worse final performance.
% this explanation is not sufficiently good
In simple terms, the reason behind this is the following one:
in the beginning of the learning process, both the policy and the model perform poorly.
For the model to perform better, the agent needs to explore the environment and
update its model.
However, many parts of the environment are inaccessible to a poorly performing agent:
for example, if an agent is playing a computer game, and it is not able to progress to further
sections of the game, it will not be able to construct a model of that portion of the game.
Thus, to explore the environment and improve its model, it needs to first learn exploit 
the model and perform sufficiently well using it.
Furthermore, what it learned at this stage may become obsolete as the model changes.
How bad this problem is depends on the specifics of the setting,
and there are many ways to ameliorate it,
but in most cases the necessary trade-offs result in a lower final performance.
All this will be further discussed in a later chapter.

\section{Goal of the thesis}
\label{sec-thesis-goal}
Given the previous discussion, the goal of the thesis may be presented:
the idea is to combine the sample-efficiency of model-based approaches
with the flexibility of model-free methods.
Another way to describe the same is to say that we want
to utilize learning signals other than the reward signal
to make the model-free learning more sample-efficient.
In particular, we want to learn a latent representation of the environment,
i.e. to find lower-dimensional embeddings of the environment,
and learn a policy in this space.
To make this a concrete and manageable goal,
we constrain ourselves to the problem of learning from images in particular.
To be able to compare our results to those of other researchers,
we will test our algorithms on the standard benchmark tasks in the field,
namely Atari57 games.
The potential benefits of the proposed approach are two-fold:
 \begin{itemize}
    \item First, we know that, in general, lower-dimensional optimization 
			problems are easier to solve than higher-dimensional ones.
    \item Second, it is known that when algorithms learn with direct state access, 
			they learn much faster and often achieve better final results.
    The main reason behind this is that images are much 
	higher-dimensional than underlying states, 
	and this is self-evident in the case of Atari games. 
	Since inferring states from observations is not directly related to the reward,
	we expect that using unsupervised learning techniques will aid in 
	feature extraction and thus make learning more sample-efficient.
\end{itemize}
%firstly, we know that in general lower-dimensional optimization problems
%are easier to solve than higher-dimensional ones.
%Secondly, it is known that when algorithms learn with direct state access,
%they learn much faster and often achieve better final results.
%The main reason behind this is that images are much higher-dimensional
%than underlying states, and this is self-evident in the case of Atari games.
%Since inferring states from observations is not directly related to the reward,
%we expect that using unsupervised learning techniques will aid in feature extraction
and thus make learning more sample-efficient.
Furthermore, since the goal is not to learn the dynamics of the environment,
but simply to find an equivalent, but lower-dimensional representation of it,
we expect that this approach won't suffer from the problems faced
by model-based approaches.
Of course, we are not the first to suggest such an approach.
An overview of the field is provided in \ref{ch-related-work}.

\subsection{Hypothesis}
\label{subsec-hypothesis}
As already stated, we believe that leveraging unsupervised learning techniques
to learn state representations will make reinforcement learning from images
more sample-efficient.
Testing whether this is in fact true is one of our tasks.
As will be shown in \ref{sec-general-latent-space-learning}, there are many different unsupervised learning
techniques which can be adapted to the goal of state representation learning.
Furthermore, there are many different ways in which state representation learning can be integrated
in the reinforcement learning process.
In this thesis, our goal is not to arrive at a new state-of-the-art algorithm,
but to investigate which properties of both the state representation learning and its integration
with reinforcement learning yield better results.
We will not test all of the existing approaches, but rather identify their common properties,
form hypothesis based on those properties and perform tests on a simple implementation.
Here we offer our hypothesis:
\begin{enumerate}
		\item State representations found by general unsupervised learning techniques 
		will not equate to true states, although they will be closer to them than
		raw image-based observations. Reinforcement learning algorithms are able
		to implicitly learn true states, but because they do so indirectly and by using
		the weak reward signal, they do so very slowly.
		Thus we hypothesize that allowing the reinforcement learning algorithm to continue updating
		feature extraction provided the state representation learning algorithm will perform better
		than feature extraction learned solely through state representation learning.
		We further hypothesize that the best feature extraction will be obtained if both
		state representation learning algorithm and the reinforcement learning algorithm 
		continuously update the feature extractor throughout the entire training process.
		\label{parallel-training-hypothesis}
\item We hypothesize that state representation learning algorithms whose learned features better match the underlying
		Markov decision process will yield better results.
		This for example means that representations learned on future prediction tasks will
		perform better than those which are not incentivized to learn dynamics.
		\label{good-features-hypothesis}
\item Finally, we hypothesize that strong regularization of the state representation learning
		algorithms will yield better results. We believe that proper regularization will
		yield broader features which the reinforcement learning algorithm will 
		more easily integrate with.
		\label{regularization-hypothesis}
\end{enumerate}




\subsection{Contributions}
\label{subsec-contributions}
As already mentioned, our main goal is not to produce a state-of-the-art algorithm,
but rather to find general properties which state representation learning algorithms 
should have and how they should best be integrated with reinforcement learning algorithms.
In this thesis we provide the following contributions:
\begin{enumerate}
	\item A systematic overview of recent works which leverage state representation learning
		to make model-free reinforcement learning more sample-efficient.
	\item Extensive testing of our hypothesis which illuminate the problem and pave the wave 
			for further algorithm development.
	\item Implementation of our method in a high-quality reinforcement learning library.
			Despite the fact that our method is not the best available one, its generality
			and its implementation makes it easily accessible to practitioners and 
			helps researchers who wish to build on top of it.
\end{enumerate}

\section{Outline}
The rest of this text is organized as follows.
We begin by describing the basics of reinforcement learning in \ref{sec-rl-intro}.
Here the problem setting and basic concepts are covered.
Then main classes of 
reinforcement learning are introduced in \ref{sec-rl-alg-classes}.
Having the basics established, in \ref we introduce the reinforcement learning algorithm 
we use in our implementation
and discuss common reinforcement learning problems in \ref{sec-rl-problems}.
Following the reinforcement learning discussion, we
turn our attention to unsupervised learning on images in \ref{sec-unsupervised-learning-on-images}
and introduce state representation learning in \ref{sec-srl-intro}.
%In previous publications, 
%this information is scattered through different sources, 
%the goal of this chapter is to give our readers one-place guide for reinforcement learning.
Following the background,
discuss related work in \ref{ch-related-work}.
Here we cover several existing approaches to bolstering the sample-efficiency of model-free reinforcement learning with
state representation learning. 
Having covered the field, in \ref{sec-hypothesis-reasoning} we identify key factors which lead to state representations which 
can be leveraged by reinforcement learning algorithms.
In other words, we then form the basis for our hypothesis.




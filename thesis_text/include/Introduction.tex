% CREATED BY DAVID FRISK, 2016
\chapter{Introduction}

%Before 1980s Reinforcement learning (RL) has two main threads. One thread associated with learning by trail and error in animal psychology and the second thread concerned with  optimal control and its solution in value function and dynamic programming[1].A third  less common type of reinforcement learning is temporal difference method which is used in tic-tac-toe.After 1980s by combining these three threads modern reinforcement learning has emerged.\\
In computer science, reinforcement learning is the formalization of trial-and-error learning.
While this is not the only legitimate interpretation of the concept, it is the most straightforward one:
``trial'' implies existance of an agent which observes its environment and interacts with it 
though its own actions.
``Error'' implies that the agent has a goal it tries to achieve and
that it does not know how to achieve it (in the most effective manner). 
What it can do is take different actions and appraise them
according to how closely they lead the agent toward its goal, thereby observing
the quality of those actions.
By repeatedly exploring the effects of various sequences of actions, the agent
can find, i.e. learn, the sequence of actions which lead to its goal.
Here it is important to discuss what a goal is.
To formalize the process outlined above, one needs to described
in purely mathematical terms, and that includes the goal as well.
To achieve that, the notion of a reward function is used:
it maps every state of the environment to a number which denotes
its value called the reward. 
The state of the environment to which the highest reward is ascribed
is then the goal.
A more general description of the goal of reinforcement learning
is to maximize the sum of rewards over time.
Formalization of the entire process will be carried out later in the text,
while here only the most important concepts will be outlined.
One of these is the trade-off between ``exploration''
and ``exploitation.''
To learn just by trial and error implies learning from experience.
This means that the agent can not know 
how a certain strategy fares unless it collects experiences 
which come by following said strategy.
Thus in order to find a good strategy,
usually refered to as a ``policy'',
the agent needs to produce various different strategies and observe their results
until it find a promising one.
The process of finding different strategies and experimenting with new random
behavior is called exploration.
Likewise, the process of repeating a good strategy is called exploitation.
Due to the curse of dimensionality, 
\footnote{The curse of dimensionality refers to the exponential rise of 
possible configurations of the problem with the number of dimensions.}
in complex multi-dimensional domains it is impossible to test but a miniscule proportion 
of all possible strategies.
Because of this, the problem of effective exploration and the trade-off between
it and exploitation is a fundamental problem in reinforcement learning.

 
%Reinforcement learning is learning what to do—how to map situations to actions—so as to maximize a numerical reward signal[1].It is a discipline that encompass learning problem,a class of solutions related to learning and field of study of problems in learning and solution methods. It is most of time confused with unsupervised learning because it does not rely on examples of correct behavior or labeled data. unlike unsupervised learning  reinforcement learning is trying to maximize a reward signal instead of learning to find hidden structure.\\

Due to its generality, reinforcement learning is studied in many different disciplines: control theory, game theory,
information theory, simulation-based optimization, multi-agent systems etc..
Of these, control theory is of particular importance because it
often enables clear analysis of various reinforcement learning algorithms.
This foremost concerns the usage of dynamic programming which
provides a basis for a large class of reinforcement learning algorithms.
Reinforcement learning is also considered to be one of the pillars of modern data-driven machine learning.
In the context of machine learning, reinforcement learning can be view as a combination of supervised and unsupervised learning:
the ``trial'' portion of the trial-and-error learning can be interpreted as unsupervised or as self-supervised learning
because in it the agent collects its own dataset without any explicit labels to guide its way.
This process is refered to as ``exploration''.
The dataset created by exploration is labelled by the reward function.
Thus the agent can learning from ``past experience'' in a supervised manner.
This text will introduce concepts from both control theory
and machine learning which are necessary to formalize the reinforcement learning objective
and to develop algorithms to achieve it.
It will not concern itself with other disciplines.

Interest in reinforcement learning has grown tremendously over the past decade.
It has been fueled by successes of deep machine learning in fields such as computer vision.
The subsequent utilization of neural networks in reinforcement learning,
dubbed deep reinforcement learning,
led to impressive results such as achieving better-than-human
perfomance on Atari games, in the game of go and in many others.
These recent success were kick-started by Deep Q-Networks (DQN) algorithm which,
by utilizing convolutional neural network, crucially enabled the agents to successfully learn from raw pixels.
Learning from pixels is incredibly important for most practical applications,
such as those in robotics
where it is usually impossible to get full access to the state of the environment.
The state then needs to be inferred from obsevations such as those from cameras.
\footnote{Here the state refers to the underlying physical parameters of the environment:
the positions and velocities of objects, the friction coefficients and so on.
Observations from sensors such as cameras do not explicitly provide such information.
However, since humans and animals are able to utilize such observations to achieve their
goals, we know that they implicitely hold enough information about the true state
of the world for successful goal completetion.}
Due to these incredible results in simulated environments,
reinforcement learning holds the promise of solving
many incredibly important engineering problems, for example robotic manipulation
and grasping.
Having that said, there exists a large gap between simple simulated environments and
the real world,
and many improvements to the current state-of-the-art algorithms are required to
bridge that gap.
To explain the approach investigated in this thesis,
a bit more context is needed.

%All reinforcement learning agents have explicit goals, can sense aspects of their environments, and can choose actions to influence their environments.One of the two successful approaches to solve sequential decision making commonly known as  Markov Decision Process(MDP) optimisation problem is reinforcement learning[7].Markov decision processes are intended to include just three aspects  sensation,action and goal  in their simplest possible forms without trivializing any of them[1].we seek actions that brings about state of highest value not highest reward,because these actions obtains the greatest amount of reward for us over the long run.\\

%These recent success in were kick-started by Deep Q-Networks algorithm [2][3].
%Its is a combination of convulational neural network trained with a variant of Q-learning from 
%raw pixels has improved agents to play many atari games at human-level performance.
%One of major problem of this algorithm is  overestimation of  action values under certain conditions, 
%to address this   
%double Q-learning algorithm(DDQN)  [4] was introduced,
%which was initially proposed in tabluar setting [5].
%By decoupling selection and evaluation of the bootstrap action the overestimation bias of DQN were resolved.
%Later the dueling network architecture [6] helps to generalize across actions by 
%separately representing state values and action advantage.
%Since then, many extensions have been proposed that enhance the speed or stability of DQN.\\

An important classification of reinforcement learning algorithm is the one between
model-based and model-free algorithms.
As the name suggests, model-free algorithms do not form an explicit model of the environment.
Instead, they function as black-box optimization algorithms, simply finding actions which maximize
reward without other concerns such as predicting the states resulting from those actions.
In other words, they only predict the reward of actions in given states.
Model-based algorithms on the other hand learn an explicit model of the environment
and use it to plan their actions.
They thus learn the dynamics of the environment and use that knowledge to choose actions
which lead the agent to states with high reward.
Both classes have their benefits and their drawbacks.
Since model-free algorithms do not require any knowledge of environment dynamics
to operate, they are more widely applicable and usually achieve better performance.
But the fact that they can not leverage environment dynamics to create plans implies
a harder learning problem: they need to implicitely learn those dynamics
while only being provided the reward signal. 
This makes them much less sample-efficient.
By contrast, model-based algorithms are of course more sample-efficient.
Furthermore, the plan generated from the learned model can be utilized to interpret the
agent's actions which in turn leads to many further benefits such as
the ability to guarantee outcomes in safety-critical operations.
Unfortunatelly, the twin learning objective of learning the best action-choosing policy 
to maximize the reward over time, and the learning of the model results
in fundamental training instabilities which usually results in worse final performance.
% this explanation is not sufficiently good
In simple terms, the reason behind this is the following one:
in the beggining of the learning process, both the policy and the model perform poorly.
For the model to perform better, the agent needs to explore the environment and
update its model.
However, many parts of the environment are inacessible to a poorly performing agent:
for example, if an agent is playing a computer game, and it is not able to progress to further
sections of the game, it will not be able to construct a model of that portion of the game.
Thus, to explore the environment and improve its model, it needs to first learn exploit 
the model and perform sufficiently well using it.
Furthermore, what it learned at this stage may become obsolete as the model changes.
How bad this problem is depends on the specifics of the setting,
and there are many ways to ameliorate it,
but in most cases the necessary trade-offs result in a lower final performance.
%All this will be further discussed in a later chapter.

Given the previous discussion, the goal of the thesis may be presented:
the idea is to combine the sample-efficiency of model-based approaches
with the flexibility of model-free methods.
Another way to describe the same is to say that we want
to utilize learning signals other than the reward signal
to make the model-free learning more sample-efficient.
In particular, we want to learn a latent representation of the environment,
i.e. to find lower-dimensional embeddings of the environment,
and learn a policy in this space.
To make this a concrete and managable goal,
we constrain ourselves to the problem of learning from images in particular.
To be able to compare our results to those of other researchers,
we will test our algorithms on the standard benchmark tasks in the field,
namely Atari57 games.
The potential benefits of the proposed approach are two-fold:
firstly, we know that in general lower-dimensional optimization problems
are easier to solve than higher-dimensional ones.
Secondly, it is known that when algorithms learn with direct state access,
they learn much faster and often achieve better final results.
The main reason behind this is that images are much higher-dimensional
than underlying states, and this is self-evident in the case of Atari games.
Since inferring states from observations is not directly related to the reward,
we expect that using unsupervised learning techniques will aid in feature extraction
and thus make learning more sample-efficient.
Furthermore, since the goal is not to learn the dynamics of the environment,
but simply to find an equivalent, but lower-dimensional representation of it,
we expect that this approach won't suffer from the problems faced
by model-based approaches.
Of course, we are not the first to suggest such an approach,
but we haven't found a systematic analysis of it accross an array of
model-free algorithms.
Related work will be discussed in its own chapter.

In total, our contributions are the following:
\begin{enumerate}
		\item nothing yet, more tests need to be done, the code is ready,
				but the hyperparameters are not.
\end{enumerate}



%Currently, the best performing algorithms are of the model-free type, they do not explicitly model the dynamics of the environment.
%one of the benefits of using model-free algorithms is its generality and applicability to a wide range of problems.
%In addition to not modeling the dynamics of the environment,
%the lack of interpretability of model -free  algorithm limits there application in safety-critical control systems.\\


%Model based reinforcement learning can be defined as any MDP approach that uses a model(known or learned) 
%and uses learning to approximate a global value or policy function[7].
%model-based approaches [6] offer an avenue to address the two most common issues related to model-free algorithms   
%modeling the dynamics of the environment and the lack of interpretability. 
%Model-based reinforcement learning  converge  faster than their model-free algorithms.\\

%In model-based reinforcement learning both the model and the policy are learned.
%In order to successfully learn the two  model-based algorithm needs to  
%balance between exploration and exploitation to find the optimal policy (like a model-free algorithm) 
%and also balance between learning the model and learning the policy itself. 
%Recent studies on model based reinforcement learning have shown a promise in this paradigm 
%by obtaining a state of the art performance on offline reinforcement learning benchmarks 
%[10][11],improving a powerful model-free approach (e.g.[12]).\\

%The main problem with model based reinforcement 
%learning is balancing between learning the model and policy.
%if the learned model is not a sufficiently good representation of the environment, 
%or if it models only certain parts of the environment well,
%the learned policy will in effect over-fit on that model, resulting in poor performance. 
%On the other hand, if a policy is not performing well during training, 
%the agent won’t be able to access new parts of the environment 
%(for example the agent will not be able to progress to a new level in a game). 
%Often this results in model-based algorithms converging to a poorer policy than their model-free counterparts.\\
%
%In recent years we have seen the use of meta-learning in reinforcement learning.
%which is a form of learning how to learn. 
%Meta-learning algorithms aim to learn models that can adapt to new scenarios or tasks with few data
%points.
%Meta-learning, in the context of RL, aims to learn a policy that 
%adapts fast to new tasks or environments [ 12,13,14].
%Another extension of model based reinforcement learning is offline learning, 
%which utilizes  previously collected data with out additional online data collection.
%In this learning the stochasticity of the environment is maintained by limiting the interaction of the agent  
%to collect additional transition by interacting with the environment by using the behaviour policy.\\

%In Practical application,
%Although the above stated have shown remarkable adavance in the field,
%it is still difficult to learn a precise environment dynamic models.
%In previous studies[2,3,4,6,16] deep Q-learning was trained using raw pixel image,
%which remarks one of greater advantages of using deep learning in reinforcement learning.
%Training reinforcement learning agents from high dimensional image representations 
%can be very computationally expensive.
%By using  Deep auto-encoders for reinforcement learning to compress high dimensional data such 
%as pixelated images into small  latent representation [21]  
%were able to achieve improved performance compared to a baseline reinforcement learning  
%agent with a convolutional feature extractor, while using less than 2W of power.
%While it is not a novel idea to use auto-encoders to accelerate reinforcement 
%learning we have not found a substantial work focused  on the use of compressed 
%representation by auto-encoders to learn to model the dynamic on an Atari  
%environment and subsequently train a policy completely in a simulation.




% CREATED BY DAVID FRISK, 2016
\chapter{Results}
Due to time constraints, we divide our testing in two parts.
In the first part, we test out the basic hypothesis only on the Pong.
This is because Pong is 10 faster to train than other games.
Furthermore, it is the only game in which essentially all states 
are observable immediately.
This makes it possible to fully pretrain the autoencoder,
thus enabling the test \ref{test-ae-fixed}.
In the second part, we test the best performing unsupervised learning
strategy on Pong on the other games to see how it fares in sample-efficiency
against non-augmented reinforcement learning algorithms.
To focus on the difference caused by state representation learning,
we keep all non-relevant hyperparameters the same across all tests.
We also perform tests with a small and a big encoder.

\section{Testing different training styles on Pong}
Here we show the results for two-step, parallel and joint training
of state representations and reinforcement learning on the game Pong.


\section{Multi-game comparison}


% CREATED BY DAVID FRISK, 2016
\chapter{Appendix 1}
The other graphs. 
%%(UNFINISED)
%
%%//UPDATE THE FORMAT LATER 
%\begin{enumerate}
%
%    \item{ Agent: It is an assumed entity which performs actions in an environment to gain some reward.}
%    \item{Environment (e): A scenario that an agent has to face.anything the agent  cannot change arbitrarily is considered to be part of the environment.} 
%    \item{Reward (R): An immediate return given to an agent when he or she performs specific action or task.}
%    \item{ State (s): State refers to the current situation returned by the environment.}
%    \item{Policy ($\pi$): It is a strategy which applies by the agent to decide the next action based on the current state.}
%    \item{ Value (V): It is expected long-term return with discount, as compared to the short-term reward.}
%    \item{ Value Function: It	specifies the value of a state that is the total amount of reward. It is an agent which should be expected beginning from that state.}
%    \item{ Model of the environment: This mimics the behavior of the environment. It helps you to make inferences to be made and also determine how the environment will behave.}
%    \item{ Model based methods: It is a method for solving reinforcement learning problems which use model-based methods.}
%    \item{ Q value or action value (Q): Q value is quite similar to value. The only difference between the two is that it takes an additional parameter as a current action.}
%\end{enumerate}


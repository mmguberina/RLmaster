\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax 
\babel@toc {english}{}
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{List of Figures}{xiii}{chapter*.3}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{List of Tables}{xv}{chapter*.4}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {1.1}What is reinforcement learning?}{1}{section.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {1.2}Why is reinforcement learning interesting?}{2}{section.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {1.3}Why learn from pixels?}{2}{section.1.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {1.4}Efforts to make reinforcement learning more efficient}{3}{section.1.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {1.4.1}Utilizing a world model}{3}{subsection.1.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {1.4.2}Utilizing state representations}{4}{subsection.1.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {1.5}Goal of the thesis}{4}{section.1.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {1.5.1}Hypothesis}{4}{subsection.1.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {1.5.2}Contributions}{5}{subsection.1.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {1.6}Outline}{5}{section.1.6}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {2}Reinforcement learning}{7}{chapter.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.1}Problem setting}{7}{section.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.1.1}Bandit problems}{7}{subsection.2.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.1.2}Markov Decision Processes}{8}{subsection.2.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.2}Key concepts in reinforcement learning}{9}{section.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.2.1}Policy}{9}{subsection.2.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.2.2}Goal of reinforcement learning}{10}{subsection.2.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.2.3}Value functions}{10}{subsection.2.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.3}Classes of reinforcement learning algorithms}{11}{section.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.3.1}Policy gradient algorithms}{11}{subsection.2.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.3.1.1}Baselines}{12}{subsubsection.2.3.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.3.1.2}Off-policy gradients}{13}{subsubsection.2.3.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.3.1.3}Advanced policy gradients}{14}{subsubsection.2.3.1.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.3.2}Actor-critic algorithms}{14}{subsection.2.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.3.3}Value function methods}{16}{subsection.2.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.3.3.1}Dynamic programming}{16}{subsubsection.2.3.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.3.3.2}Fitted value iteration}{17}{subsubsection.2.3.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.4}Deep reinforcement learning with Q-functions}{18}{section.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.4.1}Double Q-networks (DDQN)}{20}{subsection.2.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.4.2}Q-learning with multi-step returns}{20}{subsection.2.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.4.3}Prioritized replay}{20}{subsection.2.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.4.4}Dueling Network}{21}{subsection.2.4.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.4.5}Noisy Nets}{21}{subsection.2.4.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.4.6}Integrated Agent:Rainbow}{21}{subsection.2.4.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.4.7}Deep autoencoders}{21}{subsection.2.4.7}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.5}Problems with RL}{22}{section.2.5}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {3}State representation learning}{25}{chapter.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {3.1}Representation learning in general}{26}{section.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.1.1}Generative models}{27}{subsection.3.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.1.1}Probabilistic models}{27}{subsubsection.3.1.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.1.2}Directed graphical models}{27}{subsubsection.3.1.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.1.3}Directly learning a parametric map from input to representation}{27}{subsubsection.3.1.1.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.1.2}Discriminative models}{28}{subsection.3.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.1.3}Common representation learning approaches}{28}{subsection.3.1.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.3.1}Deterministic autoencoders}{28}{subsubsection.3.1.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.3.2}Variational autoencoders}{29}{subsubsection.3.1.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.3.3}Deterministic autoencoder regularization}{30}{subsubsection.3.1.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {3.2}Representation models for control}{30}{section.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.2.1}Autoencoder}{31}{subsection.3.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.2.2}Forward model}{32}{subsection.3.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.2.3}Inverse model}{32}{subsection.3.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.2.4}Using prior knowledge to constrain the state space}{33}{subsection.3.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.2.5}Using hybring objectives}{34}{subsection.3.2.5}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {3.3}Model-based reinforcement learning}{34}{section.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {4}Related Work}{35}{chapter.4}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.1}Reinforcement learning on Atari}{35}{section.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.2}Efforts in increasing efficiency in Atari}{36}{section.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.3}State representation learning for efficient model-free learning}{36}{section.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.3.1}Deterministic generative models}{37}{subsection.4.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.3.2}Stochastic generative models}{37}{subsection.4.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.3.3}Discriminative models}{38}{subsection.4.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {5}Methods}{39}{chapter.5}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {5.1}Formulating our hypotheses}{39}{section.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {5.2}Our approach}{40}{section.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {5.3}Environment and Preprocessing}{42}{section.5.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {5.4}Implementation}{42}{section.5.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.4.1}Tianshou}{42}{subsection.5.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.4.2}Trainer}{43}{subsection.5.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.4.3}Implementing state representation learning in Tianshou}{44}{subsection.5.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {5.5}Hyperparameters}{44}{section.5.5}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {6}Results}{45}{chapter.6}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {6.1}Testing different training styles on Pong}{45}{section.6.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {6.2}Multi-game comparison}{45}{section.6.2}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {7}Discussion}{47}{chapter.7}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {8}Conclusion}{49}{chapter.8}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{Bibliography}{51}{chapter.8}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {A}Appendix 1}{I}{appendix.A}%

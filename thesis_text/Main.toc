\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax 
\babel@toc {english}{}
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{List of Figures}{xiii}{chapter*.3}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{List of Tables}{xv}{chapter*.4}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {1.1}What is reinforcement learning?}{1}{section.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {1.2}Why is reinforcement learning interesting?}{2}{section.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {1.3}Why learn from pixels?}{2}{section.1.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {1.4}Efforts to make reinforcement learning more efficient}{3}{section.1.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {1.4.1}Utilizing a world model}{3}{subsection.1.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {1.4.2}Utilizing state representations}{4}{subsection.1.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {1.5}Goal of the thesis}{4}{section.1.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {1.5.1}Hypothesis}{4}{subsection.1.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {1.5.2}Contributions}{5}{subsection.1.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {1.6}Outline}{5}{section.1.6}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {2}Reinforcement learning}{7}{chapter.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.1}Problem setting}{7}{section.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.1.1}Bandit problems}{7}{subsection.2.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.1.2}Markov Decision Processes}{8}{subsection.2.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.2}Key concepts in reinforcement learning}{9}{section.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.2.1}Policy}{9}{subsection.2.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.2.2}Goal of reinforcement learning}{10}{subsection.2.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.2.3}Value functions}{10}{subsection.2.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.3}Classes of reinforcement learning algorithms}{11}{section.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.3.1}Policy gradients}{11}{subsection.2.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.3.1.1}Baselines}{12}{subsubsection.2.3.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.3.1.2}Off-policy gradients}{13}{subsubsection.2.3.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.3.1.3}Advanced policy gradients}{14}{subsubsection.2.3.1.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.3.2}Actor-critic algorithms}{14}{subsection.2.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.3.3}Value function methods}{16}{subsection.2.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.3.3.1}Dynamic programming}{16}{subsubsection.2.3.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.4}Deep Q-networks and their extensions}{17}{section.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.4.1}Double Deep Q-networks: DDQN}{17}{subsection.2.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.4.2}Prioritized replay}{18}{subsection.2.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.4.3}Dueling Network}{18}{subsection.2.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.4.4}Multi-step learning}{18}{subsection.2.4.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.4.5}Noisy Nets}{18}{subsection.2.4.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.4.6}Integrated Agent:Rainbow}{19}{subsection.2.4.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.4.7}Deep autoencoders}{19}{subsection.2.4.7}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.5}Problems with RL}{20}{section.2.5}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {3}State representation learning}{23}{chapter.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {3.1}Representation learning in general}{24}{section.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.1.1}Generative models}{25}{subsection.3.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.1.1}Probabilistic models}{25}{subsubsection.3.1.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.1.2}Directed graphical models}{25}{subsubsection.3.1.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.1.3}Directly learning a parametric map from input to representation}{25}{subsubsection.3.1.1.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.1.2}Discriminative models}{26}{subsection.3.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.1.3}Common representation learning approaches}{26}{subsection.3.1.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.3.1}Deterministic autoencoders}{26}{subsubsection.3.1.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.3.2}Variational autoencoders}{27}{subsubsection.3.1.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {3.1.3.3}Deterministic autoencoder regularization}{28}{subsubsection.3.1.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {3.2}Representation models for control}{28}{section.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.2.1}Autoencoder}{29}{subsection.3.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.2.2}Forward model}{30}{subsection.3.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.2.3}Inverse model}{30}{subsection.3.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.2.4}Using prior knowledge to constrain the state space}{31}{subsection.3.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {3.2.5}Using hybring objectives}{32}{subsection.3.2.5}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {3.3}Model-based reinforcement learning}{32}{section.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {4}Related Work}{33}{chapter.4}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.1}Reinforcement learning on Atari}{33}{section.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.2}Efforts in increasing efficiency in Atari}{34}{section.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.3}State representation learning for efficient model-free learning}{34}{section.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.3.1}Deterministic generative models}{35}{subsection.4.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.3.2}Stochastic generative models}{35}{subsection.4.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.3.3}Discriminative models}{36}{subsection.4.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {5}Methods}{37}{chapter.5}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {5.1}Formulating our hypotheses}{37}{section.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {5.2}Our approach}{38}{section.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {5.3}Environment and Preprocessing}{40}{section.5.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {5.4}Implementation}{40}{section.5.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.4.1}Tianshou}{40}{subsection.5.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.4.2}Trainer}{41}{subsection.5.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.4.3}Implementing state representation learning in Tianshou}{42}{subsection.5.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {5.5}Hyperparameters}{42}{section.5.5}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {6}Results}{43}{chapter.6}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {6.1}Testing different training styles on Pong}{43}{section.6.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {6.2}Multi-game comparison}{43}{section.6.2}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {7}Conclusion}{45}{chapter.7}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{Bibliography}{47}{chapter.7}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {A}Appendix 1}{I}{appendix.A}%

\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax 
\babel@toc {english}{}
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{List of Figures}{xi}{chapter*.3}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{List of Tables}{xiii}{chapter*.4}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {2}Background}{5}{chapter.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.1}Introduction to reinforcement learning}{5}{section.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.1.1}Problem setting}{5}{subsection.2.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.1.2}Bandit problems}{6}{subsection.2.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.1.3}Markov Decision Processes}{6}{subsection.2.1.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.1.4}Key concepts in reinforcement learning}{7}{subsection.2.1.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.1.4.1}Policy}{7}{subsubsection.2.1.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.1.4.2}Goal of reinforcement learning}{8}{subsubsection.2.1.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.1.4.3}Value functions}{9}{subsubsection.2.1.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.2}Classes of reinforcement learning algorithms}{9}{section.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.2.1}Policy gradients}{9}{subsection.2.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.2.1.1}Baselines}{10}{subsubsection.2.2.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.2.1.2}Off-policy gradients}{11}{subsubsection.2.2.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {paragraph}{\numberline {2.2.1.2.1}Advanced policy gradient}{11}{paragraph.2.2.1.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.2.2}Actor-critic algorithms}{12}{subsection.2.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.2.3}Value function methods}{14}{subsection.2.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.2.3.1}Dynamic programming}{14}{subsubsection.2.2.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.3}Deep Learning }{14}{section.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.3.1}Auto Encoders : Preliminary location}{14}{subsection.2.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.4}Deep Reinforcement Learning and DQN}{16}{section.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.4.1}Extension of DQN }{16}{subsection.2.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.4.1.1}Double Deep Q-networks: DDQN}{16}{subsubsection.2.4.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.4.1.2}Prioritized replay}{17}{subsubsection.2.4.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.4.1.3}Dueling Network}{17}{subsubsection.2.4.1.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.4.1.4}Multi-step learning}{17}{subsubsection.2.4.1.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.4.1.5}Noisy Nets}{17}{subsubsection.2.4.1.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.4.1.6}Integrated Agent:Rainbow}{18}{subsubsection.2.4.1.6}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.5}Problems with RL}{18}{section.2.5}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.6}general computer vision stuff}{18}{section.2.6}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.7}General latent space learning}{18}{section.2.7}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.7.1}Auto-encoder}{20}{subsection.2.7.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.7.2}Forward model}{20}{subsection.2.7.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.7.3}Inverse model}{21}{subsection.2.7.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.7.4}Using prior knowledge to constrain the state space}{21}{subsection.2.7.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.7.5}Using hybring objectives}{22}{subsection.2.7.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {2.7.6}Common neural network architectures}{22}{subsection.2.7.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.7.6.1}AE}{22}{subsubsection.2.7.6.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.7.6.2}DAE}{22}{subsubsection.2.7.6.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.7.6.3}VAE}{22}{subsubsection.2.7.6.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subsubsection}{\numberline {2.7.6.4}Siamese networks}{23}{subsubsection.2.7.6.4}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {2.8}Model-based reinforcement learning}{23}{section.2.8}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {3}Related Work}{25}{chapter.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {3.1}Work on top of which we build}{25}{section.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {3.2}Current state-of-the-art on Atari, Agent 57}{26}{section.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {3.3}Current state-of-the-art on Atari}{26}{section.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {3.4}Work whose techniques we share}{26}{section.3.4}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {3.5}Work achieving same goals as us, but differently}{26}{section.3.5}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {3.6}Strongly related to our work}{27}{section.3.6}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {4}Methods}{29}{chapter.4}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.1}Problems to be tackled}{29}{section.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {4.2}Hypotheses}{29}{section.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.2.1}Enviroment and Preprocessing}{30}{subsection.4.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.2.2}Deep Auto-encoder and Model Architecture}{30}{subsection.4.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {4.2.3}Training the RL Agent}{30}{subsection.4.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {5}Results}{33}{chapter.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.0.1}Two Step Training}{33}{subsection.5.0.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subsection}{\numberline {5.0.2}Parallel Training}{33}{subsection.5.0.2}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {6}Conclusion}{35}{chapter.6}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {6.1}Discussion}{35}{section.6.1}%
\defcounter {refsection}{0}\relax 
\contentsline {section}{\numberline {6.2}Conclusion}{35}{section.6.2}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{Bibliography}{37}{section.6.2}%
\defcounter {refsection}{0}\relax 
\contentsline {chapter}{\numberline {A}Appendix 1}{I}{appendix.A}%

\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Plan}{5}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsubsection}{Old plan, look at update}{5}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Update}{5}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.1}TODOs}{6}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Purpose}{6}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Immitation learning}{7}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Formal setting}{8}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Markov chain}{8}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Markov decision process}{8}{section.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Partially observed Markov decision process}{9}{section.3.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}The goal of reinforcement learning}{9}{subsection.3.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Note}{10}{section*.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Value functions}{10}{section.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Definition: Q-function}{11}{section*.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Definition: value function}{11}{section*.7}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Policy gradients}{12}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsubsection}{The idea}{12}{section*.8}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Reducing variance}{14}{section.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Causality}{14}{section*.9}\protected@file@percent }
\newlabel{eq:reward_to_go}{{4.20}{14}{Causality}{equation.4.1.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Baselines}{15}{subsection.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Off-policy gradients}{16}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Importance sampling}{16}{section*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Policy gradient with automatic differentiation}{18}{subsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Policy gradients in practice}{18}{subsection.4.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Advanced policy gradients}{19}{section.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Actor-critic algorithms}{20}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Policy evaluation}{21}{section.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}From evaluation to actor-critic}{23}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Aside: discount factors}{23}{section.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Actor-critic design choises}{24}{section.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Online actor-critic in practise}{24}{section.5.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Critics as state-dependent baselines}{26}{section.5.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.1}Eligibility traces and n-step returns}{27}{subsection.5.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.2}Generalied advantage estimation (GAE)}{28}{subsection.5.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Value function methods}{29}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.0.1}Can we omit policy gradient completely?}{29}{subsection.6.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Policy iteration}{29}{section.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Dynamic programming}{29}{subsection.6.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}Policy iteration with dynamic programming}{30}{subsection.6.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.3}Even simpler dynamic programming}{30}{subsection.6.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.4}Fitted value iteration and Q-iteration}{30}{subsection.6.1.4}\protected@file@percent }
\newlabel{eq:fitted_q_iteration_algorithm}{{6.1.4}{31}{Fitted value iteration and Q-iteration}{Item.38}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}From Q-iteration to Q-learning}{31}{section.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Why is this algorithm off-policy}{31}{subsection.6.2.1}\protected@file@percent }
\newlabel{eq:online_q_iteration_algorithm}{{6.2.1}{32}{Why is this algorithm off-policy}{equation.6.2.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Value function in theory}{32}{section.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}A sad corollary}{34}{subsection.6.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Deep RL with Q-functions}{35}{chapter.7}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.0.1}Replay buffers}{35}{subsection.7.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Target networks}{36}{section.7.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.2}A general view of Q-learning}{36}{section.7.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Improving Q-learning}{37}{section.7.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.1}Are Q-values accurate?}{37}{subsection.7.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.4}Double Q-learning}{37}{section.7.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.1}Double Q-learning in practise}{37}{subsection.7.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.2}Multi-step returns}{38}{subsection.7.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4.3}Q-learning with N-step returns}{38}{subsection.7.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.5}Q-learning with continuous actions}{38}{section.7.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Option 1}{38}{section*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Option 2}{39}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Option 3}{39}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.1}DDPG}{39}{subsection.7.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7.6}Implementation tips and examples}{40}{section.7.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Tips}{40}{section*.14}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Even more advanced policy gradients (PPO and TRPO)}{41}{chapter.8}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.0.1}Policy gradient as policy iteration}{42}{subsection.8.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.0.2}Bounding the objective value}{44}{subsection.8.0.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Policy gradients with constraints}{45}{section.8.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1.1}How do we enforce the constraint}{45}{subsection.8.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Natural gradient}{46}{section.8.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Do these results carry over in practise?}{47}{section*.15}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {8.3}Practical methods and notes}{47}{section.8.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Natural policy gradient}{47}{section*.16}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Trust region policy optimization}{47}{section*.17}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Just using importance sampling objective directly}{47}{section*.18}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Optimal control and planning}{48}{chapter.9}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsubsection}{The objective}{48}{section*.19}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.0.1}Stochastic optimization}{49}{subsection.9.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.0.2}Cross-entropy method (CEM)}{49}{subsection.9.0.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.0.3}Discrete case: Monte Carlo tree search (MCTS)}{50}{subsection.9.0.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9.1}Trajectory optimization with derivatives}{50}{section.9.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.1}Linear case: LQR}{51}{subsection.9.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {9.2}LQR for stochastic and nonlinear systems}{53}{section.9.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.1}Nonlinear case: differential dynamic programming (DDP)/ iterative LQR}{54}{subsection.9.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.2}Nonlinear model-predictive control}{56}{subsection.9.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {10}Model-based reinforcement learning}{57}{chapter.10}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {10.1}Uncertainty in model-based RL}{58}{section.10.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.1}Uncertainty-aware neural network models}{59}{subsection.10.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Idea 1:}{59}{section*.20}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Idea 2:}{59}{section*.21}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.2}Quick overview of Bayesian neural networks}{59}{subsection.10.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.3}Bootstrap ensembles}{60}{subsection.10.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1.4}How to plan with uncertainty}{61}{subsection.10.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Other options:}{61}{section*.22}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {10.2}Model-based reinforcement learning with images}{61}{section.10.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.1}State space (latent space models)}{62}{subsection.10.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {11}Model-based policy learning}{64}{chapter.11}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.0.1}What's the solution?}{65}{subsection.11.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{First class of solutions}{65}{section*.23}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Second class of solutions}{65}{section*.24}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {11.1}Model-free learning with a model}{65}{section.11.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.1.1}Dyna}{66}{subsection.11.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{General ``Dyna-style'' model-based RL recipe}{66}{section*.25}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {11.1.2}Local models}{67}{subsection.11.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {12}Exploration algorithms}{68}{chapter.12}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.0.1}Optimistic exploration}{68}{subsection.12.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.0.2}Probability matching/posterior sampling}{68}{subsection.12.0.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.0.3}Information gain}{69}{subsection.12.0.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.0.4}General themes}{69}{subsection.12.0.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12.1}Exploration in deep reinforcement learning}{70}{section.12.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1.1}Fitting generative models}{70}{subsection.12.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1.2}What kind of bonus to use?}{71}{subsection.12.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{What kind of model to use?}{71}{section*.26}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12.2}Posterior sampling in deep RL}{71}{section.12.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12.3}Information gain in DRL}{72}{section.12.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12.4}Exploration with model errors}{72}{section.12.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {12.5}Unsupervised exploration}{72}{section.12.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {12.5.1}Information theoretic quantities in RL}{73}{subsection.12.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {13}Unsupervised reinforcement learning (sketches)}{74}{chapter.13}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.0.1}Aside: exploration with intrinsic motivation}{75}{subsection.13.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {13.1}Learning diverse skills}{76}{section.13.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1.1}Diversity-promoting reward function}{76}{subsection.13.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {14}Generalisation gap}{77}{chapter.14}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsubsection}{What makes modern machine learning work}{77}{section*.27}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.0.1}Why is offline RL hard?}{78}{subsection.14.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.0.2}Where does RL suffer from distributional shift?}{79}{subsection.14.0.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {14.1}Batch RL via importance sampling}{79}{section.14.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.1.1}The doubly robust estimator}{81}{subsection.14.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {14.1.2}Marginalized importance sampling}{81}{subsection.14.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {14.2}Batch RL via linear fitted value functions}{82}{section.14.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {15}Reinforcement learning as an inference problem}{83}{chapter.15}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {15.1}Optimal control as a model of human behavior}{83}{section.15.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {15.2}Control as inference}{84}{section.15.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.2.1}Backward messages}{85}{subsection.15.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.2.2}But what the if the action prior is not uniform?}{86}{subsection.15.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {15.3}Policy computation}{87}{section.15.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {15.4}Forward messages}{88}{section.15.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {15.5}Control as variational inference}{88}{section.15.5}\protected@file@percent }
\newlabel{eq:policy_inference}{{15.56}{89}{Control as variational inference}{equation.15.5.56}{}}
\newlabel{eq:transition_probability_inference}{{15.57}{89}{Control as variational inference}{equation.15.5.57}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.5.1}Variational lower bound}{89}{subsection.15.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.1}{\ignorespaces $ p (\bm  {s}_{1:T}, \bm  {a}_{1:T}|\mathcal  {O}_{ 1:T }) $}}{90}{figure.15.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {15.2}{\ignorespaces $ q (\bm  {s}_{1:T}, \bm  {a}_{1:T}) $}}{90}{figure.15.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.5.2}Optimizing the variational lower bound}{91}{subsection.15.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {15.6}Algorithms for RL as inference}{92}{section.15.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.6.1}Q-learning with soft optimality}{92}{subsection.15.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.6.2}Policy gradient with soft optimality}{93}{subsection.15.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.6.3}Policy gradient vs Q-learning}{93}{subsection.15.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.6.4}Benefits of soft optimality}{94}{subsection.15.6.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {15.6.5}Example methods}{94}{subsection.15.6.5}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {16}Inverse reinforcement learning}{95}{chapter.16}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {subsection}{\numberline {16.0.1}Why should we worry about learning rewards?}{95}{subsection.16.0.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Imitation learning perspective}{95}{section*.28}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The RL perspective}{95}{section*.29}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Inverse RL}{96}{section*.30}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.0.2}Feature matching IRL}{96}{subsection.16.0.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.0.3}Learning the optimality variable}{97}{subsection.16.0.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16.1}{\ignorespaces $ p (\bm  {s}_{1:T}, \bm  {a}_{1:T}|\mathcal  {O}_{ 1:T }) $}}{97}{figure.16.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The IRL partition function}{97}{section*.31}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Estimating the expectation}{98}{section*.32}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{The MaxEnt IRL algorithm}{99}{section*.33}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {16.1}Approximations to higher dimensions}{99}{section.16.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.1.1}Unknown dynamics and larges state/action spaces}{100}{subsection.16.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Guided policy cost}{100}{section*.34}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16.2}{\ignorespaces Guided cost learning algorithm}}{101}{figure.16.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {16.1.2}IRL and GANs}{101}{subsection.16.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {17}Transfer and multi-task learning}{102}{chapter.17}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {paragraph}{Question}{102}{section*.35}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {17.1}Transfer learning terminology}{102}{section.17.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Transfer learning:}{102}{section*.36}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.1.1}How can we frame transfer learning problems?}{103}{subsection.17.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {17.2}Forward transfer}{103}{section.17.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.2.1}What are the likely issues?}{103}{subsection.17.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Domain shift}{103}{section*.37}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Difference in the MDP}{103}{section*.38}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Finetuning issues}{103}{section*.39}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Domain adaptation in computer vision}{104}{section*.40}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.2.2}How can we apply this idea in RL?}{104}{subsection.17.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Domain adaptation in RL for dynamics?}{104}{section*.41}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{What if we can also finetune?}{104}{section*.42}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {17.3}Forward transfer with randomization}{105}{section.17.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.3.1}What if we can manipulate the source domain?}{105}{subsection.17.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{EDOpt: randomizing physical parameters}{105}{section*.43}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {17.4}Multi-task transfer}{105}{section.17.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {17.1}{\ignorespaces Single-task MDP}}{105}{figure.17.1}\protected@file@percent }
\newlabel{fig:}{{17.1}{105}{Single-task MDP}{figure.17.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17.2}{\ignorespaces Multi-task MDP}}{105}{figure.17.2}\protected@file@percent }
\newlabel{fig:}{{17.2}{105}{Multi-task MDP}{figure.17.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{What's difficult about this?}{105}{section*.44}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Distillation for multi-task transfer}{106}{section*.45}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Divide and conquer RL}{106}{section*.46}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{How does the model know what to do?}{106}{section*.47}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {17.5}Transfering models and value functions}{106}{section.17.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.5.1}The problem setting}{106}{subsection.17.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Assumption:}{106}{section*.48}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.5.2}Transferring models}{106}{subsection.17.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {17.5.3}Transferring value functions}{107}{subsection.17.5.3}\protected@file@percent }

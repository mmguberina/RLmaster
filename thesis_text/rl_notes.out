\BOOKMARK [0][-]{chapter.1}{Plan}{}% 1
\BOOKMARK [1][-]{section.1.1}{TODOs}{chapter.1}% 2
\BOOKMARK [0][-]{chapter.2}{Immitation learning}{}% 3
\BOOKMARK [0][-]{chapter.3}{Formal setting}{}% 4
\BOOKMARK [1][-]{section.3.1}{Markov chain}{chapter.3}% 5
\BOOKMARK [1][-]{section.3.2}{Markov decision process}{chapter.3}% 6
\BOOKMARK [1][-]{section.3.3}{Partially observed Markov decision process}{chapter.3}% 7
\BOOKMARK [2][-]{subsection.3.3.1}{The goal of reinforcement learning}{section.3.3}% 8
\BOOKMARK [1][-]{section.3.4}{Value functions}{chapter.3}% 9
\BOOKMARK [0][-]{chapter.4}{Policy gradients}{}% 10
\BOOKMARK [1][-]{section.4.1}{Reducing variance}{chapter.4}% 11
\BOOKMARK [2][-]{subsection.4.1.1}{Baselines}{section.4.1}% 12
\BOOKMARK [1][-]{section.4.2}{Off-policy gradients}{chapter.4}% 13
\BOOKMARK [2][-]{subsection.4.2.1}{Policy gradient with automatic differentiation}{section.4.2}% 14
\BOOKMARK [2][-]{subsection.4.2.2}{Policy gradients in practice}{section.4.2}% 15
\BOOKMARK [1][-]{section.4.3}{Advanced policy gradients}{chapter.4}% 16
\BOOKMARK [0][-]{chapter.5}{Actor-critic algorithms}{}% 17
\BOOKMARK [1][-]{section.5.1}{Policy evaluation}{chapter.5}% 18
\BOOKMARK [1][-]{section.5.2}{From evaluation to actor-critic}{chapter.5}% 19
\BOOKMARK [1][-]{section.5.3}{Aside: discount factors}{chapter.5}% 20
\BOOKMARK [1][-]{section.5.4}{Actor-critic design choises}{chapter.5}% 21
\BOOKMARK [1][-]{section.5.5}{Online actor-critic in practise}{chapter.5}% 22
\BOOKMARK [1][-]{section.5.6}{Critics as state-dependent baselines}{chapter.5}% 23
\BOOKMARK [2][-]{subsection.5.6.1}{Eligibility traces and n-step returns}{section.5.6}% 24
\BOOKMARK [2][-]{subsection.5.6.2}{Generalied advantage estimation \(GAE\)}{section.5.6}% 25
\BOOKMARK [0][-]{chapter.6}{Value function methods}{}% 26
\BOOKMARK [1][-]{subsection.6.0.1}{Can we omit policy gradient completely?}{chapter.6}% 27
\BOOKMARK [1][-]{section.6.1}{Policy iteration}{chapter.6}% 28
\BOOKMARK [2][-]{subsection.6.1.1}{Dynamic programming}{section.6.1}% 29
\BOOKMARK [2][-]{subsection.6.1.2}{Policy iteration with dynamic programming}{section.6.1}% 30
\BOOKMARK [2][-]{subsection.6.1.3}{Even simpler dynamic programming}{section.6.1}% 31
\BOOKMARK [2][-]{subsection.6.1.4}{Fitted value iteration and Q-iteration}{section.6.1}% 32
\BOOKMARK [1][-]{section.6.2}{From Q-iteration to Q-learning}{chapter.6}% 33
\BOOKMARK [2][-]{subsection.6.2.1}{Why is this algorithm off-policy}{section.6.2}% 34
\BOOKMARK [1][-]{section.6.3}{Value function in theory}{chapter.6}% 35
\BOOKMARK [2][-]{subsection.6.3.1}{A sad corollary}{section.6.3}% 36
\BOOKMARK [0][-]{chapter.7}{Deep RL with Q-functions}{}% 37
\BOOKMARK [1][-]{subsection.7.0.1}{Replay buffers}{chapter.7}% 38
\BOOKMARK [1][-]{section.7.1}{Target networks}{chapter.7}% 39
\BOOKMARK [1][-]{section.7.2}{A general view of Q-learning}{chapter.7}% 40
\BOOKMARK [1][-]{section.7.3}{Improving Q-learning}{chapter.7}% 41
\BOOKMARK [2][-]{subsection.7.3.1}{Are Q-values accurate?}{section.7.3}% 42
\BOOKMARK [1][-]{section.7.4}{Double Q-learning}{chapter.7}% 43
\BOOKMARK [2][-]{subsection.7.4.1}{Double Q-learning in practise}{section.7.4}% 44
\BOOKMARK [2][-]{subsection.7.4.2}{Multi-step returns}{section.7.4}% 45
\BOOKMARK [2][-]{subsection.7.4.3}{Q-learning with N-step returns}{section.7.4}% 46
\BOOKMARK [1][-]{section.7.5}{Q-learning with continuous actions}{chapter.7}% 47
\BOOKMARK [2][-]{subsection.7.5.1}{DDPG}{section.7.5}% 48
\BOOKMARK [1][-]{section.7.6}{Implementation tips and examples}{chapter.7}% 49
\BOOKMARK [0][-]{chapter.8}{Even more advanced policy gradients \(PPO and TRPO\)}{}% 50
\BOOKMARK [1][-]{subsection.8.0.1}{Policy gradient as policy iteration}{chapter.8}% 51
\BOOKMARK [2][-]{subsection.8.0.2}{Bounding the objective value}{subsection.8.0.1}% 52
\BOOKMARK [1][-]{section.8.1}{Policy gradients with constraints}{chapter.8}% 53
\BOOKMARK [2][-]{subsection.8.1.1}{How do we enforce the constraint}{section.8.1}% 54
\BOOKMARK [1][-]{section.8.2}{Natural gradient}{chapter.8}% 55
\BOOKMARK [1][-]{section.8.3}{Practical methods and notes}{chapter.8}% 56
\BOOKMARK [0][-]{chapter.9}{Optimal control and planning}{}% 57
\BOOKMARK [1][-]{subsection.9.0.1}{Stochastic optimization}{chapter.9}% 58
\BOOKMARK [2][-]{subsection.9.0.2}{Cross-entropy method \(CEM\)}{subsection.9.0.1}% 59
\BOOKMARK [2][-]{subsection.9.0.3}{Discrete case: Monte Carlo tree search \(MCTS\)}{subsection.9.0.1}% 60
\BOOKMARK [1][-]{section.9.1}{Trajectory optimization with derivatives}{chapter.9}% 61
\BOOKMARK [2][-]{subsection.9.1.1}{Linear case: LQR}{section.9.1}% 62
\BOOKMARK [1][-]{section.9.2}{LQR for stochastic and nonlinear systems}{chapter.9}% 63
\BOOKMARK [2][-]{subsection.9.2.1}{Nonlinear case: differential dynamic programming \(DDP\)/ iterative LQR}{section.9.2}% 64
\BOOKMARK [2][-]{subsection.9.2.2}{Nonlinear model-predictive control}{section.9.2}% 65
\BOOKMARK [0][-]{chapter.10}{Model-based reinforcement learning}{}% 66
\BOOKMARK [1][-]{section.10.1}{Uncertainty in model-based RL}{chapter.10}% 67
\BOOKMARK [2][-]{subsection.10.1.1}{Uncertainty-aware neural network models}{section.10.1}% 68
\BOOKMARK [2][-]{subsection.10.1.2}{Quick overview of Bayesian neural networks}{section.10.1}% 69
\BOOKMARK [2][-]{subsection.10.1.3}{Bootstrap ensembles}{section.10.1}% 70
\BOOKMARK [2][-]{subsection.10.1.4}{How to plan with uncertainty}{section.10.1}% 71
\BOOKMARK [1][-]{section.10.2}{Model-based reinforcement learning with images}{chapter.10}% 72
\BOOKMARK [2][-]{subsection.10.2.1}{State space \(latent space models\)}{section.10.2}% 73
\BOOKMARK [0][-]{chapter.11}{Model-based policy learning}{}% 74
\BOOKMARK [1][-]{subsection.11.0.1}{What's the solution?}{chapter.11}% 75
\BOOKMARK [1][-]{section.11.1}{Model-free learning with a model}{chapter.11}% 76
\BOOKMARK [2][-]{subsection.11.1.1}{Dyna}{section.11.1}% 77
\BOOKMARK [2][-]{subsection.11.1.2}{Local models}{section.11.1}% 78
\BOOKMARK [0][-]{chapter.12}{Exploration algorithms}{}% 79
\BOOKMARK [1][-]{subsection.12.0.1}{Optimistic exploration}{chapter.12}% 80
\BOOKMARK [2][-]{subsection.12.0.2}{Probability matching/posterior sampling}{subsection.12.0.1}% 81
\BOOKMARK [2][-]{subsection.12.0.3}{Information gain}{subsection.12.0.1}% 82
\BOOKMARK [2][-]{subsection.12.0.4}{General themes}{subsection.12.0.1}% 83
\BOOKMARK [1][-]{section.12.1}{Exploration in deep reinforcement learning}{chapter.12}% 84
\BOOKMARK [2][-]{subsection.12.1.1}{Fitting generative models}{section.12.1}% 85
\BOOKMARK [2][-]{subsection.12.1.2}{What kind of bonus to use?}{section.12.1}% 86
\BOOKMARK [1][-]{section.12.2}{Posterior sampling in deep RL}{chapter.12}% 87
\BOOKMARK [1][-]{section.12.3}{Information gain in DRL}{chapter.12}% 88
\BOOKMARK [1][-]{section.12.4}{Exploration with model errors}{chapter.12}% 89
\BOOKMARK [1][-]{section.12.5}{Unsupervised exploration}{chapter.12}% 90
\BOOKMARK [2][-]{subsection.12.5.1}{Information theoretic quantities in RL}{section.12.5}% 91
\BOOKMARK [0][-]{chapter.13}{Unsupervised reinforcement learning \(sketches\)}{}% 92
\BOOKMARK [1][-]{subsection.13.0.1}{Aside: exploration with intrinsic motivation}{chapter.13}% 93
\BOOKMARK [1][-]{section.13.1}{Learning diverse skills}{chapter.13}% 94
\BOOKMARK [2][-]{subsection.13.1.1}{Diversity-promoting reward function}{section.13.1}% 95
\BOOKMARK [0][-]{chapter.14}{Generalisation gap}{}% 96
\BOOKMARK [1][-]{subsection.14.0.1}{Why is offline RL hard?}{chapter.14}% 97
\BOOKMARK [2][-]{subsection.14.0.2}{Where does RL suffer from distributional shift?}{subsection.14.0.1}% 98
\BOOKMARK [1][-]{section.14.1}{Batch RL via importance sampling}{chapter.14}% 99
\BOOKMARK [2][-]{subsection.14.1.1}{The doubly robust estimator}{section.14.1}% 100
\BOOKMARK [2][-]{subsection.14.1.2}{Marginalized importance sampling}{section.14.1}% 101
\BOOKMARK [1][-]{section.14.2}{Batch RL via linear fitted value functions}{chapter.14}% 102
\BOOKMARK [0][-]{chapter.15}{Reinforcement learning as an inference problem}{}% 103
\BOOKMARK [1][-]{section.15.1}{Optimal control as a model of human behavior}{chapter.15}% 104
\BOOKMARK [1][-]{section.15.2}{Control as inference}{chapter.15}% 105
\BOOKMARK [2][-]{subsection.15.2.1}{Backward messages}{section.15.2}% 106
\BOOKMARK [2][-]{subsection.15.2.2}{But what the if the action prior is not uniform?}{section.15.2}% 107
\BOOKMARK [1][-]{section.15.3}{Policy computation}{chapter.15}% 108
\BOOKMARK [1][-]{section.15.4}{Forward messages}{chapter.15}% 109
\BOOKMARK [1][-]{section.15.5}{Control as variational inference}{chapter.15}% 110
\BOOKMARK [2][-]{subsection.15.5.1}{Variational lower bound}{section.15.5}% 111
\BOOKMARK [2][-]{subsection.15.5.2}{Optimizing the variational lower bound}{section.15.5}% 112
\BOOKMARK [1][-]{section.15.6}{Algorithms for RL as inference}{chapter.15}% 113
\BOOKMARK [2][-]{subsection.15.6.1}{Q-learning with soft optimality}{section.15.6}% 114
\BOOKMARK [2][-]{subsection.15.6.2}{Policy gradient with soft optimality}{section.15.6}% 115
\BOOKMARK [2][-]{subsection.15.6.3}{Policy gradient vs Q-learning}{section.15.6}% 116
\BOOKMARK [2][-]{subsection.15.6.4}{Benefits of soft optimality}{section.15.6}% 117
\BOOKMARK [2][-]{subsection.15.6.5}{Example methods}{section.15.6}% 118
\BOOKMARK [0][-]{chapter.16}{Inverse reinforcement learning}{}% 119
\BOOKMARK [1][-]{subsection.16.0.1}{Why should we worry about learning rewards?}{chapter.16}% 120
\BOOKMARK [2][-]{subsection.16.0.2}{Feature matching IRL}{subsection.16.0.1}% 121
\BOOKMARK [2][-]{subsection.16.0.3}{Learning the optimality variable}{subsection.16.0.1}% 122
\BOOKMARK [1][-]{section.16.1}{Approximations to higher dimensions}{chapter.16}% 123
\BOOKMARK [2][-]{subsection.16.1.1}{Unknown dynamics and larges state/action spaces}{section.16.1}% 124
\BOOKMARK [2][-]{subsection.16.1.2}{IRL and GANs}{section.16.1}% 125
\BOOKMARK [0][-]{chapter.17}{Transfer and multi-task learning}{}% 126
\BOOKMARK [1][-]{section.17.1}{Transfer learning terminology}{chapter.17}% 127
\BOOKMARK [2][-]{subsection.17.1.1}{How can we frame transfer learning problems?}{section.17.1}% 128
\BOOKMARK [1][-]{section.17.2}{Forward transfer}{chapter.17}% 129
\BOOKMARK [2][-]{subsection.17.2.1}{What are the likely issues?}{section.17.2}% 130
\BOOKMARK [2][-]{subsection.17.2.2}{How can we apply this idea in RL?}{section.17.2}% 131
\BOOKMARK [1][-]{section.17.3}{Forward transfer with randomization}{chapter.17}% 132
\BOOKMARK [2][-]{subsection.17.3.1}{What if we can manipulate the source domain?}{section.17.3}% 133
\BOOKMARK [1][-]{section.17.4}{Multi-task transfer}{chapter.17}% 134
\BOOKMARK [1][-]{section.17.5}{Transfering models and value functions}{chapter.17}% 135
\BOOKMARK [2][-]{subsection.17.5.1}{The problem setting}{section.17.5}% 136
\BOOKMARK [2][-]{subsection.17.5.2}{Transferring models}{section.17.5}% 137
\BOOKMARK [2][-]{subsection.17.5.3}{Transferring value functions}{section.17.5}% 138

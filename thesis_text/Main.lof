\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax 
\babel@toc {english}{}
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.1}{\ignorespaces Conceptual schematic of reinforcement learning.\relax }}{7}{figure.caption.6}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.2}{\ignorespaces Schematic of a Markov chain.\relax }}{8}{figure.caption.7}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.3}{\ignorespaces Schematic of a Markov decision process.\relax }}{8}{figure.caption.8}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.4}{\ignorespaces Schematic of a partially observable Markov decision process.\relax }}{9}{figure.caption.9}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {2.5}{\ignorespaces Schematic of a Markov decision process with a policy $ \pi $.\relax }}{10}{figure.caption.10}%
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.1}{\ignorespaces Auto-encoder: learned by reconstructing the observation (one-to-one). The observation is the input and the computed state is the vector at the auto-encoder's bottleneck layer, i.e. is the output of the encoder part of the auto-encoder network. The loss is calculated between the true observation and the reconstructing observation (which is obtained by passing the observation though both the encoder and the decoder).\relax }}{30}{figure.caption.11}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.2}{\ignorespaces Forward model: predicting the future state from the state-action pair. The loss is computer from comparing the predicted state against the true next state (the states being the learned states). This can also be done directly by predicting the next observation and comparing against it. \relax }}{31}{figure.caption.12}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {3.3}{\ignorespaces Inverse model: predicting the action between two consecutive states. The loss is computer from comparing the predicted action between two consecutive states against the true action that was taken by the agent between those two states. (the states being the learned states). \relax }}{31}{figure.caption.13}%
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.1}{\ignorespaces Schematic of the feature extractor neural network parameter space. Since unsupervised learning converges much faster, it constricts the search space for the features extracted through reinforcement learning. This constriction is enforced joint training of both unsupervised state representation and reinforcement learning.\relax }}{38}{figure.caption.14}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {5.2}{\ignorespaces Tianshou concepts\relax }}{41}{figure.caption.15}%
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.1}{\ignorespaces The graphs of potential efficiency gains. As can be observed from the graphs, if training is started using an encoder which was already trained using only reinforcement learning better results are achieved more quickly. Of course, this does not encompass all potential benefits --- unsupervised learning could make the problem easier overall and thereby allow for both even faster learning and higher final scores.\relax }}{48}{figure.caption.17}%
\defcounter {refsection}{0}\relax 
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {}}}{48}{subfigure.1.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {}}}{48}{subfigure.1.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {}}}{48}{subfigure.1.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {}}}{48}{subfigure.1.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subfigure}{\numberline {(e)}{\ignorespaces {}}}{48}{subfigure.1.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subfigure}{\numberline {(f)}{\ignorespaces {}}}{48}{subfigure.1.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subfigure}{\numberline {(g)}{\ignorespaces {}}}{48}{subfigure.1.7}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.2}{\ignorespaces Runs with Rainbow only, but with different encoder sizes. As can be seen, some games strongly benefit from having a larger encoder, while learning fails on others.\relax }}{49}{figure.caption.18}%
\defcounter {refsection}{0}\relax 
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {}}}{49}{subfigure.2.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {}}}{49}{subfigure.2.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {}}}{49}{subfigure.2.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {}}}{49}{subfigure.2.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subfigure}{\numberline {(e)}{\ignorespaces {}}}{49}{subfigure.2.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subfigure}{\numberline {(f)}{\ignorespaces {}}}{49}{subfigure.2.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subfigure}{\numberline {(g)}{\ignorespaces {}}}{49}{subfigure.2.7}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.3}{\ignorespaces Effectiveness of parallel training. Overall the results are negative, some slightly and others strongly. Interestingly, using a pretrained encoder has a negative effect in this case. We suspect that this is due to the fixation on a particular local minimum caused by reconstruction loss. Importantly, the negative effect is present despite regularization maintaining a fixed latent space over a relatively large number of epochs (5-10). Results tend to be worse in games where MSE loss is less suitable and in games with larger visual complexity.\relax }}{50}{figure.caption.19}%
\defcounter {refsection}{0}\relax 
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {}}}{50}{subfigure.3.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {}}}{50}{subfigure.3.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {}}}{50}{subfigure.3.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {}}}{50}{subfigure.3.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subfigure}{\numberline {(e)}{\ignorespaces {}}}{50}{subfigure.3.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subfigure}{\numberline {(f)}{\ignorespaces {}}}{50}{subfigure.3.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subfigure}{\numberline {(g)}{\ignorespaces {}}}{50}{subfigure.3.7}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.4}{\ignorespaces Comparison of using and L2 latent space regularization and data augmentation in parallel training. L2 regularization helps overall because it helps fix the latent representations in place, thereby stabilizing the overall training process. Data augmentation on the other hand clearly hurt across the board.\relax }}{51}{figure.caption.20}%
\defcounter {refsection}{0}\relax 
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {}}}{51}{subfigure.4.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {}}}{51}{subfigure.4.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {}}}{51}{subfigure.4.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {}}}{51}{subfigure.4.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subfigure}{\numberline {(e)}{\ignorespaces {}}}{51}{subfigure.4.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subfigure}{\numberline {(f)}{\ignorespaces {}}}{51}{subfigure.4.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subfigure}{\numberline {(g)}{\ignorespaces {}}}{51}{subfigure.4.7}%
\defcounter {refsection}{0}\relax 
\contentsline {figure}{\numberline {6.5}{\ignorespaces Comparison between latent representation trained to compress the current frames and those trained to perform one-step forward predictions. Despite having a significantly higher reconstruction error, encoder trained to perform forward prediction perform better.\relax }}{52}{figure.caption.21}%
\defcounter {refsection}{0}\relax 
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {}}}{52}{subfigure.5.1}%
\defcounter {refsection}{0}\relax 
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {}}}{52}{subfigure.5.2}%
\defcounter {refsection}{0}\relax 
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {}}}{52}{subfigure.5.3}%
\defcounter {refsection}{0}\relax 
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {}}}{52}{subfigure.5.4}%
\defcounter {refsection}{0}\relax 
\contentsline {subfigure}{\numberline {(e)}{\ignorespaces {}}}{52}{subfigure.5.5}%
\defcounter {refsection}{0}\relax 
\contentsline {subfigure}{\numberline {(f)}{\ignorespaces {}}}{52}{subfigure.5.6}%
\defcounter {refsection}{0}\relax 
\contentsline {subfigure}{\numberline {(g)}{\ignorespaces {}}}{52}{subfigure.5.7}%
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
\defcounter {refsection}{0}\relax 
\addvspace {10\p@ }
